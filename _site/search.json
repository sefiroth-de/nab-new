[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Norman’s Academic Blog",
    "section": "",
    "text": "Willkommen in meinem Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nGrafiken nebeneinander setzen mit ggplot2 oder ggformula\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nCSV Dateien bearbeiten mit Miller\n\n\n\nAllgemeines\n\nMiller\n\nCSV\n\nDatenjudo\n\nDatenformate\n\n\n\n\n\n\n\n\n\nAug 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDatenjudo für Fragebögen\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDinge die man in zwei Dimensionen machen kann - Multiple lineare Regression\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 24, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nRegression mit studentisierten Daten\n\n\n\nStatistisches\n\nstatistic\n\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nInteraktionseffekte leichter interpretieren durch Transformationen\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nWege zur Normalverteilung\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 11, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nÜber die Koeffizienten einer linearen Regression\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 9, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nGraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest\n\n\n\nPython\n\nPyPy\n\nGraalPython\n\nGraalPy\n\nSpeedtest\n\n\n\n\n\n\n\n\n\nApr 8, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nÜber die t-Verteilung mit einem bzw. zwei Freiheitsgraden\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 17, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nBehäbige Funktionen aka slowly varying function\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nEin paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nGedankenstütze zu wichtigen Funktionsbegriffen in der Statistik\n\n\n\nStatistisches\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nCook Abstand\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDer Zentrale Grenzwertsatz\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nApr 5, 2017\n\n\nNorman Markgraf\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Norman Markgraf",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\nNorman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer. Als Lehrbeauftragter hat er an der Hochschule Bochum, Hochschule Rhein-Waal (am Campus Kleve) und der FOM Hochschule für Oekonomie und Management an den Standorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss, Wuppertal und München verschiedene Lehrveranstaltungen abgehalten."
  },
  {
    "objectID": "index.html#biografie",
    "href": "index.html#biografie",
    "title": "Norman Markgraf",
    "section": "",
    "text": "Norman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer. Als Lehrbeauftragter hat er an der Hochschule Bochum, Hochschule Rhein-Waal (am Campus Kleve) und der FOM Hochschule für Oekonomie und Management an den Standorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss, Wuppertal und München verschiedene Lehrveranstaltungen abgehalten."
  },
  {
    "objectID": "index.html#interessen",
    "href": "index.html#interessen",
    "title": "Norman Markgraf",
    "section": "Interessen",
    "text": "Interessen\n\nInteressen\nMathematik\nStatistik\nDatenanalyse\nData Literacy\nDatenkompetenz\nData Science\nFinanzmathematik\nWirtschaftsmathematik\nIngenieurmathematik\nDatenbanken\nBig Data"
  },
  {
    "objectID": "index.html#bildung",
    "href": "index.html#bildung",
    "title": "Norman Markgraf",
    "section": "Bildung",
    "text": "Bildung\n\nWissenschaftlicher Mitarbeiter am Lehrstuhl für Prozessinformatik der Fakultät für Elektro- und Informationstechnik, 2006 Ruhr-Universität Bochum\nDiplom-Mathematiker mit dem Schwerpunkt Informatik und dem Nebenfach Wirtschaftsinformatik, 1992 Ruhr-Universität Bochum"
  },
  {
    "objectID": "index.html#kontakt-impressum",
    "href": "index.html#kontakt-impressum",
    "title": "Norman Markgraf",
    "section": "Kontakt / Impressum",
    "text": "Kontakt / Impressum\n\nAngabe gemäß §5 TMG:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\n\n\nHaftungsausschluss - Disclaimer\n\nHaftung für Inhalte\nAlle Inhalte unseres Internetauftritts wurden mit größter Sorgfalt und nach bestem Gewissen erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt.\nEine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverzüglich entfernen.\n\n\nHaftungsbeschränkung für externe Links\nUnsere Webseite enthält Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher können wir für die „externen Links“ auch keine Gewähr auf Richtigkeit der Inhalte übernehmen. Für die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverstöße überprüft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine ständige inhaltliche Überprüfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht möglich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die außerhalb unseres Verantwortungsbereichs liegen, würde eine Haftungsverpflichtung ausschließlich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch möglich und zumutbar wäre, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\nDiese Haftungsausschlusserklärung gilt auch innerhalb des eigenen Internetauftrittes „Name Ihrer Domain“ gesetzten Links und Verweise von Fragestellern, Blogeinträgern, Gästen des Diskussionsforums. Für illegale, fehlerhafte oder unvollständige Inhalte und insbesondere für Schäden, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der über Links auf die jeweilige Veröffentlichung lediglich verweist.\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverzüglich entfernt.\n\n\nUrheberrecht\nDie auf unserer Webseite veröffentlichen Inhalte und Werke unterliegen dem deutschen Urheberrecht (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external)) . Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers außerhalb der Grenzen des Urheberrechtes bedürfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external) ). Downloads und Kopien dieser Seite sind nur für den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverzüglich entfernen.\nDieses Impressum wurde freundlicherweise von (www.jurarat.de) zur Verfügung gestellt."
  },
  {
    "objectID": "index.html#impressum",
    "href": "index.html#impressum",
    "title": "Norman Markgraf",
    "section": "Impressum",
    "text": "Impressum\n\nAngabe gemäß §5 TMG:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\n\n\nHaftungsausschluss - Disclaimer\n\nHaftung für Inhalte\nAlle Inhalte unseres Internetauftritts wurden mit größter Sorgfalt und nach bestem Gewissen erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt.\nEine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverzüglich entfernen.\n\n\nHaftungsbeschränkung für externe Links\nUnsere Webseite enthält Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher können wir für die „externen Links“ auch keine Gewähr auf Richtigkeit der Inhalte übernehmen. Für die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverstöße überprüft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine ständige inhaltliche Überprüfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht möglich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die außerhalb unseres Verantwortungsbereichs liegen, würde eine Haftungsverpflichtung ausschließlich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch möglich und zumutbar wäre, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\nDiese Haftungsausschlusserklärung gilt auch innerhalb des eigenen Internetauftrittes „Norman’s Academic Blog“ gesetzten Links und Verweise von Fragestellern, Blogeinträgern, Gästen des Diskussionsforums. Für illegale, fehlerhafte oder unvollständige Inhalte und insbesondere für Schäden, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der über Links auf die jeweilige Veröffentlichung lediglich verweist.\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverzüglich entfernt.\n\n\nUrheberrecht\nDie auf unserer Webseite veröffentlichen Inhalte und Werke unterliegen dem deutschen Urheberrecht (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external)) . Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers außerhalb der Grenzen des Urheberrechtes bedürfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external) ). Downloads und Kopien dieser Seite sind nur für den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverzüglich entfernen.\nDieses Impressum wurde freundlicherweise von (www.jurarat.de) zur Verfügung gestellt."
  },
  {
    "objectID": "posts/Willkommen/index.html",
    "href": "posts/Willkommen/index.html",
    "title": "Willkommen in meinem Blog",
    "section": "",
    "text": "Das ist mein erster Blog-Eintrag! Ich werde nun hoffentlich (langsam) meinen Blog von Hugo aus Quarto umstellen. Ich arbeite viel mit R und eine extra Sprache nur für den Blog ist mir einfach auf Dauer zu umständlich. Aber mal sehen, wann ich Zeit finde dieses Projekt umzustezen."
  },
  {
    "objectID": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "href": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "title": "CSV Dateien bearbeiten mit Miller",
    "section": "",
    "text": "Miller beschreibt sich selbst folgendermaßen:\n\nMiller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. You get to work with your data using named fields, without needing to count positional column indices.\n\nMiller kombiniert die Funktionalität von awk, sed und cut und eignet sich besonders für feldbasierte Datenmanipulation.\n\nBeispiel 1: Erstellung einer Markdown-Tabelle\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --omd cat\nMit diesem Befehl wird eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, anschließend werden alle Kommata durch Punkte ersetzt und schließlich wird daraus eine Markdown-Tabelle erzeugt.\n\n\nBeispiel 2: Gerahmte Darstellung\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --opprint --barred cat\nMit diesem Befehl wird ebenfalls eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, alle Kommata werden durch Punkte ersetzt und am Ende wird eine eingerahmte Tabelle erzeugt."
  },
  {
    "objectID": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html",
    "href": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "href": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#neues-setup",
    "href": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#neues-setup",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse für n = 11 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‘import nqueens; nqueens.main()’\nerzeugt.\n\n\nDie Ergebnisse des Test vom 20.7.2025\n\nCPython:\n\n3.13.1: 1 loop, best of 50: 673 msec per loop\n3.13.5: 1 loop, best of 50: 661 msec per loop\n\nPyPy:\n\n3.10-7.3.17: 1 loops, average of 50: 75.6 +- 2.9 msec per loop (using standard deviation)\n3.11-7.3.19: 1 loops, average of 50: 74.9 +- 2.89 msec per loop (using standard deviation)\n\nGraalPython/GraalPy\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1):1 loop, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 1 loop, best of 50: 30 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 1 loop, best of 50: 24.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 1 loop, best of 50: 28.1 msec per loop"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse für n = 11 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‘import nqueens; nqueens.main()’\nerzeugt.\n\n\nDie Ergebnisse des Test vom 20.7.2025\n\nCPython:\n\n3.13.1: 1 loop, best of 50: 673 msec per loop\n3.13.5: 1 loop, best of 50: 661 msec per loop\n\nPyPy:\n\n3.10-7.3.17: 1 loops, average of 50: 75.6 +- 2.9 msec per loop (using standard deviation)\n3.11-7.3.19: 1 loops, average of 50: 74.9 +- 2.89 msec per loop (using standard deviation)\n\nGraalPython/GraalPy\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1):1 loop, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 1 loop, best of 50: 30 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 1 loop, best of 50: 24.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 1 loop, best of 50: 28.1 msec per loop"
  },
  {
    "objectID": "Datenschutzerklaerung.html",
    "href": "Datenschutzerklaerung.html",
    "title": "Datenschutzerklaerung",
    "section": "",
    "text": "Personenbezogene Daten (nachfolgend zumeist nur „Daten“ genannt) werden von uns nur im Rahmen der Erforderlichkeit sowie zum Zwecke der Bereitstellung eines funktionsfähigen und nutzerfreundlichen Internetauftritts, inklusive seiner Inhalte und der dort angebotenen Leistungen, verarbeitet. Gemäß Art. 4 Ziffer 1. der Verordnung (EU) 2016/679, also der Datenschutz-Grundverordnung (nachfolgend nur „DSGVO“ genannt), gilt als „Verarbeitung“ jeder mit oder ohne Hilfe automatisierter Verfahren ausgeführter Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten, wie das Erheben, das Erfassen, die Organisation, das Ordnen, die Speicherung, die Anpassung oder Veränderung, das Auslesen, das Abfragen, die Verwendung, die Offenlegung durch Übermittlung, Verbreitung oder eine andere Form der Bereitstellung, den Abgleich oder die Verknüpfung, die Einschränkung, das Löschen oder die Vernichtung. Mit der nachfolgenden Datenschutzerklärung informieren wir Sie insbesondere über Art, Umfang, Zweck, Dauer und Rechtsgrundlage der Verarbeitung personenbezogener Daten, soweit wir entweder allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung entscheiden. Zudem informieren wir Sie nachfolgend über die von uns zu Optimierungszwecken sowie zur Steigerung der Nutzungsqualität eingesetzten Fremdkomponenten, soweit hierdurch Dritte Daten in wiederum eigener Verantwortung verarbeiten.\nUnsere Datenschutzerklärung ist wie folgt gegliedert:\nI. Informationen über uns als Verantwortliche\n…"
  },
  {
    "objectID": "Datenschutzerklaerung.html#i.-informationen-über-uns-als-verantwortliche",
    "href": "Datenschutzerklaerung.html#i.-informationen-über-uns-als-verantwortliche",
    "title": "Datenschutzerklaerung",
    "section": "I. Informationen über uns als Verantwortliche",
    "text": "I. Informationen über uns als Verantwortliche\nVerantwortlicher Anbieter dieses Internetauftritts im datenschutzrechtlichen Sinne ist:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\nDeutschland\nTelefon: +49-176-20077335\nE-Mail: admin(at)sefiroth.net\n\nDatenschutzbeauftragte/r beim Anbieter ist:\nNorman Markgraf"
  },
  {
    "objectID": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "href": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "title": "Datenschutzerklaerung",
    "section": "III. Informationen zur Datenverarbeitung",
    "text": "III. Informationen zur Datenverarbeitung\nIhre bei Nutzung unseres Internetauftritts verarbeiteten Daten werden gelöscht oder gesperrt, sobald der Zweck der Speicherung entfällt, der Löschung der Daten keine gesetzlichen Aufbewahrungspflichten entgegenstehen und nachfolgend keine anderslautenden Angaben zu einzelnen Verarbeitungsverfahren gemacht werden.\n\nCookies\n\nSitzungs-Cookies/Session-Cookies\n\nWir verwenden mit unserem Internetauftritt sog. Cookies. Cookies sind kleine Textdateien oder andere Speichertechnologien, die durch den von Ihnen eingesetzten Internet-Browser auf Ihrem Endgerät ablegt und gespeichert werden. Durch diese Cookies werden im individuellen Umfang bestimmte Informationen von Ihnen, wie beispielsweise Ihre Browser- oder Standortdaten oder Ihre IP-Adresse, verarbeitet. Durch diese Verarbeitung wird unser Internetauftritt benutzerfreundlicher, effektiver und sicherer, da die Verarbeitung bspw. die Wiedergabe unseres Internetauftritts in unterschiedlichen Sprachen oder das Angebot einer Warenkorbfunktion ermöglicht. Rechtsgrundlage dieser Verarbeitung ist Art. 6 Abs. 1 lit b.) DSGVO, sofern diese Cookies Daten zur Vertragsanbahnung oder Vertragsabwicklung verarbeitet werden. Falls die Verarbeitung nicht der Vertragsanbahnung oder Vertragsabwicklung dient, liegt unser berechtigtes Interesse in der Verbesserung der Funktionalität unseres Internetauftritts. Rechtsgrundlage ist in dann Art. 6 Abs. 1 lit. f) DSGVO. Mit Schließen Ihres Internet-Browsers werden diese Session-Cookies gelöscht.\n\nDrittanbieter-Cookies\n\nGegebenenfalls werden mit unserem Internetauftritt auch Cookies von Partnerunternehmen, mit denen wir zum Zwecke der Werbung, der Analyse oder der Funktionalitäten unseres Internetauftritts zusammenarbeiten, verwendet. Die Einzelheiten hierzu, insbesondere zu den Zwecken und den Rechtsgrundlagen der Verarbeitung solcher Drittanbieter-Cookies, entnehmen Sie bitte den nachfolgenden Informationen.\n\nBeseitigungsmöglichkeit\n\nSie können die Installation der Cookies durch eine Einstellung Ihres Internet-Browsers verhindern oder einschränken. Ebenfalls können Sie bereits gespeicherte Cookies jederzeit löschen. Die hierfür erforderlichen Schritte und Maßnahmen hängen jedoch von Ihrem konkret genutzten Internet-Browser ab. Bei Fragen benutzen Sie daher bitte die Hilfefunktion oder Dokumentation Ihres Internet-Browsers oder wenden sich an dessen Hersteller bzw. Support. Bei sog. Flash-Cookies kann die Verarbeitung allerdings nicht über die Einstellungen des Browsers unterbunden werden. Stattdessen müssen Sie insoweit die Einstellung Ihres Flash-Players ändern. Auch die hierfür erforderlichen Schritte und Maßnahmen hängen von Ihrem konkret genutzten Flash-Player ab. Bei Fragen benutzen Sie daher bitte ebenso die Hilfefunktion oder Dokumentation Ihres Flash-Players oder wenden sich an den Hersteller bzw. Benutzer-Support. Sollten Sie die Installation der Cookies verhindern oder einschränken, kann dies allerdings dazu führen, dass nicht sämtliche Funktionen unseres Internetauftritts vollumfänglich nutzbar sind.\n\n\nGoogle Analytics\nIn unserem Internetauftritt setzen wir Google Analytics ein. Hierbei handelt es sich um einen Webanalysedienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Der Dienst Google Analytics dient zur Analyse des Nutzungsverhaltens unseres Internetauftritts. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Analyse, Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Nutzungs- und nutzerbezogene Informationen, wie bspw. IP-Adresse, Ort, Zeit oder Häufigkeit des Besuchs unseres Internetauftritts, werden dabei an einen Server von Google in den USA übertragen und dort gespeichert. Allerdings nutzen wir Google Analytics mit der sog. Anonymisierungsfunktion. Durch diese Funktion kürzt Google die IP-Adresse schon innerhalb der EU bzw. des EWR. Die so erhobenen Daten werden wiederum von Google genutzt, um uns eine Auswertung über den Besuch unseres Internetauftritts sowie über die dortigen Nutzungsaktivitäten zur Verfügung zu stellen. Auch können diese Daten genutzt werden, um weitere Dienstleistungen zu erbringen, die mit der Nutzung unseres Internetauftritts und der Nutzung des Internets zusammenhängen. Google gibt an, Ihre IP-Adresse nicht mit anderen Daten zu verbinden. Zudem hält Google unter https://www.google.com/intl/de/policies/privacy/partners weitere datenschutzrechtliche Informationen für Sie bereit, so bspw. auch zu den Möglichkeiten, die Datennutzung zu unterbinden. Zudem bietet Google unter https://tools.google.com/dlpage/gaoptout?hl=de ein sog. Deaktivierungs-Add-on nebst weiteren Informationen hierzu an. Dieses Add-on lässt sich mit den gängigen Internet-Browsern installieren und bietet Ihnen weitergehende Kontrollmöglichkeit über die Daten, die Google bei Aufruf unseres Internetauftritts erfasst. Dabei teilt das Add-on dem JavaScript (ga.js) von Google Analytics mit, dass Informationen zum Besuch unseres Internetauftritts nicht an Google Analytics übermittelt werden sollen. Dies verhindert aber nicht, dass Informationen an uns oder an andere Webanalysedienste übermittelt werden. Ob und welche weiteren Webanalysedienste von uns eingesetzt werden, erfahren Sie natürlich ebenfalls in dieser Datenschutzerklärung.\n\n\nGoogle-Maps\nIn unserem Internetauftritt setzen wir Google Maps zur Darstellung unseres Standorts sowie zur Erstellung einer Anfahrtsbeschreibung ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Sofern Sie die in unseren Internetauftritt eingebundene Komponente Google Maps aufrufen, speichert Google über Ihren Internet-Browser ein Cookie auf Ihrem Endgerät. Um unseren Standort anzuzeigen und eine Anfahrtsbeschreibung zu erstellen, werden Ihre Nutzereinstellungen und -daten verarbeitet. Hierbei können wir nicht ausschließen, dass Google Server in den USA einsetzt. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung der Funktionalität unseres Internetauftritts. Durch die so hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Anfahrtsbeschreibung zu übermitteln ist. Sofern Sie mit dieser Verarbeitung nicht einverstanden sind, haben Sie die Möglichkeit, die Installation der Cookies durch die entsprechenden Einstellungen in Ihrem Internet-Browser zu verhindern. Einzelheiten hierzu finden Sie vorstehend unter dem Punkt „Cookies“. Zudem erfolgt die Nutzung von Google Maps sowie der über Google Maps erlangten Informationen nach den Google-Nutzungsbedingungen https://policies.google.com/terms?gl=DE&hl=de und den Geschäftsbedingungen für Google Maps https://www.google.com/intl/de_de/help/terms_maps.html. Überdies bietet Google unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitergehende Informationen an.\n\n\nGoogle reCAPTCHA\nIn unserem Internetauftritt setzen wir Google reCAPTCHA zur Überprüfung und Vermeidung von Interaktionen auf unserer Internetseite durch automatisierte Zugriffe, bspw. durch sog. Bots, ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Durch diesen Dienst kann Google ermitteln, von welcher Webseite eine Anfrage gesendet wird sowie von welcher IP-Adresse aus Sie die sog. reCAPTCHA-Eingabebox verwenden. Neben Ihrer IP-Adresse werden womöglich noch weitere Informationen durch Google erfasst, die für das Angebot und die Gewährleistung dieses Dienstes notwendig sind. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Sicherheit unseres Internetauftritts sowie in der Abwehr unerwünschter, automatisierter Zugriffe in Form von Spam o.ä.. Google bietet unter https://policies.google.com/privacy weitergehende Informationen zu dem allgemeinen Umgang mit Ihren Nutzerdaten an.\n\n\nGoogle Fonts\nIn unserem Internetauftritt setzen wir Google Fonts zur Darstellung externer Schriftarten ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Durch die bei Aufruf unseres Internetauftritts hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Darstellung der Schrift zu übermitteln ist. Google bietet unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitere Informationen an und zwar insbesondere zu den Möglichkeiten der Unterbindung der Datennutzung.\n\n\n„Facebook“-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Facebook ein. Bei Facebook handelt es sich um einen Internetservice der facebook Inc., 1601 S. California Ave, Palo Alto, CA 94304, USA. In der EU wird dieser Service wiederum von der Facebook Ireland Limited, 4 Grand Canal Square, Dublin 2, Irland, betrieben, nachfolgend beide nur „Facebook“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000GnywAAC&status=Active garantiert Facebook, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Weitergehende Informationen über die möglichen Plug-ins sowie über deren jeweilige Funktionen hält Facebook unter https://developers.facebook.com/docs/plugins/ für Sie bereit.\nSofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Facebook in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Facebook Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Facebook eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Facebook erkannt. Die so gesammelten Informationen weist Facebook womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Gefällt mir“-Button von Facebook benutzen, werden diese Informationen in Ihrem Facebook-Nutzerkonto gespeichert und ggf. über die Plattform von Facebook veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Facebook ausloggen oder durch den Einsatz eines Add-ons für Ihren Internetbrowser verhindern, dass das Laden des Facebook-Plug-in blockiert wird. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Facebook in den unter https://www.facebook.com/policy.php abrufbaren Datenschutzhinweisen bereit.\n\n\nX-(ehemals „Twitter“)-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Twitter ein. Bei X handelt es sich um einen Internetservice der X.AI Corp., 795 Folsom St., Suite 600, San Francisco, CA 94107, USA, nachfolgend nur „xAI“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000TORzAAO&status=Active garantiert Twitter, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Sofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Twitter in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Twitter Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Twitter eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Twitter erkannt. Die so gesammelten Informationen weist Twitter womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Teilen“-Button von Twitter benutzen, werden diese Informationen in Ihrem Twitter-Nutzerkonto gespeichert und ggf. über die Plattform von Twitter veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Twitter ausloggen oder die entsprechenden Einstellungen in Ihrem Twitter-Benutzerkonto vornehmen. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Twitter in den unter https://twitter.com/privacy abrufbaren Datenschutzhinweisen bereit.\nMuster-Datenschutzerklärung der Anwaltskanzlei Weiß & Partner"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "Ein Beispiel:",
    "text": "Ein Beispiel:\nNehmen wir drei Verteilungen mit Zufallsvariable \\(U\\), \\(X\\), \\(Y\\) und jeweils \\(n\\) Realisationen \\(u_1,\\dots, u_n\\), \\(x_1,\\dots, x_n\\), \\(y_1,\\dots, y_n\\).\nWählen wir zunächst \\(n=5\\):\n\nu\n\n[1] 19.726 69.683 60.790  0.955 42.901\n\nx\n\n[1]  7.942 15.905 12.917  6.818  4.434\n\ny\n\n[1] 59.961 56.552 51.094 75.288 47.985\n\n\nStandardisieren wir die Werte:\n\nlibrary(mosaic)\nzscore(u)\n\n[1] -0.6695256  1.0830283  0.7710507 -1.3280357  0.1434823\n\nzscore(x)\n\n[1] -0.3543069  1.3440714  0.7067796 -0.5940379 -1.1025063\n\nzscore(y)\n\n[1]  0.1677971 -0.1526624 -0.6657361  1.6085958 -0.9579944\n\n\nDie Behauptung des Zentralengrenzwertsatzes ist nun, dass mit steigender Anzahl an Werten \\(n\\) die standardisierten Werte in der empirischen Verteilungsfunktion sich immer mehr der Verteilungsfunktion der Standardnormalverteilung annähern:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeiterführende Literatur und Quellen dieses Eintrags:"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu möchte mensch zwei (oder mehr) Grafiken neben- oder übereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zunächst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausführen oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu möchte mensch zwei (oder mehr) Grafiken neben- oder übereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zunächst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausführen oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "",
    "text": "Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine möglichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt. Dabei tolerieren wir eine (kleine) Abweichung \\(e_i\\).\nBei einer einfachen linearen Regression gehen wir davon aus, dass die Datenpunkte (im wesentlichen) auf einer Geraden liegen. Mit \\(g(x)=\\beta_0 + \\beta1 \\cdot x\\) ergibt sich dann für die Datenpunkte die Gleichung:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i\\]\nUnsere Aufgabe besteht nun darin die Parameter \\(\\beta_0\\) (y-Achsenabschnitt) und \\(\\beta_1\\) (Steigung) an Hand der \\(n\\) Datenpunkte zu schätzen. Alle unsere Schätzungen kennzeichnen wir mit einem Dach (\\(\\hat{.}\\)), um sie von den (in der Regel unbekannten) Parametern besser zu unterscheiden.\nWir suchen somit nach \\(\\hat\\beta= \\left(\\hat\\beta_0,\\, \\hat\\beta_1\\right)\\), so dass die Gerade \\(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x\\) zu gegebenem \\(x_i\\) eine möglichst gute Schätzung von \\(y_i\\) (genannt \\(\\hat{y}_i\\)) hat:\n\\[\n\\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i\n\\]\nDie Abweichung \\(\\hat{e_i}\\) unserer Schätzung \\(\\hat{y}_i\\) von dem gegebenen Wert \\(y_i\\) lässt sich schreiben als:\n\\[\n\\hat{e_i} =  \\hat{y_i} - y_i =  \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i\n\\]\nWenn wir diese Abweichung über alle \\(i\\) minimieren, finden wir unser \\(\\hat\\beta\\).\nDoch das wirft eine Frage auf: Wie genau messen wir die möglichst kleinste Abweichung der \\(\\hat{e_i}\\) konkret?\nWir betrachten zunächst drei einfache Ideen:\nGewöhnlich nutzen wir die quadratischen Abweichungen, weshalb wir die drei Ideen ebenso in umgekehrter Reihenfolge betrachten wollen:"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "3. Idee: Summe der quadratischen Abweichungen",
    "text": "3. Idee: Summe der quadratischen Abweichungen\nWir bezeichnen mit\n\\[\\begin{aligned}\nQS &= QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n  &= \\sum\\limits_{i=1}^n \\hat{e_i}^2 = \\sum\\limits_{i=1}^n \\left(\\hat{y_i} - y_i \\right)^2 \\\\\n  &= \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\n\\end{aligned}\\]\ndie Quadrat-Summe der Abweichungen.\nGesucht wird \\(\\hat\\beta=\\left(\\hat\\beta_0,\\,\\hat\\beta_1\\right)\\), so das \\(QS\\) minimiert wird.\nDies ist ein Minimierungsproblem, bei dem wir zu mindestens eine (exakte) mathematisch-algebraisch Lösung in Form eines stationären Punktes finden können. Dazu berechnen wir die Nullstelle der ersten partiellen Ableitung von \\(QS\\) nach \\(\\hat\\beta_0\\) bzw. \\(\\hat\\beta_1\\).\n\nVorbemerkungen\nWegen \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i\\) ist \\(n \\cdot \\bar{x} =\\sum\\limits_{i=1}^n x_i\\) und analog \\(n \\cdot \\bar{y} =\\sum\\limits_{i=1}^n y_i\\)\n\n\nSchätzen des y-Achenabschnitts \\(\\hat\\beta_0\\)\nEs ist:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot 1 \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 + \\sum\\limits_{i=1}^n\\hat\\beta_1 \\cdot x_i - \\sum\\limits_{i=1}^n y_i\\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot\\sum\\limits_{i=1}^n x_i - \\sum\\limits_{i=1}^n y_i \\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot n \\cdot \\bar{x} - n \\cdot\\bar{y} \\right) \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right)\n\\end{aligned}\\]\nUm stationäre Punkte zu ermitteln, müssen wir den Ausdruck nun gleich Null setzen und erhalten:\n\\[\\begin{aligned}\n  0 &= \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\qquad | : (2 \\cdot n) \\\\\n  &= \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y}\n\\end{aligned}\\]\nStellen wir nach \\(\\hat\\beta_0\\) um, erhalten wir:\n\\[\\begin{aligned}\n  \\hat\\beta_0 &= - \\hat\\beta_1\\cdot\\bar{x} + \\bar{y} \\\\\n  \\hat\\beta_0 &= \\bar{y} - \\hat\\beta_1\\cdot\\bar{x}\n\\end{aligned}\\]\nUm \\(\\hat\\beta_0\\) zu bestimmen, benötigen wir \\(\\hat\\beta_1\\).\n\n\nSchätzen der Steigung \\(\\hat\\beta_1\\)\nEs ist:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot x_i \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 \\cdot x_i + \\sum\\limits_{i=1}^n \\hat\\beta_1 \\cdot x_i\\cdot x_i- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot \\sum\\limits_{i=1}^n  x_i + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right)\n\\end{aligned}\\]\nWir ersetzen nun \\(\\hat\\beta_0\\) durch \\(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\) und erhalten:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  &=\n  2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(\\left(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\right) \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - n \\cdot \\hat\\beta_1 \\cdot  \\bar{x}^2  + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n\\end{aligned}\\]\nMit Hilfe des Verschiebesatzes von Steiner (zweimal angewendet) erhalten wir:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  \n    &=2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)+ \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(\\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)\n\\end{aligned}\\]\nWir setzen nun wieder den Ausdruck gleich Null:\n\\[\\begin{aligned}\n0 &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)  \\qquad | : 2\\\\\n   &= \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{aligned}\\]\nUnd stellen dann nach \\(\\hat\\beta_1\\) um:\n\\[\\begin{aligned}\n  \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\n    &= \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\\\\n  \\hat\\beta_1\n    &= \\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\n\\end{aligned}\\]\nWir können nun Zähler und Nenner der rechten Seite mit \\(\\frac{1}{n}\\) erweitern und erhalten so:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\\\\n\\end{aligned}\\]\nOder aber wir erweitern mit \\(\\frac{1}{n-1}\\) und erhalten:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{s_{x,y}}{s^2_{x}}\n\\end{aligned}\\]\nDamit können wir zur Berechnung sowohl die Kovarianz der Grundgesamtheit \\(\\sigma_{x,y}\\) und die Varianz \\(\\sigma^2_x\\) von \\(x\\), als auch deren Schätzer \\(s_{x,y}\\) und \\(s^2_x\\) verwendet werden!\nDiese Methode nennt sich Methode der kleinsten Quadrate (engl. ordenary least square method) und wir sprechen dann auch von den Kleinste-Quadrate-Schätzern (oder kurz KQ-Schätzer bzw. OLS-Schätzer) \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\).\nErweitern wir den Ausdruck mit Standardabweichung \\(\\sigma_y\\) bzw. \\(s_y\\), so erhalten wir:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n&= \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\end{aligned}\\]\nund analog für die Schätzer:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{s_{x,y}}{s^2_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_y} \\cdot \\frac{s_y}{s_x} \\\\\n&= r_{x,y} \\cdot \\frac{s_y}{s_x} \\\\\n\\end{aligned}\\]\nDie Steigung \\(\\hat\\beta_1\\) hat somit eine direkte Beziehung mit dem Korrelationskoeffizenten \\(\\rho\\) (der Grundgesamtheit) bzw. \\(r\\) (der Stichprobe).\nFür eine Berechnung in R heißt dies: wir können die Regressionskoeffizienten \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\) direkt algebraisch ausrechnen, wenn wir\n\ndie Standardabweichungen von \\(x\\) und \\(y\\) und den Korrelationskoeffizienten oder\ndie Varianz von \\(x\\) und Kovarianz von \\(x\\) und \\(y\\)\n\nhaben.\n\n\nEin Beispiel in R:\nAuf Grundlage der Datentabelle mtcars wollen wir Prüfen wie ein linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdestärke hp) modelliert werden kann.1\n\nlibrary(mosaic)\n\n# Wir nehmen die Datentabelle 'mtcars':\nmtcars %&gt;%\n  select(hp, mpg) -&gt; dt\n\n# Ein kurzer Blick auf die Daten:\nfavstats(~ hp, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  146.6875 68.56287\nfavstats(~ mpg, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  20.09062 6.026948\n\n# Wir vergleichen den Verbrauch (mpg, miles per gallon) \n# mit den Pferdestärken (hp) mit Hilfe eines Streudiagramms:\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir zunächst die Mittelwerte von \\(x\\) (also ‘hp’) und \\(y\\) (also ‘mpg’)\n\n(mean_hp &lt;- mean(~ hp, data = dt))\n#&gt; [1] 146.6875\n(mean_mpg &lt;- mean(~ mpg, data = dt))\n#&gt; [1] 20.09062\n\nund zeichnen die Punkt \\((\\bar{x}, \\bar{y}) = (146.69, 20.09)\\) in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir nun die Schätzwerte für die Regressionsgerade\n\n(beta_1 &lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt))\n#&gt; [1] -0.06822828\n(beta_0 &lt;- mean_mpg - beta_1 * mean_hp)\n#&gt; [1] 30.09886\n\nund zeichnen diese in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \"dodgerblue\") %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988605 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nStudentisieren – einmal hin und einmal zurück\nWas passiert eigentlich, wenn wir unsere \\(x\\) und \\(y\\) Werte studentisieren (aka standardisieren oder z-transformieren)?\nZur Erinnerung, studentisieren geht so: \\[\nx^{stud} = \\frac{x - \\bar{x}}{s_x}\n\\]\nIn R können wir das mit der Funktion ‘zscore’ wie folgt machen:\n\ndt %&gt;%\n  mutate(\n    hp_stud = zscore(hp),\n    mpg_stud = zscore(mpg)\n  ) -&gt; dt\n\nNatürlich sind die Mittelwerte nun Null und die Standardabweichungen Eins:\n\nfavstats(~ hp_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;           mean sd\n#&gt;  -4.857226e-17  1\nfavstats(~ mpg_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;          mean sd\n#&gt;  4.336809e-17  1\n\nDer Grund für die kleinen Abweichungen von der Null bei den Mittelwerten sind unumgängliche Rundungsfehler, die der Computer macht!\nSchauen wir uns nun das Streudiagramm an, zusammen mit dem Mittelpunkt \\((0,0)\\)\n\ngf_point(mpg_stud ~ hp_stud, data = dt) %&gt;%\n  gf_point(0 ~ 0, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(-2, 2))\n\n\n\n\n\n\n\n\nAuch wenn die Skalierungen sich geändert haben, die Diagramme sind sehr ähnlich.\nBestimmen wir die Koeffizienten der Regressionsgerade\n\n(beta_stud_1 &lt;- cov(mpg_stud ~ hp_stud, data = dt))\n#&gt; [1] -0.7761684\n(beta_stud_0 &lt;- 0 - beta_stud_1 * 0)\n#&gt; [1] 0\n\nund setzen sie in das Streudiagramm ein:\n\n\n\n\n\n\n\n\n\nWir können das studentisierte Problem auch wieder auf unser ursprüngliches zurück rechnen.\nDie Regressionsgerade im studentisierten Problem lautet:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761684 \\cdot x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot x^{stud}\n\\end{aligned}\n\\]\nRechnen wir nun mittels der Formel \\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\]\ndie Steigung um, so erhalten wir:\n\n(b1 &lt;- beta_stud_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822828\n\nUnd setzen wir das in unsere Gleichung zur Bestimmung von \\(\\hat\\beta_0\\) ein:\n\n(b0 &lt;- mean(dt$mpg) - b1 * mean(dt$hp))\n#&gt; [1] 30.09886\n\nso erhalten wir die Schätzwerte des ursprünglichen Problem.\n\n\nEin anderer Weg um die Regressionskoeffizenten zu bestimmen…\nGehen wir das Problem noch einmal neu an. Wir suchen \\(\\hat\\beta=(\\hat\\beta_0, \\hat\\beta_1)\\) welches \\(QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\\) minimiert.\nStatt es direkt, wie oben durch Null setzen der partiellen Ableitungen, zu bestimmen, wählen wir nun einen mathematisch-numerischen Ansatz und wollen \\(\\hat\\beta \\in \\mathbf{R}^2\\) als Optimierungsproblem mit Hilfe des Gradientenverfahrens lösen.\nBeim Gradientenverfahren wird versucht, ausgehend von einem Startwert \\(\\hat\\beta^0 \\in \\mathbf{R}^2\\), gemäß der Iterationsvorschrift\n\\[\n\\hat\\beta^{k+1} = \\hat\\beta^{k} + \\alpha^k \\cdot d^k\n\\]\nfür alle \\(k=0,1, ...\\) eine Näherungslösung für \\(\\hat\\beta\\) zu finden. Dabei ist \\(\\alpha^k &gt; 0\\) eine positive Schrittweite und \\(d^k\\in\\mathbf{R}^n\\) eine Abstiegsrichtung, welche wir in jedem Iterationsschritt \\(k\\) so bestimmen, dass die Folge \\(\\hat\\beta^k\\) zu einem stationären Punkt, unserer Näherungslösung, konvergiert.\nIm einfachsten Fall, dem Verfahren des steilsten Abstieges, wird der Abstiegsvektor \\(d^k\\) aus dem Gradienten \\(\\nabla QS\\) wie folgt bestimmt:\n\\[\nd^k = -\\nabla QS\\left(\\hat\\beta^k\\right)\n\\]\nWegen \\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot n \\cdot \\left(  \\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y} \\right)\n\\]\nund\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS = 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\right)\n\\]\ngilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot(\\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y})  \\\\\n\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{pmatrix}\n\\end{aligned}\n\\]\nWir wollen hier von Anfang an mit den studentisierten Werten arbeiten, weil diese numerisch viele Vorteile haben. Darum vereinfachen sich die beiden partiellen Ableitungen noch einmal zu:\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot v\n\\]\nund\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\\\\n&= 2 \\cdot (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{aligned}\n\\]\nSomit gilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot \\hat\\beta_0 \\\\\n(n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{pmatrix}\n\\end{aligned}\n\\]\nUm die Varianz und die Kovarianz nicht jedesmal neu zu berechnen, speichern wir die Ergebnisse vorab. Ebenso, damit der Quellcode kürzer wird, speichern wir in \\(x\\) und \\(y\\) die studentisierten Werte von \\(hp\\) und \\(mpg\\):\n\n# Vorbereitungen \nvar_x &lt;- var(~ hp_stud, data = dt)\ncov_xy &lt;- cov(mpg_stud ~ hp_stud, data = dt)\n\nn &lt;- length(dt$hp_stud)\n\nx &lt;- dt$hp_stud\ny &lt;- dt$mpg_stud\n\nNun erstellen wir die \\(QS\\) und \\(\\nabla QS\\) Funktionen: Wir definieren diese Funktion wie folgt in R:\n\nqs &lt;- function(b_0, b_1) {\n  sum((b_1 * x - y)**2)\n}\n\nnabla_qs &lt;- function(b_0, b_1) {\n  c(2 * n * b_0,\n    2 * (n - 1) * (b_1 * var_x - cov_xy)\n  )\n}\n\nDie Schrittweite \\(alpha\\) bestimmen wir mit Hilfe der Armijo-Bedingung und der Backtracking Liniensuche: Diese formalisiert das Konzept “genügend” in der geforderten Verringerung des Funktionswertes. Die Bedingung \\(f(x^k + \\alpha d^k) &lt; f(x^k)\\) wird modifiziert zu \\[f(x^k + \\alpha d^k) \\leq f(x^k) + \\sigma \\alpha \\left(\\nabla f(x^k)\\right)^T d^k,\\] mit \\(\\sigma\\in (0,1)\\). Die Armijo-Bedingung umgeht Konvergenzprobleme der einfachen Bedingung, indem sie fordert, dass die Verringerung zumindest proportional zur Schrittweite und zur Richtungsableitung \\(\\left(\\nabla f(x^k)\\right)^T d^k\\) ist, mit Hilfe der Proportionalitätskonstante \\(\\sigma\\). In der Praxis werden oft sehr kleine Werte verwendet, z.B. \\(\\sigma=0.0001\\).\nDie Backtracking-Liniensuche verringert die Schrittweite wiederholt um den Faktor \\(\\rho\\) (rho) , bis die Armijo-Bedingung erfüllt ist. Sie terminiert garantiert nach einer endlichen Anzahl von Schritten. Weshalb wir sie hier einsetzen:\n\nalpha_k &lt;- function(b_0, b_1, d_k, alpha = 1, sigma = 0.0001, rho = 0.5) {\n  d_0 &lt;- d_k[1]\n  d_1 &lt;- d_k[2]\n  nabla &lt;- nabla_qs(b_0, b_1)\n  n_0 &lt;- nabla[1]\n  n_1 &lt;- nabla[2]\n\n  lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n  rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n\n  while (lhs &gt; rhs) {\n    alpha &lt;- rho * alpha\n    lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n    rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n  }\n  return(alpha)\n}\n\nEin paar Einstellungen vorab:\n\n# maximale Anzahl an Iterationen\nmax_iter &lt;- 1000\niter &lt;- 0\n\n# Genauigkeit\neps &lt;- 10**-6\n\n# Startwerte\nb_0 &lt;- 0 \nb_1 &lt;- -1 \n\nFür eine vorgegebene Genauigkeit \\(eps=10^{-6}\\), den Startwerten \\(\\hat\\beta_0^0 = 0\\) und \\(\\hat\\beta_1^0 = -1\\) können wir somit das Verfahren starten:\n\nwhile (TRUE) {\n  iter &lt;- iter + 1\n\n  d_k &lt;- -nabla_qs(b_0, b_1)\n\n  ad_ &lt;- alpha_k(b_0, b_1, d_k) * d_k\n\n  x0 &lt;- b_0 + ad_[1]\n  x1 &lt;- b_1 + ad_[2]\n\n  if ((abs(b_0 - x0) &lt; eps) & (abs(b_1 - x1) &lt; eps) | (iter &gt; max_iter)) {\n    break\n  }\n  b_0 &lt;- x0\n  b_1 &lt;- x1\n}\n\nWir haben in \\(203\\) Iterationsschritten das folgende Ergebnis für die Regressionskoeffizienten:\n\\[\n\\hat\\beta_0^{stud} = 0 \\qquad \\hat\\beta_1^{stud} = -0.7761689\n\\]\nBetrachten wir die daraus erstellte Regressionsgerade:\n\n\n\n\n\n\n\n\n\nUm die Regressionskoeffizienten für unser ursprüngliches Problem zu erhalten müssen wir wie folgt zurück rechnen:\n\n(b1 &lt;- b_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822832\n(b0 &lt;- mean(dt$mpg) -  b1 * mean(dt$hp))\n#&gt; [1] 30.09887\n\nDie Geradengleichung für das ursprüngliches Problem lautet somit:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988668 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nDie R Funktion optim\nIn R gibt es bessere Optimierungsmethoden, als die hier verwendete. Zum Beispiel können wir die Funktion optim verwenden. Die Funktion optim benötigt die zu optimierende \\(f(x)\\) und ggf. die Gradientenfunktion \\(gf(x)\\) sowie einen Startpunkt \\(x^0\\):\n\nf &lt;- function(beta) {\n  qs(beta[1], beta[2])\n}\n\ngrf &lt;- function(beta) {\n  nabla_qs(beta[1], beta[2])\n}\n\n# Der eigentliche Aufruf von optim:\nergb &lt;- optim(c(0,-0.5),f ,grf, method = \"CG\")\n\n# Auslesen der Schätzer aus dem Ergebnis:\n(optim_beta_0 &lt;- ergb$par[1])\n#&gt; [1] 0\n(optim_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.7761683\n\nWir erhalten somit für das studentisierte Problem die Gerade:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta_0^{stud} + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761683 \\cdot  x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot  x^{stud}\n\\end{aligned}\n\\]\nFür das ursprüngliche Problem rechnen wir mittels\n\noptim_b1 &lt;- optim_beta_1 * sd(dt$mpg) / sd(dt$hp)\noptim_b0 &lt;- mean(dt$mpg) -  optim_b1 * mean(dt$hp)\n\num und erhalten:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988601 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "2. Idee: Summe der absoluten Abweichungen",
    "text": "2. Idee: Summe der absoluten Abweichungen\nWir ändern nun die Abweichungsmessfunktion von der Quadrat-Summe hin zu den Absolut-Summen:\n\\[\nAS = AS(\\hat\\beta) = AS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n |\\hat{y}_i - y_i|\n\\]\nAuch hier wollen wir mit den studentisierten Daten arbeiten und stellen die Funktion der Absolut-Summen auf:\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  return(sum(abs(b_0 + b_1 * x - y)))\n}\n\nDanach konstruieren wir die zu optimierende Funktion \\(f\\):\n\n# Zu optimierende Funktion\nf &lt;- function(beta) {\n  as(beta[1], beta[2])\n}\n\nDiesmal nutzen wir optim ohne eine Gradientenfunktion:\n\nergb &lt;- optim(c(0,-1), f)\n\n# Schätzer auslesen\n(opti_as_beta_0 &lt;- ergb$par[1])\n#&gt; [1] -0.1304518\n(opti_as_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.6844911\n\nSchauen wir uns nun die so erhaltene Gerade im Vergleich mit der ‘normalen’ Regressionsgerade an:\n\n\n\n\n\n\n\n\n\nIn grün und gestrichelt sehen wir die Gerade aus der Idee der quadratischen Abweichungssummen, in blau die aus der Idee der absoluten Abweichungssummen.\nFür unser ursprüngliches Problem rechnen wir um:\n\n# Umrechnen in die ursprüngliche Fragestellung\n(as_b1 &lt;- opti_as_beta_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06016948\n(as_b0 &lt;- (mean(dt$mpg) - as_b1 * mean(dt$hp)) + opti_as_beta_0 * sd(dt$mpg))\n#&gt; [1] 28.13051\n\nUnd die dazu gehörige Darstellung:\n\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 28.1305094 -0.0601695 \\cdot x \\\\\n          &\\approx 28.131 -0.06 \\cdot x\n\\end{aligned}\n\\]\nDiese Methode nennt sich Median-Regression und ein ein Spezialfall der Quantilsregression, die sich u.a. mit dem R-Paket quantreg unmittelbar umsetzen lässt:\n\nlibrary(quantreg)\nergmedianreg &lt;- rq(mpg ~ hp, data = dt)\ncoef(ergmedianreg)\n#&gt; (Intercept)          hp \n#&gt; 28.13050847 -0.06016949"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "1. Idee: Betrag der Summe der Abweichungen",
    "text": "1. Idee: Betrag der Summe der Abweichungen\nWenn wir die Summe der Abweichungen \\(\\sum\\limits_{i=1}^n \\hat{e}_i\\) minimieren wollen, dann ist es sinnvoll den Betrag davon zu minimieren. Wir suchen also die Schätzer \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\), so dass der Ausdruck\n\\[\n\\left| \\sum_{i=1}^n \\hat{e}_i \\right| = \\left| \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \\right|\n\\]\nminimal ist.\nWegen:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i)\n&= \\sum_{i=1}^n \\hat\\beta_0 + \\sum_{i=1}^n \\hat\\beta_1 \\cdot x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot n \\cdot \\bar{x} - n \\cdot \\bar{y} \\\\\n&= n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\bar{x} - \\bar{y} \\right) \\\\\n&= n \\cdot \\left( \\hat\\beta_0 - \\bar{y} + \\hat\\beta_1 \\cdot \\bar{x}  \\right)\n\\end{aligned}\n\\]\nkönnen wir das absolute Minimum bei \\(\\hat\\beta_0 - \\bar{y} =0\\) und \\(\\hat\\beta_1 \\cdot \\bar{x}=0\\) erreichen, was zur Lösung \\(\\hat\\beta_0 =\\bar{y}\\) und \\(\\hat\\beta_1 = 0\\) führt. Dies ist unser Nullmodel in dem die \\(x_i\\) keinen Einfluss auf die \\(y_i\\) haben und wir daher pauschal die \\(y_i\\) mit \\(\\hat{y}_i=\\bar{y}\\), also dem Mittelwert der \\(y_i\\) abschätzen."
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nAls Vergleich können wir uns die Quadratsumme \\(QS\\) und Absolutsumme \\(AS\\) der drei Modelle einmal ansehen:\n\n# Quadratische Abweichungssummen\nqs &lt;- function(b_0, b_1) {\n  sum(((b_0 + b_1 * dt$hp) - dt$mpg )**2)\n}\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  sum(abs((b_0 + b_1 * dt$hp) - dt$mpg))\n}\n\n\n# Quadratsummen:\nquad_sum &lt;- c(qs(b0, b1), qs(as_b0, as_b1), qs(mean_mpg, 0))\n\n# Absolutsummen:\nabs_sum &lt;- c(as(b0, b1), as(as_b0, as_b1), as(mean_mpg, 0))\n\ntab &lt;- tibble(\n  sums = c(quad_sum, abs_sum),\n  sum_type = rep(c(\"quad\", \"abs\"), each = 3),\n  methode = rep(c(\"Idee 3\", \"Idee 2\", \"Idee 1\"), 2)\n)\n\npivot_wider(tab, names_from=sum_type, values_from=sums, names_sort=T)\n#&gt; # A tibble: 3 × 3\n#&gt;   methode   abs  quad\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Idee 3   93.0  448.\n#&gt; 2 Idee 2   87.3  477.\n#&gt; 3 Idee 1  151.  1126."
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 quantreg_6.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nDas “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "",
    "text": "Bei einer multiplen linearen Regression kann man den Einfluss einer unabhägigen Variable auf das Verhalten einer anderen unabhägigen Variable in Bezug auf die abhägige Variable mit modellieren.\nWir wollen das einmal an dem Beispiel der folgenden Datentabelle Impact of Beauty on Instructor’s Teaching Ratings und der Fragestellung in wie weit das Alter und das Geschlecht einen Einfluss auf das Evaluationsergebnis haben.\nDazu stellen laden wir die Daten aus dem Internet:\n\nlibrary(mosaic)\nurl &lt;- paste0(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/\",\n              \"TeachingRatings.csv\")\nteacherratings &lt;- read.csv(url)\n\nund betrachten das Streudiagramm:\n\ngf_point(eval ~ age, color = ~gender, data = teacherratings)"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "Nachtrag und Danksagung",
    "text": "Nachtrag und Danksagung\nDie Idee zu diesem Blog-Post verdanke ich dem Blog von Prof. Dr. Sebastian Sauer. Hier der Link zum Orginal-Blog: https://data-se.netlify.app/2021/06/17/beispiel-zur-interpretation-des-interaktionseffekts/\nDanke auch für die kritische Durchsicht und die hilfreichen Anmerkungen."
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#reproduzierbarkeitsinformationen",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "",
    "text": "Wir wollen den Fall unterschen bei dem wir mit zwei statistischen Variabeln (\\(X\\) und \\(Y\\)) eine dritte Variable (\\(Z\\)) mittels einer multiplen linearen Regression modellieren.\nEs seien die Datenpunkte \\((x_1, y_1, z_1), \\dots, (x_n, y_n, z_n)\\) gegeben und wir wollen eine lineare Funktion \\(g(x,y)\\) finden, so dass\n\\[\nz_i = g(x_i,y_i)+ \\epsilon_i =\\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot y_i + \\epsilon_i\n\\]\ngilt und der Abweichungsterm \\(\\epsilon_i\\) möglichst klein ist.\nAuf Grundlage unserer Datenpunkt wollen wir die Koeffizienten so schätzen, dass die Summe der quadratische Abweichungen minimal ist. \\[\n  QS = QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\n  = \\sum\\limits_{i=1}^n (z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i )^2\n\\]\nDas führt zu der folgenden, notwendigen Bedingen (für stationäre Punkte):\n\\[\n\\nabla QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\end{pmatrix}\n\\]\nIm einzelnen heißt das:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot\\sum\\limits_{i=1}^n \\left(z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i \\right) \\\\\n&= -2 \\cdot n \\cdot \\left(\\bar{z} - \\hat\\beta_0 - \\hat\\beta_1 \\cdot\\bar{x} - \\hat\\beta_2 \\cdot\\bar{y} \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot x_i - \\hat\\beta_0 \\cdot x_i - \\hat\\beta_1 \\cdot x_i\\cdot x_i - \\hat\\beta_2 \\cdot y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot y_i - \\hat\\beta_0 \\cdot y_i - \\hat\\beta_1 \\cdot x_i\\cdot y_i - \\hat\\beta_2 \\cdot y_i\\cdot y_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die 1. Gleichung gleich Null und stellen nach \\(\\hat\\beta_0\\) um:\n\\[\n  \\hat\\beta_0 = \\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}\n\\] Nun ersetzen wir \\(\\hat\\beta_0\\) in den verbleibenden Gleichungen durch \\(z_i - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i\\) und nutzen den Verschiebesatz:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (x_i -\\bar{x})^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i -\\bar{y})^2 - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die beiden Gleichungen nun gleich Null und formen nach \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\) um:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2}\n\\end{aligned}\n\\]\nDurch Erweiterung von Zähler nun Nenner mit \\(\\frac{1}{n-1}\\) erhalten wir:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  &= \\frac{s_{x,z}-\\hat\\beta_2\\cdot s_{x,y}}{s^2_{x}} = \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i - \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2} \\\\\n  &= \\frac{s_{y,z}-\\hat\\beta_1\\cdot s_{x,y}}{s^2_{y}} = \\frac{s_{y,z}}{s^2_y}-\\hat\\beta_1 \\frac{s_{x,y}}{s^2_{y}} \\\\\n\\end{aligned}\n\\]\nwir setzen nun die erste in die zweite Gleichung ein und erhalten:\n\\[\n\\begin{aligned}\n\\hat\\beta_2\n  &= \\frac{s_{y,z}}{s^2_y} - \\left(\\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\right) \\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}} + \\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{\\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}}}{1-\\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}}} \\\\\n  &= \\frac{\\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x\\cdot s^2_y}}{\\frac{s^2_x s^2_y-(s_{x,y})^2}{s^2_x \\cdot s^2_y}}\n  = \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2}\n\\end{aligned}\n\\]\nUnd damit weiter:\n\\[\n\\begin{aligned}\n\\hat\\beta_1  \n  &= \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z}}{s^2_x} - \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2} \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z} (s^2_x s^2_y - (s_{x,y})^2) - s_{y,z}s_{x,y}s^2_x + s_{x,z}s_{x,y}s_{x,y}}{s^2_x (s^2_x s^2_y - (s_{x,y})^2)} \\\\\n    &= \\frac{s_{x,z}s^2_x s^2_y - s_{x,z}(s_{x,y})^2 - s_{y,z}s_{x,y}s^2_x + s_{x,z}(s_{x,y})^2}{s^2_x s^2_x s^2_y- s^2_x(s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nlibrary(mosaic)\n\nmtcars %&gt;%\n  select(mpg, hp, wt) -&gt; dt\n\n# Von R berechnete Koeffizienten:\ncoef(lm(mpg ~ hp + wt, data = dt))\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074\n\nmean_x = mean( ~ hp, data = dt)\nmean_y = mean( ~ wt, data = dt)\nmean_z = mean( ~ mpg, data = dt)\n\ns_xy &lt;- cov(hp ~ wt, data = dt)\ns_xz &lt;- cov(hp ~ mpg, data = dt)\ns_yz &lt;- cov(wt ~ mpg, data = dt)\n\nvar_x &lt;- var(~ hp, data = dt)\nvar_y &lt;- var(~ wt, data = dt)\nb1z &lt;- s_xz*var_x*var_y - s_xz*(s_xy)**2 - s_yz*s_xy*var_x + s_xz*s_xy**2\nb1n &lt;- var_x*var_x*var_y - var_x*s_xy**2\nb1 &lt;- b1z / b1n\nb2 &lt;- (s_yz*var_x - s_xz*s_xy) / (var_x * var_y - s_xy*s_xy)\nb0 &lt;- mean_z - b1 * mean_x - b2 * mean_y\n\n# Koeffizienten zur Ausgabe aufbereiten:\nmy_coef &lt;- c(b0, b1, b2)\nnames(my_coef) &lt;- c(\"(Intercept)\", \"hp\", \"wt\")\n\n# Von Hand berechnete Koeffizienten:\nmy_coef\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074"
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "Was passiert, wenn wir alle Datenpunkte studentisieren?",
    "text": "Was passiert, wenn wir alle Datenpunkte studentisieren?\nWir rechnen um in:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x}; \\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}; \\quad z_i^{\\text{stud}} = \\frac{z_i-\\bar{z}}{s_z}\n\\]\nDamit ist\n\\[\n\\bar{x_i}^\\text{stud} = 0; \\quad \\bar{y_i}^\\text{stud} = 0;\\quad \\bar{z_i}^\\text{stud} = 0\n\\] und\n\\[\ns_{{x_i}^\\text{stud}} = 1; \\quad s_{{y_i}^\\text{stud}} = 1;\\quad s_{{z_i}^\\text{stud}} = 1\n\\]\nZur Vereinfachung lassen wir die Kennzeichnung “stud” weg. Damit ist dann:\n\\[\n\\begin{aligned}\n\\hat\\beta_0\n  &= 0\\\\\n\\\\\n\\hat\\beta_1\n  &= \\frac{s_{x,z} \\cdot s^2_x \\cdot s^2_y - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y}s^2_x + s_{x,z} \\cdot (s_{x,y})^2}{s^2_x \\cdot s^2_x s^2_y- s^2_x \\cdot (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} \\cdot 1 \\cdot 1 -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} \\cdot 1 + s_{x,z} \\cdot (s_{x,y})^2}{1 \\cdot 1 \\cdot 1 - 1 \\cdot (s_{x,y})^2}\\\\\n  &= \\frac{s_{x,z} -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} + s_{x,z} \\cdot (s_{x,y})^2}{1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} - s_{y,z} \\cdot s_{x,y} }{1 - (s_{x,y})^2} \\\\\n\\\\\n\\hat\\beta_2\n  &= \\frac{s_{y,z} \\cdot s^2_x - s_{x,z} \\cdot s_{x,y}}{s^2_x \\cdot s^2_y - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} \\cdot 1 - s_{x,z} \\cdot s_{x,y}}{1 \\cdot 1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} - s_{x,z} \\cdot s_{x,y}}{1 - (s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nWir schauen uns ein paar Fälle genauer an:\n\nFall: \\(X\\) und \\(Y\\) sind unabhängig. Dann ist \\(s_{x,y}=0\\) und wir erhalten \\(\\hat\\beta_1=s_{x,z}\\in[-1;1]\\) und \\(\\hat\\beta_2=s_{y,z}\\in[-1;1]\\).\nFall: \\(X\\) und \\(Y\\) sind abhängig. Dann ist \\(|s_{x,y}|=1\\) und es gibt keine Lösung für \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\).\nFall: \\(0 &lt; |s_{x,y}| &lt; 1\\). …"
  }
]