[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator\n\n\n\n\n\n\n\n\nJan 30, 2026\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nB-Bäume und mehr: Ein Vergleich von Python-Implementierungen\n\n\n\n\n\n\n\n\nJan 4, 2026\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nDas Lotto-Abbildungsproblem\n\n\n\n\n\n\n\n\nDec 22, 2025\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nWarum Exponentiation und Logarithmen wichtig sind in der Statistik.\n\n\n\n\n\n\n\n\nDec 22, 2025\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nQuarto-Dokumente mit WebP anstatt PNG Bilddateien erstellen\n\n\n\n\n\n\n\n\nDec 3, 2025\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nBrotli statt Gzip auf dem Server konfigurieren\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nWie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nBeweisverfahren in der Mathematik\n\n\n\n\n\n\n\n\nJul 27, 2025\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nWillkommen in meinem Blog\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nGrafiken nebeneinander setzen mit ggplot2 oder ggformula\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nCSV Dateien bearbeiten mit Miller\n\n\n\n\n\n\n\n\nAug 27, 2021\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nDatenjudo für Fragebögen\n\n\n\n\n\n\n\n\nJun 27, 2021\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nDinge die man in zwei Dimensionen machen kann - Multiple lineare Regression\n\n\n\n\n\n\n\n\nJun 24, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nRegression mit studentisierten Daten\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nInteraktionseffekte leichter interpretieren durch Transformationen\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nWege zur Normalverteilung\n\n\n\n\n\n\n\n\nJun 11, 2021\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nÜber die Koeffizienten einer linearen Regression\n\n\n\n\n\n\n\n\nJun 9, 2021\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nGraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest\n\n\n\n\n\n\n\n\nApr 8, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nÜber die t-Verteilung mit einem bzw. zwei Freiheitsgraden\n\n\n\n\n\n\n\n\nFeb 17, 2021\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nBehäbige Funktionen aka slowly varying function\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nEin paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nGedankenstütze zu wichtigen Funktionsbegriffen in der Statistik\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nCook Abstand\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nDer Zentrale Grenzwertsatz\n\n\n\n\n\n\n\n\nApr 5, 2017\n\n4 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projekte.html",
    "href": "projekte.html",
    "title": "Projekte",
    "section": "",
    "text": "Workshop zum Thema quarto\n\n\n\n\n\n\nworkshop\n\nquarto\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDozenten-Workshop: Methoden der quantitativen Forschung\n\n\nEinstieg-in-die-Datenanalyse-mit-R\n\n\n\nworkshop\n\nR\n\nStatistik\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nFUMS - Fresh Up your Maths Skills\n\n\n\n\n\n\nVorlesungsskript\n\nMathematik\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nMathematische Grundlagen der Informatik\n\n\n\n\n\n\nVorlesungsskript\n\nMathematik\n\nInformatik\n\n\n\n\n\n\n\n\n\nOct 18, 2019\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nNPBT aka Norman’s Pandoc Beamer Themes\n\n\n\n\n\n\nVorlesungsskript\n\nBeamer\n\nTheme\n\npandoc\n\n\n\n\n\n\n\n\n\nFeb 6, 2017\n\n\nNorman Markgraf\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Norman Markgraf",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     X\n  \n  \n    \n     Buy me a coffee\n  \n  \n    \n     ko-fi\n  \n\n  \n  \n\n\nNorman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer.\nAls Lehrbeauftragter hat er an der Hochschule Bochum (am Campus Bochum), der Hochschule Rhein-Waal (am Campus Kleve), der FOM Hochschule für Oekonomie und Management an den Studienorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss, Wuppertal und München, der Frankfurt University of Applied Sciences (im Online-Studium) und der IU Internationale Hochschule an den Studienorten Düsseldorf und Köln verschiedene Lehrveranstaltungen, Mentorate und Tutorien in den Bereichen Wirtschafts-/ Finanz- und Ingenieurmathematik, Progammieren in Python, Grundlagen der Informatik, Formale Beschreibungsverfahren, Big Data, Datenmodellierung / Datenbanken (inkl. NO SQL), Statistik, Quantiative (Forschungs-)Methoden und Qualitative Methoden abgehalten."
  },
  {
    "objectID": "index.html#biografie",
    "href": "index.html#biografie",
    "title": "Norman Markgraf",
    "section": "",
    "text": "Norman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer.\nAls Lehrbeauftragter hat er an der Hochschule Bochum (am Campus Bochum), der Hochschule Rhein-Waal (am Campus Kleve), der FOM Hochschule für Oekonomie und Management an den Studienorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss, Wuppertal und München, der Frankfurt University of Applied Sciences (im Online-Studium) und der IU Internationale Hochschule an den Studienorten Düsseldorf und Köln verschiedene Lehrveranstaltungen, Mentorate und Tutorien in den Bereichen Wirtschafts-/ Finanz- und Ingenieurmathematik, Progammieren in Python, Grundlagen der Informatik, Formale Beschreibungsverfahren, Big Data, Datenmodellierung / Datenbanken (inkl. NO SQL), Statistik, Quantiative (Forschungs-)Methoden und Qualitative Methoden abgehalten."
  },
  {
    "objectID": "index.html#interessen",
    "href": "index.html#interessen",
    "title": "Norman Markgraf",
    "section": "Interessen",
    "text": "Interessen\n\nMathematik\n\nFinanzmathematik\nWirtschaftsmathematik\nIngenieurmathematik\n\nStatistik\n\nDatenanalyse\nData Literacy\nDatenkompetenz\nData Science\nR\n\nInformatik\n\nDatenbanken (SQL und NoSQL)\nBig Data\nPython\nMicroPython"
  },
  {
    "objectID": "index.html#ausbildung",
    "href": "index.html#ausbildung",
    "title": "Norman Markgraf",
    "section": "Ausbildung",
    "text": "Ausbildung\n\nWissenschaftlicher Mitarbeiter am Lehrstuhl für Prozessinformatik der Fakultät für Elektro- und Informationstechnik, 2006 Ruhr-Universität Bochum\nDiplom-Mathematiker mit dem Schwerpunkt Informatik und dem Nebenfach Wirtschaftsinformatik, 1992 Ruhr-Universität Bochum"
  },
  {
    "objectID": "index.html#impressum",
    "href": "index.html#impressum",
    "title": "Norman Markgraf",
    "section": "Impressum",
    "text": "Impressum\n\nAngabe gemäß §5 TMG:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\n\n\nHaftungsausschluss - Disclaimer\n\nHaftung für Inhalte\nAlle Inhalte unseres Internetauftritts wurden mit größter Sorgfalt und nach bestem Gewissen erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt.\nEine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverzüglich entfernen.\n\n\nHaftungsbeschränkung für externe Links\nUnsere Webseite enthält Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher können wir für die „externen Links“ auch keine Gewähr auf Richtigkeit der Inhalte übernehmen. Für die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverstöße überprüft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine ständige inhaltliche Überprüfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht möglich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die außerhalb unseres Verantwortungsbereichs liegen, würde eine Haftungsverpflichtung ausschließlich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch möglich und zumutbar wäre, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\nDiese Haftungsausschlusserklärung gilt auch innerhalb des eigenen Internetauftrittes „Norman’s Academic Blog“ gesetzten Links und Verweise von Fragestellern, Blogeinträgern, Gästen des Diskussionsforums. Für illegale, fehlerhafte oder unvollständige Inhalte und insbesondere für Schäden, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der über Links auf die jeweilige Veröffentlichung lediglich verweist.\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverzüglich entfernt.\n\n\nUrheberrecht\nDie auf unserer Webseite veröffentlichen Inhalte und Werke unterliegen dem deutschen Urheberrecht (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external)) . Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers außerhalb der Grenzen des Urheberrechtes bedürfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external) ). Downloads und Kopien dieser Seite sind nur für den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverzüglich entfernen.\nDieses Impressum wurde freundlicherweise von (www.jurarat.de) zur Verfügung gestellt."
  },
  {
    "objectID": "projekte/NPBT/index.html",
    "href": "projekte/NPBT/index.html",
    "title": "NPBT aka Norman’s Pandoc Beamer Themes",
    "section": "",
    "text": "Norman’s Pandoc Beamer Themes ist eine Sammlung von Themes um schnell das Design für verschiedene Anlässe relativ schnell zu ändern."
  },
  {
    "objectID": "projekte/NPBT/index.html#npbt-aka-normans-pandoc-beamer-themes",
    "href": "projekte/NPBT/index.html#npbt-aka-normans-pandoc-beamer-themes",
    "title": "NPBT aka Norman’s Pandoc Beamer Themes",
    "section": "",
    "text": "Norman’s Pandoc Beamer Themes ist eine Sammlung von Themes um schnell das Design für verschiedene Anlässe relativ schnell zu ändern."
  },
  {
    "objectID": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html",
    "href": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html",
    "title": "Dozenten-Workshop: Methoden der quantitativen Forschung",
    "section": "",
    "text": "Einstieg in die Datenanalyse mit R ist ein Workshop den ich regelmässig für die FOM und ihre Dozent:innen gehalten habe.\nEr stellt einen Einstieg in R und RStudio-Desktop dar und fokusiert dann auf Simulationsbasiserte Inferenz (SBI)."
  },
  {
    "objectID": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html#dozenten-workshop-methiden-der-quantitativen-forschung",
    "href": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html#dozenten-workshop-methiden-der-quantitativen-forschung",
    "title": "Dozenten-Workshop: Methoden der quantitativen Forschung",
    "section": "",
    "text": "Einstieg in die Datenanalyse mit R ist ein Workshop den ich regelmässig für die FOM und ihre Dozent:innen gehalten habe.\nEr stellt einen Einstieg in R und RStudio-Desktop dar und fokusiert dann auf Simulationsbasiserte Inferenz (SBI)."
  },
  {
    "objectID": "projekte/MathematischeGrundelagenDerInformatik/index.html",
    "href": "projekte/MathematischeGrundelagenDerInformatik/index.html",
    "title": "Mathematische Grundlagen der Informatik",
    "section": "",
    "text": "Die ist das Skript zu den von mir an der FOM gehaltenen Vorlesungen im Fach “Mathematische Grundlagen der Informatik”.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdrückliches und schriftliches Einverständnis benutzt werden.\nFür alle anderen gilt:\n© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "projekte/MathematischeGrundelagenDerInformatik/index.html#vorlesungsskript-mathematische-grundlagen-der-informatik",
    "href": "projekte/MathematischeGrundelagenDerInformatik/index.html#vorlesungsskript-mathematische-grundlagen-der-informatik",
    "title": "Mathematische Grundlagen der Informatik",
    "section": "",
    "text": "Die ist das Skript zu den von mir an der FOM gehaltenen Vorlesungen im Fach “Mathematische Grundlagen der Informatik”.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdrückliches und schriftliches Einverständnis benutzt werden.\nFür alle anderen gilt:\n© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "publications/STATISTIK21/index.html",
    "href": "publications/STATISTIK21/index.html",
    "title": "Statistik21",
    "section": "",
    "text": "Statistik21 ist ein Projekt der FOM mit dem Ziel, die Passgenauigkeit der Statistik-Lehre an der FOM in Bezug auf die notwendigen Methodenkennnisse von Studierenden didaktisch und konzeptionell kontinuierlich weiter zu verbessern."
  },
  {
    "objectID": "publications/STATISTIK21/index.html#zusammenfassung",
    "href": "publications/STATISTIK21/index.html#zusammenfassung",
    "title": "Statistik21",
    "section": "",
    "text": "Statistik21 ist ein Projekt der FOM mit dem Ziel, die Passgenauigkeit der Statistik-Lehre an der FOM in Bezug auf die notwendigen Methodenkennnisse von Studierenden didaktisch und konzeptionell kontinuierlich weiter zu verbessern."
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "",
    "text": "Wir wollen den Fall unterschen bei dem wir mit zwei statistischen Variabeln (\\(X\\) und \\(Y\\)) eine dritte Variable (\\(Z\\)) mittels einer multiplen linearen Regression modellieren.\nEs seien die Datenpunkte \\((x_1, y_1, z_1), \\dots, (x_n, y_n, z_n)\\) gegeben und wir wollen eine lineare Funktion \\(g(x,y)\\) finden, so dass\n\\[\nz_i = g(x_i,y_i)+ \\epsilon_i =\\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot y_i + \\epsilon_i\n\\]\ngilt und der Abweichungsterm \\(\\epsilon_i\\) möglichst klein ist.\nAuf Grundlage unserer Datenpunkt wollen wir die Koeffizienten so schätzen, dass die Summe der quadratische Abweichungen minimal ist. \\[\n  QS = QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\n  = \\sum\\limits_{i=1}^n (z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i )^2\n\\]\nDas führt zu der folgenden, notwendigen Bedingen (für stationäre Punkte):\n\\[\n\\nabla QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\end{pmatrix}\n\\]\nIm einzelnen heißt das:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot\\sum\\limits_{i=1}^n \\left(z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i \\right) \\\\\n&= -2 \\cdot n \\cdot \\left(\\bar{z} - \\hat\\beta_0 - \\hat\\beta_1 \\cdot\\bar{x} - \\hat\\beta_2 \\cdot\\bar{y} \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot x_i - \\hat\\beta_0 \\cdot x_i - \\hat\\beta_1 \\cdot x_i\\cdot x_i - \\hat\\beta_2 \\cdot y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot y_i - \\hat\\beta_0 \\cdot y_i - \\hat\\beta_1 \\cdot x_i\\cdot y_i - \\hat\\beta_2 \\cdot y_i\\cdot y_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die 1. Gleichung gleich Null und stellen nach \\(\\hat\\beta_0\\) um:\n\\[\n  \\hat\\beta_0 = \\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}\n\\] Nun ersetzen wir \\(\\hat\\beta_0\\) in den verbleibenden Gleichungen durch \\(z_i - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i\\) und nutzen den Verschiebesatz:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (x_i -\\bar{x})^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i -\\bar{y})^2 - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die beiden Gleichungen nun gleich Null und formen nach \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\) um:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2}\n\\end{aligned}\n\\]\nDurch Erweiterung von Zähler nun Nenner mit \\(\\frac{1}{n-1}\\) erhalten wir:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  &= \\frac{s_{x,z}-\\hat\\beta_2\\cdot s_{x,y}}{s^2_{x}} = \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i - \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2} \\\\\n  &= \\frac{s_{y,z}-\\hat\\beta_1\\cdot s_{x,y}}{s^2_{y}} = \\frac{s_{y,z}}{s^2_y}-\\hat\\beta_1 \\frac{s_{x,y}}{s^2_{y}} \\\\\n\\end{aligned}\n\\]\nwir setzen nun die erste in die zweite Gleichung ein und erhalten:\n\\[\n\\begin{aligned}\n\\hat\\beta_2\n  &= \\frac{s_{y,z}}{s^2_y} - \\left(\\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\right) \\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}} + \\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{\\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}}}{1-\\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}}} \\\\\n  &= \\frac{\\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x\\cdot s^2_y}}{\\frac{s^2_x s^2_y-(s_{x,y})^2}{s^2_x \\cdot s^2_y}}\n  = \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2}\n\\end{aligned}\n\\]\nUnd damit weiter:\n\\[\n\\begin{aligned}\n\\hat\\beta_1  \n  &= \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z}}{s^2_x} - \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2} \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z} (s^2_x s^2_y - (s_{x,y})^2) - s_{y,z}s_{x,y}s^2_x + s_{x,z}s_{x,y}s_{x,y}}{s^2_x (s^2_x s^2_y - (s_{x,y})^2)} \\\\\n    &= \\frac{s_{x,z}s^2_x s^2_y - s_{x,z}(s_{x,y})^2 - s_{y,z}s_{x,y}s^2_x + s_{x,z}(s_{x,y})^2}{s^2_x s^2_x s^2_y- s^2_x(s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nlibrary(mosaic)\n\nmtcars %&gt;%\n  select(mpg, hp, wt) -&gt; dt\n\n# Von R berechnete Koeffizienten:\ncoef(lm(mpg ~ hp + wt, data = dt))\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074\n\nmean_x = mean( ~ hp, data = dt)\nmean_y = mean( ~ wt, data = dt)\nmean_z = mean( ~ mpg, data = dt)\n\ns_xy &lt;- cov(hp ~ wt, data = dt)\ns_xz &lt;- cov(hp ~ mpg, data = dt)\ns_yz &lt;- cov(wt ~ mpg, data = dt)\n\nvar_x &lt;- var(~ hp, data = dt)\nvar_y &lt;- var(~ wt, data = dt)\nb1z &lt;- s_xz*var_x*var_y - s_xz*(s_xy)**2 - s_yz*s_xy*var_x + s_xz*s_xy**2\nb1n &lt;- var_x*var_x*var_y - var_x*s_xy**2\nb1 &lt;- b1z / b1n\nb2 &lt;- (s_yz*var_x - s_xz*s_xy) / (var_x * var_y - s_xy*s_xy)\nb0 &lt;- mean_z - b1 * mean_x - b2 * mean_y\n\n# Koeffizienten zur Ausgabe aufbereiten:\nmy_coef &lt;- c(b0, b1, b2)\nnames(my_coef) &lt;- c(\"(Intercept)\", \"hp\", \"wt\")\n\n# Von Hand berechnete Koeffizienten:\nmy_coef\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074"
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "Was passiert, wenn wir alle Datenpunkte studentisieren?",
    "text": "Was passiert, wenn wir alle Datenpunkte studentisieren?\nWir rechnen um in:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x}; \\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}; \\quad z_i^{\\text{stud}} = \\frac{z_i-\\bar{z}}{s_z}\n\\]\nDamit ist\n\\[\n\\bar{x_i}^\\text{stud} = 0; \\quad \\bar{y_i}^\\text{stud} = 0;\\quad \\bar{z_i}^\\text{stud} = 0\n\\] und\n\\[\ns_{{x_i}^\\text{stud}} = 1; \\quad s_{{y_i}^\\text{stud}} = 1;\\quad s_{{z_i}^\\text{stud}} = 1\n\\]\nZur Vereinfachung lassen wir die Kennzeichnung “stud” weg. Damit ist dann:\n\\[\n\\begin{aligned}\n\\hat\\beta_0\n  &= 0\\\\\n\\\\\n\\hat\\beta_1\n  &= \\frac{s_{x,z} \\cdot s^2_x \\cdot s^2_y - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y}s^2_x + s_{x,z} \\cdot (s_{x,y})^2}{s^2_x \\cdot s^2_x s^2_y- s^2_x \\cdot (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} \\cdot 1 \\cdot 1 -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} \\cdot 1 + s_{x,z} \\cdot (s_{x,y})^2}{1 \\cdot 1 \\cdot 1 - 1 \\cdot (s_{x,y})^2}\\\\\n  &= \\frac{s_{x,z} -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} + s_{x,z} \\cdot (s_{x,y})^2}{1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} - s_{y,z} \\cdot s_{x,y} }{1 - (s_{x,y})^2} \\\\\n\\\\\n\\hat\\beta_2\n  &= \\frac{s_{y,z} \\cdot s^2_x - s_{x,z} \\cdot s_{x,y}}{s^2_x \\cdot s^2_y - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} \\cdot 1 - s_{x,z} \\cdot s_{x,y}}{1 \\cdot 1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} - s_{x,z} \\cdot s_{x,y}}{1 - (s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nWir schauen uns ein paar Fälle genauer an:\n\nFall: \\(X\\) und \\(Y\\) sind unabhängig. Dann ist \\(s_{x,y}=0\\) und wir erhalten \\(\\hat\\beta_1=s_{x,z}\\in[-1;1]\\) und \\(\\hat\\beta_2=s_{y,z}\\in[-1;1]\\).\nFall: \\(X\\) und \\(Y\\) sind abhängig. Dann ist \\(|s_{x,y}|=1\\) und es gibt keine Lösung für \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\).\nFall: \\(0 &lt; |s_{x,y}| &lt; 1\\). …"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "",
    "text": "Bei einer multiplen linearen Regression kann man den Einfluss einer unabhägigen Variable auf das Verhalten einer anderen unabhägigen Variable in Bezug auf die abhägige Variable mit modellieren.\nWir wollen das einmal an dem Beispiel der folgenden Datentabelle Impact of Beauty on Instructor’s Teaching Ratings und der Fragestellung in wie weit das Alter und das Geschlecht einen Einfluss auf das Evaluationsergebnis haben.\nDazu stellen laden wir die Daten aus dem Internet:\n\nlibrary(mosaic)\nurl &lt;- paste0(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/\",\n              \"TeachingRatings.csv\")\nteacherratings &lt;- read.csv(url)\n\nund betrachten das Streudiagramm:\n\ngf_point(eval ~ age, color = ~gender, data = teacherratings)"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "Nachtrag und Danksagung",
    "text": "Nachtrag und Danksagung\nDie Idee zu diesem Blog-Post verdanke ich dem Blog von Prof. Dr. Sebastian Sauer. Hier der Link zum Orginal-Blog: https://data-se.netlify.app/2021/06/17/beispiel-zur-interpretation-des-interaktionseffekts/\nDanke auch für die kritische Durchsicht und die hilfreichen Anmerkungen."
  },
  {
    "objectID": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html",
    "href": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html",
    "title": "Warum Exponentiation und Logarithmen wichtig sind in der Statistik.",
    "section": "",
    "text": "Die Exponentiation und der Logarithmus sind wichtig in der Statistik, weil sie helfen, komplexe Zusammenhänge zu verstehen und zu modellieren. Hier sind einige Gründe, warum sie so wichtig sind:\n\nDaten Transformation: Viele statistische Modelle setzen voraus, dass die Daten normalverteilt sind. Durch die Anwendung von Logarithmen können schiefe Verteilungen in eine normalverteilte Form transformiert werden, was die Analyse erleichtert.\nMultiplikative Beziehungen: In vielen Fällen sind die Beziehungen zwischen Variablen multiplikativ statt additiv. Logarithmen ermöglichen es, diese Beziehungen in eine additive Form zu überführen, was die Interpretation und Modellierung vereinfacht.\nWachstumsprozesse: Exponentielle Funktionen werden häufig verwendet, um Wachstumsprozesse zu modellieren, wie z.B. Bevölkerungswachstum oder Zinseszinsen. Logarithmen helfen dabei, diese Prozesse zu analysieren und zu verstehen.\nLinearisierung: Viele nicht-lineare Modelle können durch die Anwendung von Logarithmen linearisiert werden, was die Anwendung von linearen Regressionsmethoden ermöglicht.\nSkalierung: Logarithmische Skalen werden oft verwendet, um Daten darzustellen, die über mehrere Größenordnungen variieren, wie z.B. in der Finanzanalyse oder bei Messungen von Erdbebenstärken. Insgesamt sind Exponentiation und Logarithmen unverzichtbare Werkzeuge in der Statistik, die helfen, Daten zu transformieren, Beziehungen zu modellieren und komplexe Phänomene zu verstehen.\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(ggplot2)\n\n# Beispiel in R: Log-Transformation einer schiefen Verteilung\nset.seed(2009)\ndata = rexp(1000, rate = 0.1)  # Exponentielle Verteilung\ndf &lt;- data.frame(x = data)\n\nggplot(aes(x = x), data = df) +\n  geom_histogram(bins = 30, fill = \"lightblue\") +\n  labs(title = \"Original Data\") +\n  xlab(\"Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\ndf_log &lt;- data.frame(x = log(data))  # Log-Transformation\n\nggplot(aes(x = x), data = df_log) +\n  geom_histogram(bins = 30, fill = \"lightgreen\") +\n  labs(title = \"Log-Transformed Data\") +\n  xlab(\"Log(Value)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Beispiel in Python: Log-Transformation einer schiefen Verteilung\n\nnp.random.seed(2009)\n\n# Exponentielle Verteilung\ndata = np.random.exponential(scale=10, size=1000)\nplt.hist(data, bins=30, color='lightblue', alpha=0.7)\nplt.title('Original Data')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\n\nlog_data = np.log(data)  # Log-Transformation\nplt.hist(log_data, bins=30, color='lightgreen', alpha=0.7)\nplt.title('Log-Transformed Data')\nplt.xlabel('Log(Value)')\nplt.show()\n\n\n\n\n\n\n\n\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRPython\n\n\n\nlibrary(ggplot2)\n\n# Beispiel in R: Linearisierung einer exponentiellen Beziehung\nset.seed(2009)\n\nx &lt;- seq(1, 10, by = 0.1)\ny &lt;- 2 * exp(0.5 * x) + rnorm(length(x),\n                 mean = 0, sd = 5)  # Exponentielle Beziehung mit Rauschen\ndf &lt;- data.frame(x = x, y = y)\nggplot(aes(x = x, y = y), data = df) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Exponentielle Beziehung\", x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlog_y &lt;- log(y)  # Log-Transformation von Y\ndf &lt;- data.frame(x = x, y = log_y)\nggplot(aes(x = x, y = y), data = df) +\n  geom_point(color = \"red\") +\n  labs(title = \"Linearisierte Beziehung\", x = \"X\", y = \"Log(Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Beispiel in Python: Linearisierung einer exponentiellen Beziehung\nnp.random.seed(2009)\n\nx = np.arange(1, 10.1, 0.1)\ny = 2 * np.exp(0.5 * x) + np.random.normal(\n    0, 5, size=len(x) \n    )\nplt.scatter(x, y, color='blue')\nplt.title('Exponentielle Beziehung')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\n\nlog_y = np.log(y)  # Log-Transformation von Y\n\n&lt;string&gt;:2: RuntimeWarning: invalid value encountered in log\n\nplt.scatter(x, log_y, color='red')\nplt.title('Linearisierte Beziehung')\nplt.xlabel('X')\nplt.ylabel('Log(Y)')\nplt.show()\n\n\n\n\n\n\n\n\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogarithmen sind besonders nützlich, um multiplikative Beziehungen zwischen Variablen zu modellieren. Wenn die Beziehung zwischen zwei Variablen durch eine Multiplikation beschrieben wird, kann die Anwendung des Logarithmus diese Beziehung in eine additive Form überführen, was die Analyse und Interpretation erleichtert. \\[\n  y = a \\cdot x_1^{b_1} \\cdot x_2^{b_2} \\cdots x_n^{b_n}\n\\] Durch die Anwendung des Logarithmus auf beide Seiten der Gleichung erhalten wir: \\[\n  \\log(y) = \\log(a) + b_1 \\cdot \\log(x_1) + b_2 \\cdot \\log(x_2) +\n            \\cdots + b_n \\cdot \\log(x_n)\n\\] Diese Transformation ermöglicht es, die multiplikative Beziehung als lineare Kombination der logarithmierten Variablen darzustellen, was die Anwendung von linearen Regressionsmethoden erleichtert.\nIn der Statistik und Datenanalyse wird diese Technik häufig verwendet, um Modelle zu erstellen, die besser zu den Daten passen und leichter zu interpretieren sind.\nZum Beispiel bei der log-likelihood-Funktion in der Maximum-Likelihood-Schätzung oder bei der Analyse von Wachstumsprozessen, bei denen die Wachstumsraten oft als multiplikative Effekte modelliert werden.\nWenn Statisiker log-likelihood-Funktionen verwenden, dann u.a. aus diesen Gründen:\nMonotonie: Logarithmen sind streng monoton wachsend, was bedeutet, dass die Maximierung der log-likelihood-Funktion äquivalent zur Maximierung der ursprünglichen likelihood-Funktion ist. Dies erleichtert die Optimierung erheblich.\nProdukte wandeln sich zu Summen: Für unabhängige Beobachtungen gilt\n\\[\n  L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta),\n\\]\nwelche mit dem Logarimus zu\n\\[\n  \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\theta)\n\\] wird.\nDies vereinfacht die Berechnungen und reduziert numerische Instabilitäten, die bei der Arbeit mit sehr kleinen Wahrscheinlichkeiten auftreten können.\nAnti-Symmetrie: Beim vergleich zweier Modelle \\(M_1\\) und \\(M_2\\) mit Likelihoods \\(L_1\\) und \\(L_2\\) ist der Log-Likelihood-Ratio\n\\[\n\\begin{aligned}\n  \\log\\left(\\frac{L_1}{L_2}\\right) &= \\log(L_1) - \\log(L_2) \\\\\n                     &= - \\log(L_2) + \\log(L_1) \\\\\n                     &= -\\left(\\log(L_2) - \\log(L_1)\\right) \\\\\n                     = - \\log\\left(\\frac{L_2}{L_1}\\right).\n\\end{aligned}\n\\]\nDas bedeutet, dass der Log-Likelihood-Ratio anti-symmetrisch ist, was bei der Interpretation von Modellen hilfreich sein kann.\nSo ist die Evidenz für Modell \\(M_1\\) gegenüber Modell \\(M_2\\) genau das negative der Evidenz für Modell \\(M_2\\) gegenüber Modell \\(M_1\\).\n\n\n\nRPython\n\n\n\nlibrary(ggplot2)\n\n# Beispiel in R: Multiplikative Beziehung\nset.seed(2009)\nx1 &lt;- runif(100, 1, 10)\nx2 &lt;- runif(100, 1, 10)\ny &lt;- 3 * x1^2 * x2^3 + rnorm(100\n                 , mean = 0, sd = 10)  # Multiplikative Beziehung mit Rauschen\ndf &lt;- data.frame(x1 = x1, x2 = x2, y = y)\nggplot(aes(x = x1, y = y), data = df) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Multiplikative Beziehung\", \n           x = \"X1\", \n           y = \"Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlog_y &lt;- log(y)  # Log-Transformation von Y\nlog_x1 &lt;- log(x1)\nlog_x2 &lt;- log(x2)\ndf_log &lt;- data.frame(x1 = log_x1, x2 = log_x2, y = log_y)\nggplot(aes(x = x1, y = y), data = df_log) +\n  geom_point(color = \"red\") +\n  labs(title = \"Additive Beziehung nach Log-Transformation\", \n           x = \"Log(X1)\", \n           y = \"Log(Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(2009)\nx1 = np.random.uniform(1, 10, 100)\nx2 = np.random.uniform(1, 10, 100)\n\ny = 3 * x1**2 * x2**3 + np.random.normal(\n    0, 10, size=100)  # Multiplikative Beziehung mit Rauschen\n    \nplt.scatter(x1, y, color='blue')\nplt.title('Multiplikative Beziehung')\nplt.xlabel('X1')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\nlog_y = np.log(y)  # Log-Transformation von Y\nlog_x1 = np.log(x1)\nlog_x2 = np.log(x2)\nplt.scatter(log_x1, log_y, color='red')\nplt.title('Additive Beziehung nach Log-Transformation')\nplt.xlabel('Log(X1)')\nplt.ylabel('Log(Y)')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExponentiation und Logarithmen sind mächtige Werkzeuge in der Statistik, die es ermöglichen, Daten zu transformieren, Beziehungen zu modellieren und komplexe Phänomene zu verstehen. Durch die Anwendung dieser mathematischen Konzepte können Statistiker und Datenwissenschaftler tiefere Einblicke in ihre Daten gewinnen und präzisere Modelle erstellen."
  },
  {
    "objectID": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#datentransformation-und-linearisierung-mit-exponentiation-und-logarithmes",
    "href": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#datentransformation-und-linearisierung-mit-exponentiation-und-logarithmes",
    "title": "Warum Exponentiation und Logarithmen wichtig sind in der Statistik.",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(ggplot2)\n\n# Beispiel in R: Log-Transformation einer schiefen Verteilung\nset.seed(2009)\ndata = rexp(1000, rate = 0.1)  # Exponentielle Verteilung\ndf &lt;- data.frame(x = data)\n\nggplot(aes(x = x), data = df) +\n  geom_histogram(bins = 30, fill = \"lightblue\") +\n  labs(title = \"Original Data\") +\n  xlab(\"Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\ndf_log &lt;- data.frame(x = log(data))  # Log-Transformation\n\nggplot(aes(x = x), data = df_log) +\n  geom_histogram(bins = 30, fill = \"lightgreen\") +\n  labs(title = \"Log-Transformed Data\") +\n  xlab(\"Log(Value)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Beispiel in Python: Log-Transformation einer schiefen Verteilung\n\nnp.random.seed(2009)\n\n# Exponentielle Verteilung\ndata = np.random.exponential(scale=10, size=1000)\nplt.hist(data, bins=30, color='lightblue', alpha=0.7)\nplt.title('Original Data')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\n\nlog_data = np.log(data)  # Log-Transformation\nplt.hist(log_data, bins=30, color='lightgreen', alpha=0.7)\nplt.title('Log-Transformed Data')\nplt.xlabel('Log(Value)')\nplt.show()\n\n\n\n\n\n\n\n\nplt.clf()"
  },
  {
    "objectID": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#beispiel-2-linearisierung-mit-logarithmen",
    "href": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#beispiel-2-linearisierung-mit-logarithmen",
    "title": "Warum Exponentiation und Logarithmen wichtig sind in der Statistik.",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(ggplot2)\n\n# Beispiel in R: Linearisierung einer exponentiellen Beziehung\nset.seed(2009)\n\nx &lt;- seq(1, 10, by = 0.1)\ny &lt;- 2 * exp(0.5 * x) + rnorm(length(x),\n                 mean = 0, sd = 5)  # Exponentielle Beziehung mit Rauschen\ndf &lt;- data.frame(x = x, y = y)\nggplot(aes(x = x, y = y), data = df) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Exponentielle Beziehung\", x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlog_y &lt;- log(y)  # Log-Transformation von Y\ndf &lt;- data.frame(x = x, y = log_y)\nggplot(aes(x = x, y = y), data = df) +\n  geom_point(color = \"red\") +\n  labs(title = \"Linearisierte Beziehung\", x = \"X\", y = \"Log(Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Beispiel in Python: Linearisierung einer exponentiellen Beziehung\nnp.random.seed(2009)\n\nx = np.arange(1, 10.1, 0.1)\ny = 2 * np.exp(0.5 * x) + np.random.normal(\n    0, 5, size=len(x) \n    )\nplt.scatter(x, y, color='blue')\nplt.title('Exponentielle Beziehung')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\n\nlog_y = np.log(y)  # Log-Transformation von Y\n\n&lt;string&gt;:2: RuntimeWarning: invalid value encountered in log\n\nplt.scatter(x, log_y, color='red')\nplt.title('Linearisierte Beziehung')\nplt.xlabel('X')\nplt.ylabel('Log(Y)')\nplt.show()\n\n\n\n\n\n\n\n\nplt.clf()"
  },
  {
    "objectID": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#multiplikative-beziehungen",
    "href": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#multiplikative-beziehungen",
    "title": "Warum Exponentiation und Logarithmen wichtig sind in der Statistik.",
    "section": "",
    "text": "Logarithmen sind besonders nützlich, um multiplikative Beziehungen zwischen Variablen zu modellieren. Wenn die Beziehung zwischen zwei Variablen durch eine Multiplikation beschrieben wird, kann die Anwendung des Logarithmus diese Beziehung in eine additive Form überführen, was die Analyse und Interpretation erleichtert. \\[\n  y = a \\cdot x_1^{b_1} \\cdot x_2^{b_2} \\cdots x_n^{b_n}\n\\] Durch die Anwendung des Logarithmus auf beide Seiten der Gleichung erhalten wir: \\[\n  \\log(y) = \\log(a) + b_1 \\cdot \\log(x_1) + b_2 \\cdot \\log(x_2) +\n            \\cdots + b_n \\cdot \\log(x_n)\n\\] Diese Transformation ermöglicht es, die multiplikative Beziehung als lineare Kombination der logarithmierten Variablen darzustellen, was die Anwendung von linearen Regressionsmethoden erleichtert.\nIn der Statistik und Datenanalyse wird diese Technik häufig verwendet, um Modelle zu erstellen, die besser zu den Daten passen und leichter zu interpretieren sind.\nZum Beispiel bei der log-likelihood-Funktion in der Maximum-Likelihood-Schätzung oder bei der Analyse von Wachstumsprozessen, bei denen die Wachstumsraten oft als multiplikative Effekte modelliert werden.\nWenn Statisiker log-likelihood-Funktionen verwenden, dann u.a. aus diesen Gründen:\nMonotonie: Logarithmen sind streng monoton wachsend, was bedeutet, dass die Maximierung der log-likelihood-Funktion äquivalent zur Maximierung der ursprünglichen likelihood-Funktion ist. Dies erleichtert die Optimierung erheblich.\nProdukte wandeln sich zu Summen: Für unabhängige Beobachtungen gilt\n\\[\n  L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta),\n\\]\nwelche mit dem Logarimus zu\n\\[\n  \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\theta)\n\\] wird.\nDies vereinfacht die Berechnungen und reduziert numerische Instabilitäten, die bei der Arbeit mit sehr kleinen Wahrscheinlichkeiten auftreten können.\nAnti-Symmetrie: Beim vergleich zweier Modelle \\(M_1\\) und \\(M_2\\) mit Likelihoods \\(L_1\\) und \\(L_2\\) ist der Log-Likelihood-Ratio\n\\[\n\\begin{aligned}\n  \\log\\left(\\frac{L_1}{L_2}\\right) &= \\log(L_1) - \\log(L_2) \\\\\n                     &= - \\log(L_2) + \\log(L_1) \\\\\n                     &= -\\left(\\log(L_2) - \\log(L_1)\\right) \\\\\n                     = - \\log\\left(\\frac{L_2}{L_1}\\right).\n\\end{aligned}\n\\]\nDas bedeutet, dass der Log-Likelihood-Ratio anti-symmetrisch ist, was bei der Interpretation von Modellen hilfreich sein kann.\nSo ist die Evidenz für Modell \\(M_1\\) gegenüber Modell \\(M_2\\) genau das negative der Evidenz für Modell \\(M_2\\) gegenüber Modell \\(M_1\\).\n\n\n\nRPython\n\n\n\nlibrary(ggplot2)\n\n# Beispiel in R: Multiplikative Beziehung\nset.seed(2009)\nx1 &lt;- runif(100, 1, 10)\nx2 &lt;- runif(100, 1, 10)\ny &lt;- 3 * x1^2 * x2^3 + rnorm(100\n                 , mean = 0, sd = 10)  # Multiplikative Beziehung mit Rauschen\ndf &lt;- data.frame(x1 = x1, x2 = x2, y = y)\nggplot(aes(x = x1, y = y), data = df) +\n  geom_point(color = \"blue\") +\n  labs(title = \"Multiplikative Beziehung\", \n           x = \"X1\", \n           y = \"Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\nlog_y &lt;- log(y)  # Log-Transformation von Y\nlog_x1 &lt;- log(x1)\nlog_x2 &lt;- log(x2)\ndf_log &lt;- data.frame(x1 = log_x1, x2 = log_x2, y = log_y)\nggplot(aes(x = x1, y = y), data = df_log) +\n  geom_point(color = \"red\") +\n  labs(title = \"Additive Beziehung nach Log-Transformation\", \n           x = \"Log(X1)\", \n           y = \"Log(Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(2009)\nx1 = np.random.uniform(1, 10, 100)\nx2 = np.random.uniform(1, 10, 100)\n\ny = 3 * x1**2 * x2**3 + np.random.normal(\n    0, 10, size=100)  # Multiplikative Beziehung mit Rauschen\n    \nplt.scatter(x1, y, color='blue')\nplt.title('Multiplikative Beziehung')\nplt.xlabel('X1')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\nlog_y = np.log(y)  # Log-Transformation von Y\nlog_x1 = np.log(x1)\nlog_x2 = np.log(x2)\nplt.scatter(log_x1, log_y, color='red')\nplt.title('Additive Beziehung nach Log-Transformation')\nplt.xlabel('Log(X1)')\nplt.ylabel('Log(Y)')\nplt.show()\n\n\n\n\n\n\n\nplt.clf()"
  },
  {
    "objectID": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#fazit",
    "href": "posts/2026-02-04-warum-exponentiation-und-logarithmen-wichtig-sind-in-der-statistik/index.html#fazit",
    "title": "Warum Exponentiation und Logarithmen wichtig sind in der Statistik.",
    "section": "",
    "text": "Exponentiation und Logarithmen sind mächtige Werkzeuge in der Statistik, die es ermöglichen, Daten zu transformieren, Beziehungen zu modellieren und komplexe Phänomene zu verstehen. Durch die Anwendung dieser mathematischen Konzepte können Statistiker und Datenwissenschaftler tiefere Einblicke in ihre Daten gewinnen und präzisere Modelle erstellen."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-für-r",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-für-r",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Vorbemerkungen und Notationen",
    "text": "Vorbemerkungen und Notationen\nDa alle t-Verteilungen symmetrisch sind, betrachten wir im wesendlichen nur den positiven Teil.\nZwei reelle Funktionen \\(f\\), \\(g\\) sind genau dann, im Sinne von de Bruijn1 (§1.4), asymptotisch äquivalent \\(f \\sim g\\), wenn\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{f(x)}{g(x)} = 1\n\\]\ngilt.\nIst \\(f \\sim g\\), so können wir auch\n\\[\nf(x) = g(x)\\cdot(1+o(1))\n\\]\ndafür schreiben. Dabei ist \\(h(x) = o(\\phi(x))\\) für \\(x \\to \\infty\\), falls \\(\\lim\\limits_{x \\to \\infty} \\frac{h(x)}{\\phi(x)} = 0\\) gilt. Aus der asymptotischen Äquivalenz von \\(f\\) und \\(g\\) folgt nun direkt:\n\\[\n\\lim\\limits_{x \\to \\infty}\\frac{f(x)}{g(x)}-1 =\\frac{f(x)-g(x)}{g(x)} = 0\n\\]\nMit \\(h(x) = \\frac{f(x)-g(x)}{g(x)}\\) ist \\(h(x) = o(1)\\) und daher \\(f(x)-g(x) = g(x)o(1)\\) und schliesslich \\(f(x) = g(x)+g(x)o(1)\\).\nEin wichtiges Korrolar sagt:\nIst \\(f \\sim g\\), so ist auch \\(\\log(f) \\sim \\log(g)\\)."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung im Allgemeinen",
    "text": "Die t-Verteilung im Allgemeinen\nDie Dichtefunktion der t-Verteilung lauten im Allgemeinen:\n\\[\nf_n(x) = \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}}\\quad \\mathrm{für}\\quad -\\infty &lt; x &lt; +\\infty\n\\]\nwobei wir mit \\(\\Gamma(x)\\) die Gammafunktion\n\\[\n\\Gamma(x)=\\int\\limits_{0}^{+\\infty}t^{x-1}e^{-t}\\operatorname{d}t\n\\]\nbezeichnen. Für einige \\(x\\) nimmt die Gammafunktion leicht zu berechnende Werte an:\nSo ist für alle \\(n\\in\\mathbf{N_0}\\):\n\\(\\Gamma(n+1) = n!\\) und \\(\\Gamma\\left(n + \\frac{1}{2}\\right) = \\frac{(2n)!}{n!4^n}\\sqrt{\\pi}\\)\nmit der gewöhnlichen Fakultät \\(n! = \\prod_{i=0}^n i\\), wobei per Definition \\(0!=1\\) ist."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung mit einem Freiheitsgrad",
    "text": "Die t-Verteilung mit einem Freiheitsgrad\nFür \\(f_1(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n&= \\frac{\\Gamma\\left(\\frac{2}{2}\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-\\frac{2}{2}} \\\\\n&= \\frac{\\Gamma\\left(1\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-1} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{1} {\\sqrt{\\pi} \\cdot \\sqrt{\\pi}} \\cdot \\left(1+x^{2}\\right)^{-1} \\\\\n       &= \\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\n\\end{align*}\n\\]\nDas ist die Dichtefunktion der standardisierten Cauchy-Verteilung\n\\[\nf_{(\\mu,\\lambda)}(x) = \\frac{1}{\\pi} \\cdot \\frac{\\lambda}{\\lambda^2+(x-\\mu)^2}\n\\]\nmit (\\(\\mu = 0\\) und \\(\\lambda=1\\)), welche – bekanntermaßen – keinen Erwartungwert hat.\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_1(k \\cdot x)}{f_1(x)}\n      &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\pi} \\cdot \\frac{1}{1+(kx)^{2}}}{\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}} = \\lim_{x \\to +\\infty} \\frac{1+x^2}{1+k^2x^2} \\\\\n      &=\\lim_{x \\to +\\infty} \\frac{\\frac{1}{x^2}+\\frac{x^2}{x^2}}{\\frac{1}{x^2}+k^2\\frac{x^2}{x^2}} =\\frac{1}{k^2}=k^{-2}\n\\end{align*}\n\\]\nfür alle reellen \\(k&gt;0\\) ist \\(f_1(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -2\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\overline{F}_1(x) = \\int_x^\\infty f_1(t) \\operatorname{d}t = \\frac{1}{\\pi} \\cdot \\int_x^\\infty  \\frac{1}{1+x^{2}} \\operatorname{d}t = \\frac{\\arctan(x)}{\\pi}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt nun:\n\\[\n\\arctan`(x)= \\frac{1}{1+x^2} \\to \\frac{1}{x^2} \\text{ für } x\\to \\infty\n\\]\nGenauer gilt wegen\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{\\frac{1}{1+x^2}}{\\frac{1}{x^2}}\n= \\lim\\limits_{x \\to \\infty} \\frac{x^2}{1+x^2} =1,\n\\]\ndass \\(\\frac{1}{1+x^2} \\sim \\frac{1}{x^2}\\), also asymptotisch äquivalent sind und somit auch \\(\\log\\left(\\frac{1}{1+x^2}\\right) \\sim \\log\\left(\\frac{1}{x^2}\\right)\\).\nZusammen gefasst gilt somit: \\[\n\\log\\left(\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\\right) \\to -2\\log(x) - \\log(\\pi) \\text{ für } x \\to \\infty\n\\]\nSei \\(f_1^*(x) = C \\cdot x^{-\\alpha}\\) mit \\(\\alpha = 2\\) und \\(C=\\frac{1}{\\pi} \\approx0.3183\\).\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 1\n\nf_star &lt;- function(x) {\n  alpha &lt;- 2\n  C &lt;- 1/pi\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie groß ist nun der (absolute) Fehler zwischen \\(f_1^*\\) und \\(f_1\\)?\nEine Grafik von \\(f_1^*-f_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(x**-2 - 1/(1+x**2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nGenauer gilt:\n\\[\nf_1^*(x) - f_1(x) = \\frac{1}{x^2+x^4}\n\\]\nWir können also für ein hinreichend großes \\(x &gt;&gt; 1\\) statt \\(f_1\\) auch \\(f_1^*\\) verwenden und erhalten somit:\n\\[\n\\begin{align*}\n\\overline{F}_1(x) &\\approx \\int_x^\\infty f_1^*(t) \\operatorname{d}t\n  = \\int_x^\\infty  C \\cdot t^{-\\alpha} \\operatorname{d}t \\\\\n  &= \\frac{1}{\\pi} \\cdot \\int_x^\\infty  t^{-2} \\operatorname{d}t\n  = \\frac{1}{\\pi}\\left[\\lim\\limits_{\\epsilon \\to \\infty} \\left(-\\epsilon^{-1}\\right) -\\left(-x^{-1}\\right)\\right]\\\\\n  &= \\frac{1}{\\pi}\\cdot\\left[0 + \\frac{1}{x}\\right] = \\frac{1}{\\pi \\cdot x}\n\\end{align*}\n\\]\nWie hinreichend ist hier hinreichend groß?\nTaleb schreibt an dieser Stelle gerne, dass man jenseits des Karamata-Punktes die Karamata-Konstante anwenden kann. Beides Begriffe, zu denen ich zunächst keine echte Definition gefunden habe.\nJovan Karamata ist der Begründer der langsam variierend Funktionen. 1930 zeite er, dass eine positive stetige Funktion \\(L\\) auf den positiven reellen Zahlen genau dann langsam variierend ist, also für alle \\(t &gt; 0\\) die Bedingung\n\\[\nL(t\\,x)/L(x) \\to 1 \\qquad\\text{für}\\qquad x \\to \\infty\n\\]\nerfüllt, wenn sie für ein \\(a &gt; 0\\) für \\(x &gt; a\\) in der Form\n\\[\nL(x) = c(x) \\cdot \\exp \\left(\\int_a^x\\!\\varepsilon(t)/t \\ dt \\right)\n\\] mit \\(c(x) \\to c &gt; 0\\) und \\(\\varepsilon(x) \\to 0\\) für \\(x \\to \\infty\\) geschrieben werden kann.\nIch vermute also, dass wir \\(L(x)\\) ab dem Punkt \\(a\\) näherungsweise durch \\(c(x)\\) besser sogar durch die Konstante \\(c\\) ersetzen könnten.\nDie Karamata-Konstante ist \\(\\rho = -\\alpha\\), also \\(\\rho = c\\)?\nDer Karamata-Punkt bleibt etwas nebulöser. Es könnte sich hier um den Punkt \\(a\\) handeln und vermutlich könnte man hier so argumentieren:\nWenn die Fehler zwischen \\(f\\) und \\(f^*\\) hinreichend klein ist.\nHierfür könnte man einen absoluten Fehler oder einen relativen Fehler als Maßstab ansehen.\nFür einen relativen Fehler vielleicht \\(\\frac{f^*-f}{x} &lt; k\\)?\nOder man betrachtet hier gleich \\(\\frac{f^*-f}{\\log(x)} &lt; k^*\\)?"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "t-Verteilung mit zwei Freiheitsgeraden",
    "text": "t-Verteilung mit zwei Freiheitsgeraden\nFür \\(f_2(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(\\frac{2}{2}\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(1\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{3}{2}\\right)=\\frac{\\sqrt{\\pi}}{2}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{1}{2\\sqrt{2}} \\cdot \\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n  &= \\frac{1}{\\sqrt[2]{2^3} \\cdot \\sqrt[2]{\\left(1+\\frac{x^{2}}{2}\\right)^3}}  \\\\\n  &= \\frac{1}{(x^2+2)^{\\frac{3}{2}}} \\\\\n  &= \\frac{1}{\\sqrt{(x^2+2)^3}}\n\\end{align*}\n\\]\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_2(k \\cdot x)}{f_2(x)}\n    &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\sqrt{((k\\cdot x)^2+2)^3}}}{\\frac{1}{\\sqrt{(x^2+2)^3}}} = \\lim_{x \\to +\\infty} \\frac{\\sqrt{(x^2+2)^3}}{\\sqrt{((k\\cdot x)^2+2)^3}} \\\\\n    &= \\lim_{x \\to +\\infty} \\left(\\frac{x^2+2}{k^2x^2+2}\\right)^\\frac{3}{2}=\\lim_{x \\to +\\infty} \\left(\\frac{\\frac{x^2}{x^2}+\\frac{2}{x^2}}{k^2\\frac{x^2}{x^2}+\\frac{2}{x^2}}\\right)^\\frac{3}{2} \\\\\n    &=\\left(\\frac{1}{k^2}\\right)^\\frac{3}{2}=\\frac{1}{k^3}=k^{-3}\n\\end{align*}\n\\]\nfür alle reellen \\(k&gt;0\\) ist \\(f_2(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -3\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\begin{align*}\n\\overline{F}_2(x) &= \\int_x^\\infty f_2(t) \\operatorname{d}t = \\int_x^\\infty \\frac{1}{\\sqrt{(t^2+2)^3}} \\operatorname{d}t \\\\\n  &= \\frac{x}{2 \\cdot \\sqrt{x^2+2}}\n\\end{align*}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt für jedes feste \\(k&gt;0\\):\n\\[\n\\begin{align*}\n\\lim\\limits_{x \\to \\infty} \\frac{\\overline{F}_2(k x)}{\\overline{F}_2(x)} &= \\lim\\limits_{x \\to \\infty}k \\cdot \\sqrt{\\frac{x^2+2}{k^2x^2+2}} \\\\\n  &=  k \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{\\frac{1}{k^2} \\cdot \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} \\\\\n  &= \\frac{k}{k} \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{ \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} = 1\\end{align*}\n\\]\nWegen\n\\[\n\\lim_{x \\to \\infty} \\frac{\\frac{1}{\\sqrt{(x^2+2)^3}}}{\\frac{1}{x^3}} =\\lim\\limits_{x \\to \\infty} \\frac{x^3}{(\\sqrt{x^2+2})^3} = \\lim\\limits_{x \\to \\infty} \\left(\\frac{x}{\\sqrt{x^2+2}}\\right)^3= 1\n\\]\nist \\(f_2 \\sim f^*_2\\) und somit auch \\(\\log(f_2) \\sim \\log(f^*_2)\\).\nAus \\(\\log\\left(\\frac{1}{x^3}\\right) = \\log(1)- 3\\cdot\\log(x)\\) können wir daher auf \\(\\alpha = 3\\) und \\(C=1\\) schliesse und schreiben:\n\\[\nf_2^*(x) = C \\cdot x^{-\\alpha} = x^{-3}\n\\]\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 2\n\nf_star &lt;- function(x) {\n  alpha &lt;- 3\n  C &lt;- 1\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie groß ist nun der absolute Fehler zwischen \\(f_2^*\\) und \\(f_2\\) genau?\nEine Grafik zeigt von \\(f_2^*-2_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(f_star(x) - dt(x,df=2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fussnoten",
    "text": "Fussnoten"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nde Bruijn, N. G. (1981), Asymptotic Methods in Analysis, Dover Publications, ISBN 9780486642215↩︎"
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "",
    "text": "In der Welt der Terminal-Emulatoren gibt es viele Optionen, aber nur wenige bieten die Kombination aus Geschwindigkeit, Funktionsreichtum und moderner Technologie wie kitty."
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#einführung",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#einführung",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "",
    "text": "In der Welt der Terminal-Emulatoren gibt es viele Optionen, aber nur wenige bieten die Kombination aus Geschwindigkeit, Funktionsreichtum und moderner Technologie wie kitty."
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#was-ist-kitty",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#was-ist-kitty",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "Was ist kitty?",
    "text": "Was ist kitty?\nKitty ist ein moderner, GPU-basierter Terminal-Emulator, der auf Leistung und Benutzerfreundlichkeit ausgelegt ist. Es nutzt die Grafikprozessoren (GPUs) moderner Computer, um eine flüssige und schnelle Darstellung von Text und Grafiken zu gewährleisten. Dies macht kitty besonders geeignet für Entwickler, die viel Zeit im Terminal verbringen."
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#installation",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#installation",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "Installation",
    "text": "Installation\nDie Installation von kitty ist einfach und kann auf verschiedenen Betriebssystemen durchgeführt werden. Hier sind die grundlegenden Schritte für die Installation auf Linux und macOS:\nLinux/macOS: - Laden Sie das Installationsskript herunter und führen Sie es aus:\n curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin\n\nFügen Sie kitty zu Ihrem PATH hinzu, indem Sie die folgende Zeile zu Ihrer .bashrc oder .zshrc hinzufügen:\n\nexport PATH=\"$HOME/.local/kitty/bin:$PATH\""
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#hauptmerkmale-von-kitty",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#hauptmerkmale-von-kitty",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "Hauptmerkmale von kitty",
    "text": "Hauptmerkmale von kitty\nKitty bietet eine Vielzahl von Funktionen, die es von anderen Terminal-Emulatoren abheben:\n\nGPU-Beschleunigung: Durch die Nutzung der GPU für die Textdarstellung bietet kitty eine außergewöhnliche Leistung, insbesondere bei der Anzeige von großen Textmengen oder komplexen Grafiken.\nTabs und Fenster: Kitty unterstützt mehrere Tabs und Fenster, sodass Sie Ihre Arbeitsumgebung effizient organisieren können.\nGrafikunterstützung: Kitty kann Bilder direkt im Terminal anzeigen, was es ideal für Entwickler macht, die mit Grafiken arbeiten.\nAnpassbare Tastenkombinationen: Sie können Ihre eigenen Tastenkombinationen definieren, um Ihre Produktivität zu steigern.\nRemote-Verbindungen: Kitty unterstützt SSH und andere Remote-Protokolle, sodass Sie nahtlos auf entfernte Systeme zugreifen können.\nLeistungsstarke Konfiguration: Kitty bietet eine umfangreiche Konfigurationsdatei, mit der Sie das Verhalten und Aussehen des Terminals an Ihre Bedürfnisse anpassen können."
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#warum-kitty-verwenden",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#warum-kitty-verwenden",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "Warum kitty verwenden?",
    "text": "Warum kitty verwenden?\nEs gibt mehrere Gründe, warum Sie kitty als Ihren bevorzugten Terminal-Emulator in Betracht ziehen sollten:\n\nLeistung: Die GPU-Beschleunigung sorgt für eine reibungslose und schnelle Benutzererfahrung.\nFunktionsreichtum: Mit einer Vielzahl von Funktionen und Anpassungsmöglichkeiten bietet kitty eine flexible Arbeitsumgebung.\nAktive Entwicklung: Kitty wird aktiv weiterentwickelt, was bedeutet, dass Sie regelmäßig neue Funktionen und Verbesserungen erhalten.\nOpen Source: Kitty ist Open Source, was bedeutet, dass Sie den Quellcode einsehen und anpassen können."
  },
  {
    "objectID": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#fazit",
    "href": "posts/2026-01-30-kitty-der_schnelle_gpu_basierte_terminal_emulator/index.html#fazit",
    "title": "kitty: Der schnelle, feature-reiche, GPU-basierte Terminal-Emulator",
    "section": "Fazit",
    "text": "Fazit\nKitty ist ein leistungsstarker und funktionsreicher Terminal-Emulator, der sich durch seine GPU-Beschleunigung und Anpassungsfähigkeit auszeichnet. Egal, ob Sie ein Entwickler sind, der viel Zeit im Terminal verbringt, oder einfach nur einen schnellen und zuverlässigen Terminal-Emulator suchen, kitty ist definitiv eine Überlegung wert.\nLink: Offizielle kitty-Website"
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html",
    "title": "B-Bäume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "",
    "text": "B-Bäume sind eine Art von selbstbalancierenden Suchbäumen, die in Datenbanken und Dateisystemen weit verbreitet sind. In diesem Beitrag werde ich verschiedene Python-Implementierungen von B-Bäumen vergleichen und ihre Vor- und Nachteile diskutieren."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#einführung",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#einführung",
    "title": "B-Bäume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "",
    "text": "B-Bäume sind eine Art von selbstbalancierenden Suchbäumen, die in Datenbanken und Dateisystemen weit verbreitet sind. In diesem Beitrag werde ich verschiedene Python-Implementierungen von B-Bäumen vergleichen und ihre Vor- und Nachteile diskutieren."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#implementierungen-im-vergleich",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#implementierungen-im-vergleich",
    "title": "B-Bäume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "Implementierungen im Vergleich",
    "text": "Implementierungen im Vergleich\n\nbintrees: Diese Bibliothek bietet eine einfache Implementierung von B-Bäumen und anderen Baumstrukturen. Sie ist leicht zu verwenden und gut dokumentiert, aber möglicherweise nicht die leistungsfähigste Option für große Datenmengen.\nbplustree: Diese Bibliothek implementiert B+-Bäume, eine Variante von B-Bäumen, die für Datenbankanwendungen optimiert ist. Sie bietet gute Leistung und Skalierbarkeit, erfordert jedoch ein tieferes Verständnis der B+-Baum-Struktur.\npybloom-live: Obwohl hauptsächlich für Bloom-Filter gedacht, enthält diese Bibliothek auch eine B-Baum-Implementierung. Sie ist nützlich für Anwendungen, die sowohl Bäume als auch probabilistische Datenstrukturen benötigen.\nbtree: Diese Implementierung ist in der Standardbibliothek von Python enthalten und bietet grundlegende B-Baum-Funktionalitäten. Sie ist einfach zu verwenden, aber möglicherweise nicht so leistungsfähig wie spezialisierte Bibliotheken."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#leistungstest",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#leistungstest",
    "title": "B-Bäume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "Leistungstest",
    "text": "Leistungstest\nUm die Leistung der verschiedenen Implementierungen zu vergleichen, habe ich einen einfachen Test durchgeführt, bei dem ich eine große Anzahl von Einfügungen und Suchvorgängen durchgeführt habe. Hier sind die Ergebnisse:\nimport time\nfrom bintrees import FastRBTree\nfrom bplustree import BPlusTree\n# Testdaten\ndata = list(range(100000))\n# bintrees Test\nbintree = FastRBTree()\nstart_time = time.time()\nfor item in data:\n    bintree.insert(item, item)\nbintree_time = time.time() - start_time\n# bplustree Test\nbplustree = BPlusTree('bplustree.db', order=50)\nstart_time = time.time()\nfor item in data:\n    bplustree.insert(item, item)\nbplustree_time = time.time() - start_time\nprint(f\"bintrees Einfügezeit: {bintree_time} Sekunden\")\nprint(f\"bplustree Einfügezeit: {bplustree_time} Sekunden\")\nDie Ergebnisse zeigten, dass bplustree bei großen Datenmengen eine bessere Leistung erzielte, insbesondere bei Suchvorgängen, während bintrees für kleinere Datensätze ausreichend war."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#fazit",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#fazit",
    "title": "B-Bäume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "Fazit",
    "text": "Fazit\nDie Wahl der richtigen B-Baum-Implementierung hängt stark von den spezifischen Anforderungen Ihres Projekkts ab. Für einfache Anwendungen kann bintrees ausreichend sein, während bplustree besser für komplexe Datenbankanwendungen geeignet ist. Es lohnt sich, die verschiedenen Optionen zu testen, um die beste Leistung für Ihre spezifischen Anwendungsfälle zu erzielen. Unabhängig von der gewählten Implementierung bieten B-Bäume eine effiziente Möglichkeit, große Datenmengen zu verwalten und zu durchsuchen. Ich hoffe, dieser Vergleich hilft Ihnen bei der Auswahl der richtigen B-Baum-Implementierung für Ihre Python-Projekte! Happy Coding! 🚀"
  },
  {
    "objectID": "posts/2025-08-09-brotli-statt-gzip-konfigurieren/index.html",
    "href": "posts/2025-08-09-brotli-statt-gzip-konfigurieren/index.html",
    "title": "Brotli statt Gzip auf dem Server konfigurieren",
    "section": "",
    "text": "Wie mensch einen Webserver davon überzeugt Brotli zu nutzen\nIch habe meinen Webserver von gzip auf brotli umgerüstet. Dachte ich jeden falls, aber alle Test mit\n&gt; curl -I -H 'Accept-Encoding: br' https://sefiroth.de\nschlugen fehl. Nach langer Suche im Internet habe ich herausgefunden, dass zwar die Datei brotli.load in den Verzeichnissen /etc/apache2/mods-enable und /etc/apache2/mods-available existieren, nicht aber die Datei brotli.conf, mit welcher die ganzen Einstellungen gemacht werden.\nFügt mensch nun in diese Datei den Inhalt\n&lt;IfModule mod_brotli.c&gt;\n    AddOutputFilterByType BROTLI_COMPRESS text/html text/plain text/xml text/css text/javascript app\nlication/javascript application/x-javascript application/json application/xml\n    BrotliCompressionQuality 6\n&lt;/IfModule&gt;\nein und startet danach den apache-Server neu, so läuft brotli endlich auch auf dem Server!"
  },
  {
    "objectID": "posts/2021-02-12-ein-paar-gedanken-ueber-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "href": "posts/2021-02-12-ein-paar-gedanken-ueber-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "title": "Ein paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)",
    "section": "",
    "text": "Eine Funktion \\(f(x)\\) heißt potenzgesetzlich, falls\n\\[f(x) = C \\cdot x^a\\]\ngilt, für mindestens alle reellen \\(x &gt; x_{min}\\).\nGewöhnlich setzt man \\(\\alpha = -a\\) und schreibt\n\\[f(x) = C \\cdot x^{-\\alpha}.\\]\nDamit ergibt sich für \\(f'(x)\\) die Form:\n\\[\nf'(x) = -C \\cdot \\alpha \\cdot x^{-\\alpha -1} = C^* \\cdot x^{-(\\alpha + 1)}\n\\] mit \\(C^* = -C \\cdot \\alpha\\).\nEine (streng) potenzgesetzliche Verteilungen (engl. (strong) power-law probability distribution) zur ZV \\(X\\) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}_X(x)=P(X &gt; x)\\) die folgende Gestalt hat: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha}\\]\nMit der Dichte \\(f_X\\) ergibt sich: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha} =\\int_x^\\infty f_X(t) \\text{d}t = C^* \\cdot \\int_x^\\infty  t^{-(\\alpha+1)} \\text{d}t = C^* \\cdot \\int_x^\\infty t^{-\\alpha} \\text{d}t\\]\nAnstelle der Konstanten \\(C\\) tritt oft eine langsam variierende Funktion (engl. slowly varying funktion). Wir erhalten somit die folgende, allgemeinere Definition:\nEine potenzgesetzliche Verteilungen (engl. power-law probability distribution) (zu einer Zufallsvariable \\(X\\)) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}(x)=P(X &gt; x)\\) die folgende Gestalt hat:\n\\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha}\\]\nDabei ist \\(L(x):(x_{\\min}, +\\infty) \\to (x_{\\min}, +\\infty)\\) eine langsam variierende Funktion, also gilt für alle \\(t&gt;0\\):\n\\[\n\\lim_{x \\to +\\infty} \\frac{L(t \\cdot x)}{L(x)} = 1\n\\]\nIst nun wieder \\(f_X\\) die Dichte, so erhalten wir: \\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha} = \\int_x^\\infty f_X(t) dt\\]\n\\[f'(x) = [L(x)x^{-\\alpha}]'\\] \\[\\int_x^\\infty f_X(t) dt= \\int_x^\\infty [L(t)t^{-\\alpha}]' dt\\]\n\\(\\Delta x_0 = h = x_1 - x_0\\) \\(x_1 = x_0 + \\Delta x_0 = x_0 + h\\) \\(x_1 = c \\cdot x_0\\) \\(x_0 + h = c \\cdot x_0 &lt;=&gt; c = 1 + \\frac{h}{x_0}\\) \\(L(x_1) = L(x_0+h) = L(c \\cdot x_0)\\) \\(L(x_1) - L(x_0) = L(c \\cdot x_0) - L(x_0) = L(x_0 + h) - L(x_0)}\\) $\nFakten:\n\nSinnvoll nur, wenn \\(\\alpha &gt; 0\\).\nIst \\(\\alpha &lt; 3\\), dann ist die Varianz und die Schiefe (engl. skewness) (mathematisch) nicht definiert.\nFür \\(k &gt; \\alpha-1\\) ist das k. Moment unendlich.\n\nLogarithmiert man \\(y=f(x)=C \\cdot x^{-\\alpha}\\), so erhält mensch:\n\\[\\log(y) = \\log(C) -\\alpha \\cdot \\log(x)\\]\nIst eine Verteilung potenzgesetzlich, dann kann man \\(\\alpha\\), wie folgt abschätzen:\nSeien \\(x_0, x_1 &gt; x_{min}\\) zwei reelle Zahlen, \\(y_0=f(x_0)\\) bzw. \\(y_1 = f(y_1)\\).\nDann kann mensch wegen\n\\[\\begin{align*}\n  \\log(y_1) - \\log(y_0) &= \\log(C) - \\alpha \\cdot\\log(x_1) - \\log(C) + \\alpha \\cdot \\log(x_0) \\\\\n                        &= \\alpha \\cdot\\left(\\log(x_0)- \\log(x_1) \\right)\n\\end{align*}\\]\nden Wert für \\(\\alpha\\), so kann man mittels\n\\[\\alpha = \\frac{\\log(y_1) - \\log(y_0)}{\\log(x_0)- \\log(x_1)}\\]\nden Wert für \\(\\alpha\\) bestimmen.\nMit dem so ermittelten \\(\\alpha\\), können wir \\(C\\) wegen\n\\[\\log(C) = \\log(y)+ \\alpha\\log(x)\\]\nmit Hilfe von\n\\[C = y \\cdot x_0^\\alpha = f(x_0) \\cdot x_0^\\alpha\\]\nfür ein \\(x_0 &gt; x_{min}\\) abschätzen.\nBeispiel:\nNehmen wir die t-Verteilung mit \\(n=2\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{2}(x)\\).\nDann dann können wir \\(\\alpha=3\\) und \\(C=1\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nNoch ein Beispiel:\nNehmen wir die t-Verteilung mit \\(n=1\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{1}(x)\\).\nDann dann können wir \\(\\alpha=2\\) und \\(C=0.32\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nEin ’Gegen-’Beispiel:\nBetrachten wir nun die (rechte Seite – \\(x&gt;1=x_{min}\\)) einer Gauß’schen Standardnormalverteilung.\nMit den Stützstellen \\(x_0 = 1\\) und \\(x_1 = 5\\) können wir \\(\\alpha=7.46\\) und \\(C=0.24\\) abschätzen. Schauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"norm\",\n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWir erkennen, dass hier etwas nicht passt. Die Standardnormalverteilung ist (vielleicht) keine potenzgesetzich Verteilung?\nEin oft verwendetes Kriterium ist, dass sich die Funktion in der doppelt-logarithmischen Darstellung als Gerade offenbart.\nSchauen wir daher einmal nach:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\n** — **\nWeiter gilt für potenzgesetzliche Verteilungen wegen\n\\[\\frac{f(x)}{f(c\\cdot x)} = \\frac{C \\cdot x^{-\\alpha}}{C \\cdot (c\\cdot x)^{-\\alpha}}\n= c^\\alpha\\]\n\\(f(x)\\) und \\(f(c\\cdot x)\\) für alle (beliebig aber festen) \\(c&gt;0\\) proportional, was man gerne als \\(f(x) \\propto f(c \\cdot x)\\) schreibt.\n** — **\nDie Wahrscheinlichkeit für ein (mindestens) \\(8-\\sigma\\) Ereignis liegt bei einer Standardnormalverteilung bei etwa \\(6.66133814775094\\times 10^{-16}\\).\nBei einer t-Verteilung mit 2 Freiheitsgeraden bei etwa 0.00763403608266899$.\nWährend die Eintrittschance eines (mindestens) \\(8-\\sigma\\) Ereignisses bei der Standardnormalverteilung bei etwa \\(1 : round(1/(1-pnorm(8)),0)\\) liegt, ist diese der t-Verteilung mit einem Freiheitsgrad bei etwa $1 : 131"
  },
  {
    "objectID": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html",
    "href": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html",
    "title": "Behäbige Funktionen aka slowly varying function",
    "section": "",
    "text": "Reelle Funktionen, die ihren Funktionswert kaum ändern, kann man mit Fug und Recht durchaus behäbig nennen, korrekter wäre aber von langsam variierenden Funktionen zu sprechen\nIm Kontext von potenzgesetzlichen Verteilungen kommt der Begriff slowly varying function vor, der Funktionen beschreibt die nur sehr gering auf Änderungen ihres Parameters reagieren.\nDie Definition dieser behäbigen besser langsam variierenden Funktionen stammt von Jovan Karamata:\nEine positive stetige Funktion \\(L\\)1 auf den positiven reelen Zahlen ist langsam variierend (im unendlichen), falls für alle reellen \\(t&gt;0\\)\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = 1\\]\ngilt.\nBeispiele:\nBeweisskizze: Mit \\(L(x) = c\\) ist \\(L(x) = L(t x) = c\\) und damit \\(\\frac{L(t x)}{L(x)}= 1\\).\nBeweisskizze: Da \\(\\lim\\limits_{x \\to +\\infty} L(x) = b = \\lim\\limits_{x \\to +\\infty} L(t\\cdot x)\\) ist \\(\\lim\\limits_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = \\frac{\\lim\\limits_{x \\to +\\infty} L(t\\cdot x)}{\\lim\\limits_{x \\to +\\infty} L(x)} = \\frac{b}{b} = 1\\)\nBeweisskizze: Es gilt: - Für jede reelle Zahl \\(x&gt;0\\) ist \\(\\log_x(x) = 1\\). - Für reelle Zahlen \\(a, b\\) gilt: \\(\\frac{log(a)}{\\log(b)} = \\log_b(a)\\) - Für reelle Zahlen \\(a, b\\) gilt. \\(\\log(a \\cdot b) = log(a) + log(b)\\) - Für jede Konstante \\(k\\) gilt \\(\\lim_{x \\to +\\infty} \\log_x (k) = 0\\) Somit gilt \\(\\frac{\\log_\\beta(k \\cdot x)}{\\log_\\beta(x)} = \\log_x(k\\cdot x) = \\log_x(k) + \\log_x(x) = \\log_x(k) +1 \\to 1\\) wenn \\(x \\to +\\infty\\)\nEine regulär variierende Funktion \\(L:(0,+\\infty) \\to (0,+\\infty)\\) ist eine Funktion für die der Term\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = g(t)\\]\nmit \\(g(t)\\) für alle \\(t&gt;0\\) einen endlichen aber nicht verschwindenen Wert (m.a.W.: \\(g(t) \\neq 0\\)) hat .\nKaramata hat die regulär variierenden Funktionen nun wie folgt charaterisiert:"
  },
  {
    "objectID": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "href": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "title": "Behäbige Funktionen aka slowly varying function",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\n\\(L\\) oder auch \\(l\\) wird (angeblich) hier für den Begriff lente (serb. für faul) verwendet. Behäbig ist also doch nicht so falsch. ;-)↩︎"
  },
  {
    "objectID": "posts/2021-06-27-datenjudo-fuer-frageboegen/index.html",
    "href": "posts/2021-06-27-datenjudo-fuer-frageboegen/index.html",
    "title": "Datenjudo für Fragebögen",
    "section": "",
    "text": "Ab und zu bekomme ich die Frage, wie man einen Fragebogen mit Likert-Scalen-Items auswerten kann.\nDazu kann etwas gezieltes Datenjudo helfen. Wir schauen uns das folgende generierte Mini-Beispiel an:\n\nlibrary(mosaic)  # Basis Paket\nlibrary(tibble)  # Eine modernere Variante der data.frames!\nset.seed(2009)   # Reproduzierbarkeit\n\nN &lt;- 25  # Anzahl der Testzeileneinträge in den \"testdaten\"!\n\n# Wir wollen eine Likert-Scale \nminLikert &lt;- 1  # bis\nmaxLikert &lt;- 6  # erstellen.\n\n# Zum späteren Umrechnen der inversen Items:\nmaxInvItem &lt;- maxLikert + 1\n\n# Wir bauen uns eine Testumfrage mit zwei Itemserien \n# (AS1-AS6 und BS1-BS6) und N Beobachtungen.\n# Die Items AS3, AS4  und BS1 und BS5 sind dabei \n# inverse Items, welche später umgerechnet werden:\ntestdaten &lt;- tibble(\n    ID = 1:N,\n    # AS1-AS6 bilden ein Itemset:\n    AS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS6 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # BS1-BS5 bilden ein Itemset:\n    BS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # Geschlecht als sex mit (1 für Frauen und 2 für Männer)\n    sex = sample(1:2, N, replace = TRUE)\n)\n\n# Orinal testdaten einmal ausgeben:\nhead(testdaten)\n#&gt; # A tibble: 6 × 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     4     4     1     1     2     4     5     5     3     2     3     2\n#&gt; 2     2     2     1     2     2     5     6     4     5     2     2     6     1\n#&gt; 3     3     4     4     6     3     3     4     3     3     4     1     5     1\n#&gt; 4     4     2     6     1     4     5     4     6     4     5     1     3     1\n#&gt; 5     5     3     1     3     5     5     6     6     1     2     6     5     1\n#&gt; 6     6     6     4     1     3     6     6     4     6     5     3     3     1\n\nDie Spalten AS3, AS4 und BS1, BS5 waren inverse Items, die wir noch umrechnen müssen:\n\n# Inverse Item umrechnen:\ntestdaten |&gt;\n    mutate(\n        AS3 = maxInvItem - AS3,\n        AS4 = maxInvItem - AS4,\n        BS1 = maxInvItem - BS1,\n        BS5 = maxInvItem - BS5\n    ) -&gt; testdaten_korrigiert \n\n# Die Daten mit den umgerechnetern inversen Items:\nhead(testdaten_korrigiert)\n#&gt; # A tibble: 6 × 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     1     4     4     6     6     2     4     2     5     3     2     4     2\n#&gt; 2     2     2     1     5     5     5     6     3     5     2     2     1     1\n#&gt; 3     3     4     4     1     4     3     4     4     3     4     1     2     1\n#&gt; 4     4     2     6     6     3     5     4     1     4     5     1     4     1\n#&gt; 5     5     3     1     4     2     5     6     1     1     2     6     2     1\n#&gt; 6     6     6     4     6     4     6     6     3     6     5     3     4     1\n\nDie jeweiligen Itemsets werden nun zur einem Wert (Gesamtscore) zusammengefasst, in dem wir jeweils den Mittelwert von AS1-AS6 und BS1-BS5 bildenund in AS bzw. BS speichern:\n\n# Wir fassen nun die AS1-AS6 und die BS1-BS5 zusammen \n# und bilden die jeweiligen Mittelwerte:\ntestdaten_korrigiert |&gt;\n    group_by(ID, sex) |&gt;  # Damit wird für jede Zeile die Zusammenfassung gemacht!\n    summarise(\n        AS = mean(c(AS1, AS2, AS3, AS4, AS5, AS6)),\n        BS = mean(c(BS1, BS2, BS3, BS4, BS5))\n    ) -&gt; testdaten_sum\n\n# Ausgabe der Mittelwerte der AS und BS\nhead(testdaten_sum)\n#&gt; # A tibble: 6 × 4\n#&gt; # Groups:   ID [6]\n#&gt;      ID   sex    AS    BS\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2  4.33   3.2\n#&gt; 2     2     1  4      2.6\n#&gt; 3     3     1  3.33   2.8\n#&gt; 4     4     1  4.33   3  \n#&gt; 5     5     1  3.5    2.4\n#&gt; 6     6     1  5.33   4.2\n\nDie Datentabelle testdaten_sum enthält nun die Spalten AS und BS mit den entsprechenden Mittelwerten der einzelnen Items AS1-AS6 sowieso BS1- BS5.\nWir wollen nun die Ergebnisse als Boxplots anzeigen lassen. Dafür benennen wir die Geschlechter von 1,2 auf “Frau”, “Mann” um:\n\ntestdaten_sum |&gt;\n    mutate(sex = factor(sex, levels = c(1, 2),\n                             labels = c(\"Frau\", \"Mann\"))\n    ) -&gt; testdaten_sex \n\nNun können wir die Boxplots erstellen:\n\n# Darstellung der Ergebnisse als Boxplot AS ~ sex:\ngf_boxplot(AS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von AS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item AS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(2.5, 4.5)  # Gibt den Bereich von 2.5 bis 4.5 aus!\n    )  \n  )\n\n# Darstellung der Ergebnisse als Boxplot BS ~ sex:\ngf_boxplot(BS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von BS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item BS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(1, 6)  # Gibt den ganzen Bereich von 1 bis 6 aus!\n    )  \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Kennzahlen dazu erhalten wir mit favstats. Dabei wählen wir die ersten sechs Einträge (Variabelbezeichnung und Q0 bis Q4) aus:\n\nfavstats(AS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex      min       Q1   median       Q3      max\n#&gt; 1 Frau 2.166667 3.333333 3.666667 4.166667 5.333333\n#&gt; 2 Mann 3.666667 4.000000 4.333333 4.333333 4.500000\nfavstats(BS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex min  Q1 median  Q3 max\n#&gt; 1 Frau 2.4 2.8    3.4 4.2 4.6\n#&gt; 2 Mann 2.6 3.2    3.3 3.8 4.2\n\nUnter der Verwendung des Pakets likert (https://github.com/jbryer/likert) können wir die Ausgaben auch noch etwas schöner gestalten:\n\nlibrary(likert)\n\n# Wir wählen nur den Itemset BS aus und speichern in in items2:\ntestdaten_korrigiert |&gt;\n  select(\n    starts_with(\"BS\")\n  ) -&gt; items2\n\n# Leider mag likert tibbels nicht so gerne, daher:\nitems2 &lt;- as.data.frame(items2)\n\n# Wir geben den Items noch ein paar Buzzwords:\nnames(items2) &lt;- c(\"Gesundheit\", \"Familie\", \"Geld\", \"Freunde\", \"Langes Leben\")\n\n# Vorbereitung:\nl2 &lt;- likert(items2, nlevels = 5)\n\n# Zusammenfassung\nsummary(l2)\n#&gt;           Item      low   neutral     high     mean       sd\n#&gt; 3         Geld 28.57143 23.809524 47.61905 3.380952 1.359272\n#&gt; 5 Langes Leben 57.14286  4.761905 38.09524 2.619048 1.532194\n#&gt; 1   Gesundheit 38.09524 28.571429 33.33333 2.857143 1.492840\n#&gt; 2      Familie 38.09524 28.571429 33.33333 2.952381 1.499206\n#&gt; 4      Freunde 42.85714 23.809524 33.33333 2.857143 1.236354\n\n# Graphische Ausgaben:\nplot(l2)\n\nplot(l2,\"bar\")\n\nplot(l2,\"heat\")\n\nplot(l2,\"density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoilà!"
  },
  {
    "objectID": "posts/2025-12-22-Das-Lotto-Abbildungs-Problem/index.html",
    "href": "posts/2025-12-22-Das-Lotto-Abbildungs-Problem/index.html",
    "title": "Das Lotto-Abbildungsproblem",
    "section": "",
    "text": "Die Beschreibung\nJeder, der Kombinatorik oder Wahrscheinlichkeitsrechnung studiert hat, kennt das klassische Lotto-Problem: Aus einer Menge von 49 Zahlen werden 6 Zahlen ohne Zurücklegen gezogen. Wie viele verschiedene Kombinationen sind möglich?\nDie Antwort ist bekanntlich: \\[\n  \\binom{49}{6} = \\frac{49!}{6!(49-6)!} = 13\\,983\\,816\n\\]\nAber viele sehen nicht, dass dies auch bedeutet, dass jedes Ziehungsergebnis ein-ein-deutig auf die Zahlen \\(1\\) bis \\(13\\,983\\,816\\) abbildet. Genauer, es gibt (mindestens) eine Abbilung aus \\(\\mathcal{L} = \\{1, 2, \\ldots, 49\\}\\) in \\(\\mathcal{M} = \\{q, 1, 2, \\ldots, 13\\,983\\,816\\}\\), so dass jede 6-elementige Teilmenge von \\(\\mathcal{L}\\) auf ein eindeutiges Element von \\(\\mathcal{M}\\) abgebildet wird.\n\n\nDie Frage\nWie sieht so eine Abbildung aus und vorallem wie lautet die Umkehrabbildung?\n\n\nEine mögliche Lösung\nWir können die Abbildung konstruieren, indem wir die 6 gezogenen Zahlen sortieren und dann eine lexikographische Ordnung verwenden, um jede Kombination zu nummerieren. Angenommen, die gezogenen Zahlen sind \\(a_1 &lt; a_2 &lt; a_3 &lt; a_4 &lt; a_5 &lt; a_6\\). Dann können wir die Abbildung \\(f: \\mathcal{L}^6 \\to \\mathcal{M}\\) wie folgt definieren:\n\\[\nf(a_1, a_2, a_3, a_4, a_5, a_6) = \\sum_{i=1}^{6} \\binom{a_i - 1}{i}\n\\]\nHierbei ist \\(\\binom{n}{k}\\) der Binomialkoeffizient, der die Anzahl der Möglichkeiten angibt, \\(k\\) Elemente aus einer Menge von \\(n\\) Elementen auszuwählen.\nDiese Abbildung ist eindeutig, da jede Kombination von 6 Zahlen eine eindeutige Summe von Binomialkoeffizienten ergibt.\nDie Umkehrabbildung \\(f^{-1}: \\mathcal{M} \\to \\mathcal{L}^6\\) kann durch eine iterative Methode konstruiert werden, bei der wir die größte Zahl \\(a_6\\) bestimmen, die noch in die Summe passt, dann \\(a_5\\), und so weiter, bis wir alle 6 Zahlen gefunden haben.\nHier ist ein Beispiel für die Umkehrabbildung:\n\nSetze \\(n = f(a_1, a_2, a_3, a_4, a_5, a_6)\\).\nFür \\(i\\) von 6 bis 1:\n\nFinde die größte Zahl \\(a_i\\) such that \\(\\binom{a_i - 1}{i} \\leq n\\).\nSetze \\(n = n - \\binom{a_i - 1}{i}\\).\n\nGib die Zahlen \\(a_1, a_2, a_3, a_4, a_5, a_6\\) zurück.\n\nDiese Methode garantiert, dass wir die ursprünglichen gezogenen Zahlen zurückerhalten, da wir die gleiche Logik wie in der Vorwärtsabbildung verwenden, aber in umgekehrter Reihenfolge.\n\n\nFazit\nDie Abbildung des Lotto-Problems zeigt, wie kombinatorische Konzepte verwendet werden können, um eindeutige Zuordnungen zwischen Mengen zu erstellen. Diese Art von Abbildungen ist nicht nur in der Mathematik interessant, sondern findet auch Anwendungen in Bereichen wie Kryptographie und Datenkompression.\n\n\nImplementierung in R und Python\n\nRPython\n\n\n\n# R-Code zur Berechnung der Abbildung und Umkehrabbildung\nlibrary(gmp)\n\n# Funktion zur Berechnung der Abbildung\nlotto_abbildung &lt;- function(a) {\n  t &lt;- sort(unique(a))\n  if (length(t) != 6 || any(t &lt; 1) || any(t &gt; 49)) {\n    stop(\"Eingabe muss 6 eindeutige Zahlen zwischen 1 und 49 sein.\")\n  }\n  l &lt;- sapply(1:6, function(i) chooseZ(a[i] - 1, i), \n              simplify = \"array\", USE.NAMES = FALSE)\n  return(1+l[[1]] + l[[2]] + l[[3]] + l[[4]] + l[[5]] + l[[6]])\n}\n\n# Funktion zur Berechnung der Umkehrabbildung\nlotto_umkehrabbildung &lt;- function(n) {\n  if (n &lt; 1 || n &gt; chooseZ(49, 6)) {\n    stop(\"Eingabe muss zwischen 1 und 13.983.816 liegen.\")\n  }\n  a &lt;- integer(6)\n  for (i in 6:1) {\n    a[i] &lt;- max(which(chooseZ(0:49, i) &lt; n))\n    n &lt;- n - chooseZ(a[i] - 1, i)\n  }\n  return(a)\n}\n\n\n\n\n# Python-Code zur Berechnung der Abbildung und Umkehrabbildung\nfrom math import comb\n\n# Funktion zur Berechnung der Abbildung\ndef lotto_abbildung(a):\n    a = sorted(set(a))\n    if len(a) != 6 or any(x &lt; 1 or x &gt; 49 for x in a):\n        raise ValueError(\"Eingabe muss 6 eindeutige Zahlen zwischen 1 und 49 sein.\")\n    l = [comb(a[i] - 1, i + 1) for i in range(6)]\n    return 1+sum(l)\n\n# Funktion zur Berechnung der Umkehrabbildung\ndef lotto_umkehrabbildung(n):\n    if n &lt; 1 or n &gt; comb(49, 6):\n        raise ValueError(\"Eingabe muss zwischen 0 und 13.983.816 liegen.\")\n    a = [0] * 6\n    for i in range(5, -1, -1):\n        tmp = max(k for k in range(50) if comb(k, i + 1) &lt; n)\n        a[i] = 1 + tmp\n        n -= comb(tmp, i + 1)\n    return a\n\n\n\n\n\n\nBeispiel\n\nRPython\n\n\n\ngezogene_zahlen &lt;- c(5, 12, 23, 34, 45, 49)\nabbildung &lt;- lotto_abbildung(gezogene_zahlen)\numkehrabbildung &lt;- lotto_umkehrabbildung(abbildung)\n\n\n\n\ngezogene_zahlen = [5, 12, 23, 34, 45, 49]\nabbildung = lotto_abbildung(gezogene_zahlen)\numkehrabbildung = lotto_umkehrabbildung(abbildung)\n\n\n\n\nDie Ausgabe zeigt die eindeutige Nummer der gezogenen Zahlen und die zurückgewonnenen Zahlen\n\nRPython\n\n\n\n# Ausgabe\nabbildung  # Eindeutige Nummer\n\nBig Integer ('bigz') :\n[1] 13400040\n\numkehrabbildung  # Zurückgewonnene Zahlen\n\n[1]  5 12 23 34 45 49\n\n\n\n\n\n# Ausgabe\nprint(abbildung)  # Eindeutige Nummer\n\n13400040\n\nprint(umkehrabbildung)  # Zurückgewonnene Zahlen\n\n[5, 12, 23, 34, 45, 49]\n\n\n\n\n\nDie Ausgabe zeigt die eindeutige Nummer der gezogenen Zahlen und die zurückgewonnenen Zahlen.\nDie Frage bleibt, geht es auch ohne (extra) Scheifen bei der Suche nach dem nächsten \\(a_i\\) in der Umkehrabbildung?\n\n\nReferenzen\n\nWikipedia: Lotto (Spiel)\nWikipedia: Binomialkoeffizient\nKombinatorik und Wahrscheinlichkeitsrechnung"
  },
  {
    "objectID": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "href": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "title": "CSV Dateien bearbeiten mit Miller",
    "section": "",
    "text": "Miller beschreibt sich selbst folgendermaßen:\n\nMiller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. You get to work with your data using named fields, without needing to count positional column indices.\n\nMiller kombiniert die Funktionalität von awk, sed und cut und eignet sich besonders für feldbasierte Datenmanipulation.\n\nBeispiel 1: Erstellung einer Markdown-Tabelle\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --omd cat\nMit diesem Befehl wird eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, anschließend werden alle Kommata durch Punkte ersetzt und schließlich wird daraus eine Markdown-Tabelle erzeugt.\n\n\nBeispiel 2: Gerahmte Darstellung\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --opprint --barred cat\nMit diesem Befehl wird ebenfalls eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, alle Kommata werden durch Punkte ersetzt und am Ende wird eine eingerahmte Tabelle erzeugt."
  },
  {
    "objectID": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html",
    "href": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html",
    "title": "Quarto-Dokumente mit WebP anstatt PNG Bilddateien erstellen",
    "section": "",
    "text": "Ich bin über die Quarto-Extension von Klaus Brunner mit dem Namen Convert PNG to WebP (Quarto Extension) gestolpert und nutze diesen nun regelmässig.\nDafür habe ich auf meinem Mac via HomeBrew die Application “webp” installiert.\n&gt; brew install webp\nDadurch habe ich das Programm cwebp installiert. Es dient zum umwandeln von PNG in webp Dateien. Zum Testen nutze ich den Befehl:\n&gt; cwebp -version\nAktuell bekomme ich damit die Ausgabe:\n1.6.0\nlibsharpyuv: 0.4.2\nDas Programm ersetzt die erzeugten Bild-Ausgaben, welche normalerweise im PNG-Format erzeugt werden, durch Neue im WebP-Format.\nDie KI sagt:\n\nWebP ist ein modernes Bildformat von Google, das eine effizientere Komprimierung als herkömmliche Formate wie JPEG und PNG ermöglicht, um Bilder im Web kleiner zu machen und Ladezeiten zu beschleunigen. Es unterstützt sowohl verlustfreie als auch verlustbehaftete Komprimierung sowie Transparenz und Animationen, ähnlich wie PNG und GIF. WebP wird von den meisten modernen Browsern unterstützt, aber für ältere Browser ist es ratsam, eine alternative Bildversion bereitzustellen\n\n\n\nUm sofort bei der Erstellung von Dokumenten, wie zum Beispiel diese Blog-Einträge, mit Quarto webp zu nutzen, installiert mensch einfach im Quarto-Projekt mittels:\n&gt; quarto add klausbrunner/convert-to-webp\nDanach in der Datei _quarto.yml noch die folgenden Zeilen hinzufügen:\nfilters:\n  - convert-to-webp\n\nwebp-delete-originals: false\nwebp-disable: false\nJetzt wird nach jedem ‘rendern’ eines Dokuments die erzeugten PNG-Dateien in WebP-Dateien übersetzt und im HTML-Code entsprechend ersetzt."
  },
  {
    "objectID": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html#installation-im-quarto-projekt",
    "href": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html#installation-im-quarto-projekt",
    "title": "Quarto-Dokumente mit WebP anstatt PNG Bilddateien erstellen",
    "section": "",
    "text": "Um sofort bei der Erstellung von Dokumenten, wie zum Beispiel diese Blog-Einträge, mit Quarto webp zu nutzen, installiert mensch einfach im Quarto-Projekt mittels:\n&gt; quarto add klausbrunner/convert-to-webp\nDanach in der Datei _quarto.yml noch die folgenden Zeilen hinzufügen:\nfilters:\n  - convert-to-webp\n\nwebp-delete-originals: false\nwebp-disable: false\nJetzt wird nach jedem ‘rendern’ eines Dokuments die erzeugten PNG-Dateien in WebP-Dateien übersetzt und im HTML-Code entsprechend ersetzt."
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html",
    "href": "posts/2020-06-29-cook-abstand/index.html",
    "title": "Cook Abstand",
    "section": "",
    "text": "Frage: Was macht einen Wert zum Ausreißer?\nEine mögliche Antwort lautet: Ein Wert gilt als Ausreißer, wenn er deutlich von den übrigen Werten abweicht und einen (starken) Einfluss auf das Modell ausübt.\nEin Verfahren zur Identifikation solcher Ausreißer ist der Cook-Abstand (engl.: Cook’s distance). Die zugrunde liegende Idee besteht darin zu messen, wie stark ein einzelner Wert das Modell beeinflusst. Dazu vergleicht man das Modell einmal mit und einmal ohne diesen Wert.\nSehen wir uns den Cook-Abstand anhand eines einfachen linearen Regressionsmodells näher an:"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#unser-modell",
    "href": "posts/2020-06-29-cook-abstand/index.html#unser-modell",
    "title": "Cook Abstand",
    "section": "Unser Modell:",
    "text": "Unser Modell:\nZuerst ein Streudiagramm zur Visualisierung der Daten:\n\ngf_point(tip ~ total_bill, data = tips)\n\n\n\n\n\n\n\n\nDann erstellen wir ein lineares Regressionsmodell:\n\nerg_lm &lt;- lm(tip ~ total_bill, data = tips)\nsummary(erg_lm)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1982 -0.5652 -0.0974  0.4863  3.7434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.920270   0.159735   5.761 2.53e-08 ***\ntotal_bill  0.105025   0.007365  14.260  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.022 on 242 degrees of freedom\nMultiple R-squared:  0.4566,    Adjusted R-squared:  0.4544 \nF-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16\n\n\nVisualisierung der Regressionsgeraden:\n\ngf_point(tip ~ total_bill, data = tips) %&gt;%\n  gf_coefline(\n    model = erg_lm,\n    color = ~ \"Regressionsgerade\",\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nEinflussreiche Ausreißer können bei linearen Modellen problematisch sein. Was passiert, wenn wir einen potenziellen Ausreißer entfernen?\nBeispiel: Wir eliminieren die Beobachtung mit dem Index 173\n\ntips %&gt;% slice(173) -&gt; tips_removed\ntips_removed\n\n  total_bill  tip\n1       7.25 5.15\n\n\n\ntips %&gt;% slice(-173) -&gt; tips_red\nerg_lm_red &lt;- lm(tip ~ total_bill, data = tips_red)\nsummary(erg_lm_red)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips_red)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2136 -0.5351 -0.0818  0.4951  3.6869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.86065    0.15709   5.479 1.08e-07 ***\ntotal_bill   0.10731    0.00723  14.843  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9992 on 241 degrees of freedom\nMultiple R-squared:  0.4776,    Adjusted R-squared:  0.4754 \nF-statistic: 220.3 on 1 and 241 DF,  p-value: &lt; 2.2e-16\n\n\nGrafischer Vergleich:\n\ngf_point(tip ~ total_bill, data = tips_red) %&gt;%\n  gf_coefline(\n    model = erg_lm,\n    color = ~ \"Regressionsgerade\"\n    ) %&gt;%\n  gf_point(\n    tip ~ total_bill, \n    colour = ~ \"Entfernter Punkt\",\n    data = tips_removed)"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#berechnung-des-cook-abstands",
    "href": "posts/2020-06-29-cook-abstand/index.html#berechnung-des-cook-abstands",
    "title": "Cook Abstand",
    "section": "Berechnung des Cook-Abstands",
    "text": "Berechnung des Cook-Abstands\nWir vergleichen die Prognosen beider Modelle:\n\nnew_data &lt;- tibble(total_bill = tips$total_bill)\nprognose_lm &lt;- predict(erg_lm, newdata = new_data)\nprognose_lm_red &lt;- predict(erg_lm_red, newdata = new_data)\n\nBerechnung:\n\\[\nd_j = \\sum_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2\n\\] Dabei ist \\(\\hat{y}_i\\) die Prognose des Wertes \\(y_i\\) auf Basis von \\(x_i\\) mit dem Originalmodell und \\(\\hat{y}_{i(j)}\\) die Prognose wenn man die \\(j\\)-te Beobachtung aus dem Modell gestrichen hat.\n\nd_j &lt;- sum((prognose_lm - prognose_lm_red)^2)\nd_j\n\n[1] 0.1511406\n\n\nDer Cook Abstand \\(D_j\\) wird nun noch normiert durch \\[{\\text{var}_{\\text{cook}}} = p \\cdot s_{\\epsilon_i^2}^2\\] Dabei ist \\(s_{\\epsilon_i^2}^2\\) der erwartungstreue Schätzer der Varianz der Residuen und \\(p\\) die Anzahl aller erklärenden Variablen plus Eins, also: $ 1 + 1 = 2$.\nNormierung des Cook-Abstands:\n\\[\n  D_j = \\frac{d_j}{\\text{var}_{\\text{cook}}} = \\frac{\\sum\\limits_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2}{p \\cdot s_{\\epsilon_i^2}^2}\n\\]\n\n# Summary des Modells\nselm &lt;- summary(erg_lm)\n\n# Wir finden p als rank im Modell\np &lt;- erg_lm$rank \n\n# Wir finden den erwatungtreuen Schätzer im Summary des Modells\ns_quad_eps_quad &lt;- (selm$sigma)^2 \n\nvar_cook = p * s_quad_eps_quad\n\nD_j = d_j / var_cook\nD_j\n\n[1] 0.07234504\n\n\nAlternativ kann der Wert direkt mit cooks.distance() berechnet werden:\n\ncooks.distance(erg_lm)[173]\n\n       173 \n0.07234504 \n\n\nWann aber ist nun ein Wert ein einflussreicher Ausreißer?"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#entscheidungskriterien",
    "href": "posts/2020-06-29-cook-abstand/index.html#entscheidungskriterien",
    "title": "Cook Abstand",
    "section": "Entscheidungskriterien",
    "text": "Entscheidungskriterien\nCook selber gibt dafür die Bedingung \\(D_j &gt; 1\\) an. Andere Autor*innen schreiben \\(D_j &gt; 4/n\\), wobei \\(n\\) die Anzahl der Beobachtung ist.\nIn unserem Beispiel liefert die Variante \\(D_j &gt; 1\\)\n\ncooks &lt;- cooks.distance(erg_lm)\nnames(cooks) &lt;- NULL\nn &lt;- nrow(tips)\n\nany(cooks &gt; 1)\n\n[1] FALSE\n\n\nkeinen Ausreißer.\nWenn wir jedoch mit \\(D_j &gt; 4/n\\) suchen .\n\nany(cooks &gt; 4/n)\n\n[1] TRUE\n\n\ndann gibt es Ausreißer.\nDie Indices dieser finden wir mit:\n\nwhich(cooks &gt; 4/n)\n\n [1]  24  48  57 103 142 157 171 173 179 183 184 185 188 208 211 213 215 238\n\n\nDaten bereinigen:\n\nremove &lt;- which(cooks &gt; 4/n)\ntips %&gt;% slice(-remove) -&gt; tips_no_outliers\ntips %&gt;% slice(remove) -&gt; tips_removed\nerg_lm_no_outliers &lt;- lm(tip ~ total_bill, data = tips_no_outliers)\n\nErgebnis des Modells:\n\nsummary(erg_lm_no_outliers)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips_no_outliers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.22592 -0.48166 -0.06794  0.46992  2.31414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.773324   0.139435   5.546  8.2e-08 ***\ntotal_bill  0.111799   0.006958  16.069  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7778 on 224 degrees of freedom\nMultiple R-squared:  0.5355,    Adjusted R-squared:  0.5334 \nF-statistic: 258.2 on 1 and 224 DF,  p-value: &lt; 2.2e-16\n\n\nDaten bereinigen:\n\ngf_point(tip ~ total_bill, data = erg_lm_no_outliers) %&gt;%\n  gf_coefline(\n    model = erg_lm_no_outliers, \n    color = ~\"Regressionsgerade\"\n  )\n\n\n\n\n\n\n\n\nGrafischer Vergleich beider Modelle:\n\ngf_point(tip ~ total_bill, data = erg_lm) %&gt;%\n  gf_coefline(\n    model =  erg_lm,\n    color = ~ \"Regressionsgerade (Orginal)\"\n  ) %&gt;%\n  gf_coefline(\n    model = erg_lm_no_outliers,\n    color = ~ \"Regressionsgerade (No Outliers)\"\n  ) %&gt;%\n  gf_point(\n    tip ~ total_bill,\n    color = ~ \"Entfernte Punkte\",\n    data = tips_removed\n  )"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#modelle-in-gleichungsform",
    "href": "posts/2020-06-29-cook-abstand/index.html#modelle-in-gleichungsform",
    "title": "Cook Abstand",
    "section": "Modelle in Gleichungsform",
    "text": "Modelle in Gleichungsform\nDas ursprüngliche Modell:\n\\[\n  \\widehat{tips}_{lm} = 0.9202696 + 0.1050245 \\cdot total\\_bill\n\\]\nDas um pot. Ausreißer bereinigte Modell:\n\\[\n  \\widehat{tips}_{lm\\_no} 0.7733236 + 0.1117985 \\cdot total\\_bill\n\\]"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html",
    "title": "Regression mit studentisierten Daten",
    "section": "",
    "text": "Bei einer einfachen linearen Regression versuchen wir zu vorgegebenen Datenpunkten \\((x_1, y_1), \\cdots (x_n, y_n)\\) die Parameter einer möglichst passenden Gerade \\(g(x)=\\beta_0 + \\beta_1 \\cdot x\\) zu schätzen.\nDie Schätzung des y-Achsenabschnitts \\(\\hat\\beta_0\\) und der Steigung \\(\\hat\\beta_1\\) erfolgt dabei algebraisch exakt mittels:\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1 \\cdot \\bar{x} \\quad\\text{und}\\quad \\hat\\beta_1 = \\frac{s_x}{s_y}\\cdot r_{x,y}\n\\]\nDabei sind \\(\\bar{x}\\) bzw. \\(\\bar{y}\\) die Mittelwerte und \\(s_x\\) bzw. \\(s_y\\) die Standardabweichungen der Datenpunkte \\(x_i\\) bzw. \\(y_i\\); darüberhinaus ist \\(r_{x,y}\\) der Korrelationskoeffizient der Datenpunkte.\nBeim studentisieren werden die Datenpunkte bzgl. des Mittelwertes zentriert und bzgl der Standardabweichung normiert:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x} \\quad\\text{bzw.}\\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}\n\\]\nWas passiert nun durch eine solche Studentisierung (oft auch z-Transformation genannt) mit den geschätzen Parametern?\nDie Mittelwerte \\(\\bar{x}^{stud}\\) und \\(\\bar{y}^{stud}\\) werden zu Null. Die Standardabweichungen \\(s_{x^{stud}}\\) und \\(s_{y^stud}\\) werden zur Eins:\n\\[\n\\bar{x}^{stud}=0=\\bar{y}^{stud} \\qquad s_{x^{stud}}= 1 = s_{y^{stud}}\n\\]\nDer y-Achsenabschnitt wird nun durch\n\\[\n\\hat\\beta_0^{stud}\n= \\bar{y}^{stud} - \\hat\\beta_1^{stud} \\cdot \\bar{x}^{stud}\n= 0 - \\hat\\beta_1^{stud} \\cdot 0 = 0\n\\]\nund die Steigung durch\n\\[\n\\hat\\beta_1^{stud}\n= \\frac{s_{x^{stud}}}{s_{y^{stud}}}\\cdot r_{x^{stud},y^{stud}}\n= \\frac{1}{1}\\cdot r_{x^{stud},y^{stud}} = r_{x^{stud},y^{stud}}\n\\]\ngeschätzt.\nFür den Korrelationskoeffienten gilt nun\n\\[\nr_{x^{stud},y^{stud}}\n= \\frac{s_{x^{stud},y^{stud}}}{s_{x^{stud}}\\cdot_{y^{stud}}}\n= \\frac{s_{x^{stud},y^{stud}}}{1 \\cdot 1}\n= s_{x^{stud},y^{stud}}.\n\\]\nDamit Schätzen wir unsere Steigung \\(\\hat\\beta_1^{stud}\\) direkt aus der Kovarianz \\(s_{x^{stud},y^{stud}}\\).\nDamit gilt:\n\\[\n\\hat\\beta_1^{stud} = r_{x^{stud},y^{stud}} = s_{x^{stud},y^{stud}} \\in [-1, 1]\n\\]\nIn Worten zusammengefasst: Im studentisierten Fall ist"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#reproduzierbarkeitsinformationen",
    "title": "Regression mit studentisierten Daten",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#footnotes",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#footnotes",
    "title": "Regression mit studentisierten Daten",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nDas “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎\nSie können hier auch die Funktion scale() verwenden!↩︎"
  },
  {
    "objectID": "posts/2021-06-11-wege-zur-normalverteilung/index.html",
    "href": "posts/2021-06-11-wege-zur-normalverteilung/index.html",
    "title": "Wege zur Normalverteilung",
    "section": "",
    "text": "Der fairen Wurf einer fairen Münze, also eine Münze bei der Kopf und Zahl gleich wahrscheinlich geworfen wird, sei der Ausgang des ersten Weges.\nWir können den Münzwurf mit R simulieren:\nlibrary(mosaic)\nset.seed(2009)\n\nrflip(1)\n\n\nFlipping 1 coin [ Prob(Heads) = 0.5 ] ...\n\nT\n\nNumber of Heads: 0 [Proportion Heads: 0]\nGenauso wie den Wurf zweier Münzen:\nrflip(2)\n\n\nFlipping 2 coins [ Prob(Heads) = 0.5 ] ...\n\nH H\n\nNumber of Heads: 2 [Proportion Heads: 1]\nOder auch von 20 Münzwürfen:\nrflip(20)\n\n\nFlipping 20 coins [ Prob(Heads) = 0.5 ] ...\n\nT T T T H H T T T H H H H T H T T H H H\n\nNumber of Heads: 10 [Proportion Heads: 0.5]\nWir wollen uns dafür interessieren, wie der Zufall auf jeweils \\(n\\) Münzwürfe einwirkt\nUnd wieder holen dafür die drei Experiment jeweils \\(N=10^{4}\\) mal und schauen uns danach anwie die Anzahl der Kopf Würfe variiert:\nN &lt;- 10000\nn &lt;- 1\nvrtlg_1 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_1) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5,n + 0.5)))\n\\(n=2\\)\nn &lt;- 2\nvrtlg_2 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_2) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\n\\(n=20\\)\nn &lt;- 20\nvrtlg_20 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_20) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\n\\(n=50\\)\nn &lt;- 50\nvrtlg_50 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_50) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\nDer faire Wurf eine fairen Münze \\(X\\) ist aber vorallem ein Gedanken-Expermiment, bei dem wir davon ausgehen, dass die Wahrscheinlichkeit für Kopf gleich der Wahrscheinlichkeit für Zahl ist:\n\\[\nP(X = \\text{\"Kopf\"}) = P(X = \\text{\"Zahl\"}) = 50\\,\\% = 0{,}5\n\\]\nWir wollen die beiden Ergebnisse kodieren: \\(\\text{\"Kopf\"}\\) mit \\(1\\) und \\(\\text{\"Zahl\"}\\) mit \\(0\\). Somit können wir schreiben:\n\\[\nP(X = 0)  = 0{,}5 = P(X = 1)\n\\]\nFür denn Fall, dass die Münze nicht mehr fair ist wollen wir vereinbaren, dass wir mit \\(q = P(X = 0)\\) und \\(p = P(X = 1)\\) die jeweiligen Wahrscheinlichkeiten bezeichnen wollen. Es gilt aber immer, dass \\(q+p = 1\\) ist!\nEine Variable \\(X\\) die dem Zufall ein Wert \\(x\\) zuweist, wollen wir Zufallsvariable nennen.1\nAusgehen von der Annahme können wir uns diese theoretischen Verteilungen auch einmal ansehen:\np = 0.5\nn &lt;- 1\ngf_dist(\"binom\", size = n, prob = p)\nn &lt;- 2\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_2)\ndist_2 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_2, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 20\ngf_dist(\"binom\", size = n, prob = p)\nn &lt;- 20\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_20)\ndist_20 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_20, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 50\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_50)\ndist_50 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = 0.5) %&gt;%\n  gf_point(density ~ x, data = dist_50, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 100\nN &lt;- 5 * n**2\nvrtlg_100 &lt;- do(N) * rflip(n)\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_100)\ndist_100 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_100, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 300\nN &lt;- 5 * n**2\nvrtlg_300 &lt;- do(N) * rflip(n)\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_300)\ndist_300 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_300, color = \"lightgreen\", alpha = 0.7)\nZeichnen wir nun die Gauß’sche Glockenkurve in rot dazu:\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_300, color = \"lightgreen\", alpha = 0.7) %&gt;%\n  gf_dist(\"norm\", mean &lt;- n*p, sd = sqrt(n*p*(1 - p)), color = \"red\")\nDie Gauß’sche Glockenkurve ist die Dichtefunktion der Normalverteilung und ist definiert durch:\n\\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\nMit den beiden Parameter \\(\\mu\\) und \\(\\sigma^2\\) kann man den Mittelwert der Verteilung (auch Erwartungswert genannt) und die Varianz der Verteilung einstellen.\nWir haben diese Werte oben mit den theoretischen Werten der Binomialverteilung \\(\\mu = E[X] = p \\cdot n\\) und \\(\\sigma = \\sqrt{\\sigma^2}= \\sqrt{Var[X]} = \\sqrt{n \\cdot p \\cdot (1-p)}\\) belegt.\nWir sehen, die (simulierten) relativen Häufigkeiten der Münzwürfe streben mit steigendem \\(N\\) mehr und mehr in Richtung der (theoretischen) Wahrscheinlichkeiten der Binomialverteilung und diese (mit steigendem \\(n\\)) gegen die Gauß’sche Glockenkurve der Normalverteilung.\nEine Verteilungsfunktion \\(F(x)\\) gibt an, wie wahrscheinlich es ist, einen Wert \\(\\leq x\\) zu beobachten:\n\\[\nF(x) = P(X \\leq x)\n\\]\nNatürlich ist damit immer \\(0 \\leq F(x) \\leq 1\\).\nEine empirische Verteilungsfunktion \\(F_n(x)\\)gibt an, wie groß die relative Häufigkeit des eintretens von Werten \\(\\leq x\\) bei einem Stichprobenumfang von \\(n\\) waren: \\[F_n(x) = \\frac{\\text{Anzahl der Werte} \\leq x}{n}\\]\nBetrachen wir nun empirische Verteilungsfunktion unserer Experimente:\nn = 2\ngf_ecdf( ~ heads, data=vrtlg_2)\nn = 20\ngf_ecdf( ~ heads, data=vrtlg_20)\nn = 50\ngf_ecdf( ~ heads, data=vrtlg_50)\nn = 300\ngf_ecdf( ~ heads, data=vrtlg_300)\nTragen wir zur empirische Verteilungsfunktion auch die Verteilungsfunktion der Normalverteilung von oben ein:\nn &lt;- 300\ngf_ecdf( ~ heads, data = vrtlg_300) %&gt;%\n  gf_dist(\"norm\", mean &lt;- n*p, sd = sqrt(n*p*(1 - p)), kind=\"cdf\", color = \"red\")\nDie empirische Verteilungsfunktion strebt also gegen die (theoretisch) Verteilungsfunktion der Normalverteilung. Sie lautet:\n\\[\nF(x) = \\int_{-\\infty}^x f(u)\\, \\text{d} u = \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "posts/2021-06-11-wege-zur-normalverteilung/index.html#footnotes",
    "href": "posts/2021-06-11-wege-zur-normalverteilung/index.html#footnotes",
    "title": "Wege zur Normalverteilung",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nEigentlich handelt es sich damit strenggenommen um eine Funktion. Und es müssen noch weitere Eigenschaften erfüllt sein. Aber darauf gehen wir hier nicht weiter ein.↩︎"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "Ein Beispiel:",
    "text": "Ein Beispiel:\nNehmen wir drei Verteilungen mit Zufallsvariable \\(U\\), \\(X\\), \\(Y\\) und jeweils \\(n\\) Realisationen \\(u_1,\\dots, u_n\\), \\(x_1,\\dots, x_n\\), \\(y_1,\\dots, y_n\\).\nWählen wir zunächst \\(n=5\\):\n\nu\n\n[1] 19.726 69.683 60.790  0.955 42.901\n\nx\n\n[1]  7.942 15.905 12.917  6.818  4.434\n\ny\n\n[1] 59.961 56.552 51.094 75.288 47.985\n\n\nStandardisieren wir die Werte:\n\nlibrary(mosaic)\nzscore(u)\n\n[1] -0.6695256  1.0830283  0.7710507 -1.3280357  0.1434823\n\nzscore(x)\n\n[1] -0.3543069  1.3440714  0.7067796 -0.5940379 -1.1025063\n\nzscore(y)\n\n[1]  0.1677971 -0.1526624 -0.6657361  1.6085958 -0.9579944\n\n\nDie Behauptung des Zentralengrenzwertsatzes ist nun, dass mit steigender Anzahl an Werten \\(n\\) die standardisierten Werte in der empirischen Verteilungsfunktion sich immer mehr der Verteilungsfunktion der Standardnormalverteilung annähern:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeiterführende Literatur und Quellen dieses Eintrags:"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "",
    "text": "Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine möglichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt. Dabei tolerieren wir eine (kleine) Abweichung \\(e_i\\).\nBei einer einfachen linearen Regression gehen wir davon aus, dass die Datenpunkte (im wesentlichen) auf einer Geraden liegen. Mit \\(g(x)=\\beta_0 + \\beta1 \\cdot x\\) ergibt sich dann für die Datenpunkte die Gleichung:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i\\]\nUnsere Aufgabe besteht nun darin die Parameter \\(\\beta_0\\) (y-Achsenabschnitt) und \\(\\beta_1\\) (Steigung) an Hand der \\(n\\) Datenpunkte zu schätzen. Alle unsere Schätzungen kennzeichnen wir mit einem Dach (\\(\\hat{.}\\)), um sie von den (in der Regel unbekannten) Parametern besser zu unterscheiden.\nWir suchen somit nach \\(\\hat\\beta= \\left(\\hat\\beta_0,\\, \\hat\\beta_1\\right)\\), so dass die Gerade \\(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x\\) zu gegebenem \\(x_i\\) eine möglichst gute Schätzung von \\(y_i\\) (genannt \\(\\hat{y}_i\\)) hat:\n\\[\n\\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i\n\\]\nDie Abweichung \\(\\hat{e_i}\\) unserer Schätzung \\(\\hat{y}_i\\) von dem gegebenen Wert \\(y_i\\) lässt sich schreiben als:\n\\[\n\\hat{e_i} =  \\hat{y_i} - y_i =  \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i\n\\]\nWenn wir diese Abweichung über alle \\(i\\) minimieren, finden wir unser \\(\\hat\\beta\\).\nDoch das wirft eine Frage auf: Wie genau messen wir die möglichst kleinste Abweichung der \\(\\hat{e_i}\\) konkret?\nWir betrachten zunächst drei einfache Ideen:\nGewöhnlich nutzen wir die quadratischen Abweichungen, weshalb wir die drei Ideen ebenso in umgekehrter Reihenfolge betrachten wollen:"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "3. Idee: Summe der quadratischen Abweichungen",
    "text": "3. Idee: Summe der quadratischen Abweichungen\nWir bezeichnen mit\n\\[\\begin{aligned}\nQS &= QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n  &= \\sum\\limits_{i=1}^n \\hat{e_i}^2 = \\sum\\limits_{i=1}^n \\left(\\hat{y_i} - y_i \\right)^2 \\\\\n  &= \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\n\\end{aligned}\\]\ndie Quadrat-Summe der Abweichungen.\nGesucht wird \\(\\hat\\beta=\\left(\\hat\\beta_0,\\,\\hat\\beta_1\\right)\\), so das \\(QS\\) minimiert wird.\nDies ist ein Minimierungsproblem, bei dem wir zu mindestens eine (exakte) mathematisch-algebraisch Lösung in Form eines stationären Punktes finden können. Dazu berechnen wir die Nullstelle der ersten partiellen Ableitung von \\(QS\\) nach \\(\\hat\\beta_0\\) bzw. \\(\\hat\\beta_1\\).\n\nVorbemerkungen\nWegen \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i\\) ist \\(n \\cdot \\bar{x} =\\sum\\limits_{i=1}^n x_i\\) und analog \\(n \\cdot \\bar{y} =\\sum\\limits_{i=1}^n y_i\\)\n\n\nSchätzen des y-Achenabschnitts \\(\\hat\\beta_0\\)\nEs ist:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot 1 \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 + \\sum\\limits_{i=1}^n\\hat\\beta_1 \\cdot x_i - \\sum\\limits_{i=1}^n y_i\\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot\\sum\\limits_{i=1}^n x_i - \\sum\\limits_{i=1}^n y_i \\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot n \\cdot \\bar{x} - n \\cdot\\bar{y} \\right) \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right)\n\\end{aligned}\\]\nUm stationäre Punkte zu ermitteln, müssen wir den Ausdruck nun gleich Null setzen und erhalten:\n\\[\\begin{aligned}\n  0 &= \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\qquad | : (2 \\cdot n) \\\\\n  &= \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y}\n\\end{aligned}\\]\nStellen wir nach \\(\\hat\\beta_0\\) um, erhalten wir:\n\\[\\begin{aligned}\n  \\hat\\beta_0 &= - \\hat\\beta_1\\cdot\\bar{x} + \\bar{y} \\\\\n  \\hat\\beta_0 &= \\bar{y} - \\hat\\beta_1\\cdot\\bar{x}\n\\end{aligned}\\]\nUm \\(\\hat\\beta_0\\) zu bestimmen, benötigen wir \\(\\hat\\beta_1\\).\n\n\nSchätzen der Steigung \\(\\hat\\beta_1\\)\nEs ist:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot x_i \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 \\cdot x_i + \\sum\\limits_{i=1}^n \\hat\\beta_1 \\cdot x_i\\cdot x_i- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot \\sum\\limits_{i=1}^n  x_i + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right)\n\\end{aligned}\\]\nWir ersetzen nun \\(\\hat\\beta_0\\) durch \\(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\) und erhalten:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  &=\n  2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(\\left(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\right) \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - n \\cdot \\hat\\beta_1 \\cdot  \\bar{x}^2  + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n\\end{aligned}\\]\nMit Hilfe des Verschiebesatzes von Steiner (zweimal angewendet) erhalten wir:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  \n    &=2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)+ \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(\\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)\n\\end{aligned}\\]\nWir setzen nun wieder den Ausdruck gleich Null:\n\\[\\begin{aligned}\n0 &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)  \\qquad | : 2\\\\\n   &= \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{aligned}\\]\nUnd stellen dann nach \\(\\hat\\beta_1\\) um:\n\\[\\begin{aligned}\n  \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\n    &= \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\\\\n  \\hat\\beta_1\n    &= \\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\n\\end{aligned}\\]\nWir können nun Zähler und Nenner der rechten Seite mit \\(\\frac{1}{n}\\) erweitern und erhalten so:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\\\\n\\end{aligned}\\]\nOder aber wir erweitern mit \\(\\frac{1}{n-1}\\) und erhalten:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{s_{x,y}}{s^2_{x}}\n\\end{aligned}\\]\nDamit können wir zur Berechnung sowohl die Kovarianz der Grundgesamtheit \\(\\sigma_{x,y}\\) und die Varianz \\(\\sigma^2_x\\) von \\(x\\), als auch deren Schätzer \\(s_{x,y}\\) und \\(s^2_x\\) verwendet werden!\nDiese Methode nennt sich Methode der kleinsten Quadrate (engl. ordenary least square method) und wir sprechen dann auch von den Kleinste-Quadrate-Schätzern (oder kurz KQ-Schätzer bzw. OLS-Schätzer) \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\).\nErweitern wir den Ausdruck mit Standardabweichung \\(\\sigma_y\\) bzw. \\(s_y\\), so erhalten wir:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n&= \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\end{aligned}\\]\nund analog für die Schätzer:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{s_{x,y}}{s^2_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_y} \\cdot \\frac{s_y}{s_x} \\\\\n&= r_{x,y} \\cdot \\frac{s_y}{s_x} \\\\\n\\end{aligned}\\]\nDie Steigung \\(\\hat\\beta_1\\) hat somit eine direkte Beziehung mit dem Korrelationskoeffizenten \\(\\rho\\) (der Grundgesamtheit) bzw. \\(r\\) (der Stichprobe).\nFür eine Berechnung in R heißt dies: wir können die Regressionskoeffizienten \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\) direkt algebraisch ausrechnen, wenn wir\n\ndie Standardabweichungen von \\(x\\) und \\(y\\) und den Korrelationskoeffizienten oder\ndie Varianz von \\(x\\) und Kovarianz von \\(x\\) und \\(y\\)\n\nhaben.\n\n\nEin Beispiel in R:\nAuf Grundlage der Datentabelle mtcars wollen wir Prüfen wie ein linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdestärke hp) modelliert werden kann.1\n\nlibrary(mosaic)\n\n# Wir nehmen die Datentabelle 'mtcars':\nmtcars %&gt;%\n  select(hp, mpg) -&gt; dt\n\n# Ein kurzer Blick auf die Daten:\nfavstats(~ hp, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  146.6875 68.56287\nfavstats(~ mpg, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  20.09062 6.026948\n\n# Wir vergleichen den Verbrauch (mpg, miles per gallon) \n# mit den Pferdestärken (hp) mit Hilfe eines Streudiagramms:\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir zunächst die Mittelwerte von \\(x\\) (also ‘hp’) und \\(y\\) (also ‘mpg’)\n\n(mean_hp &lt;- mean(~ hp, data = dt))\n#&gt; [1] 146.6875\n(mean_mpg &lt;- mean(~ mpg, data = dt))\n#&gt; [1] 20.09062\n\nund zeichnen die Punkt \\((\\bar{x}, \\bar{y}) = (146.69, 20.09)\\) in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir nun die Schätzwerte für die Regressionsgerade\n\n(beta_1 &lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt))\n#&gt; [1] -0.06822828\n(beta_0 &lt;- mean_mpg - beta_1 * mean_hp)\n#&gt; [1] 30.09886\n\nund zeichnen diese in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \"dodgerblue\") %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988605 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nStudentisieren – einmal hin und einmal zurück\nWas passiert eigentlich, wenn wir unsere \\(x\\) und \\(y\\) Werte studentisieren (aka standardisieren oder z-transformieren)?\nZur Erinnerung, studentisieren geht so: \\[\nx^{stud} = \\frac{x - \\bar{x}}{s_x}\n\\]\nIn R können wir das mit der Funktion ‘zscore’ wie folgt machen:\n\ndt %&gt;%\n  mutate(\n    hp_stud = zscore(hp),\n    mpg_stud = zscore(mpg)\n  ) -&gt; dt\n\nNatürlich sind die Mittelwerte nun Null und die Standardabweichungen Eins:\n\nfavstats(~ hp_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;           mean sd\n#&gt;  -4.857226e-17  1\nfavstats(~ mpg_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;          mean sd\n#&gt;  4.336809e-17  1\n\nDer Grund für die kleinen Abweichungen von der Null bei den Mittelwerten sind unumgängliche Rundungsfehler, die der Computer macht!\nSchauen wir uns nun das Streudiagramm an, zusammen mit dem Mittelpunkt \\((0,0)\\)\n\ngf_point(mpg_stud ~ hp_stud, data = dt) %&gt;%\n  gf_point(0 ~ 0, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(-2, 2))\n\n\n\n\n\n\n\n\nAuch wenn die Skalierungen sich geändert haben, die Diagramme sind sehr ähnlich.\nBestimmen wir die Koeffizienten der Regressionsgerade\n\n(beta_stud_1 &lt;- cov(mpg_stud ~ hp_stud, data = dt))\n#&gt; [1] -0.7761684\n(beta_stud_0 &lt;- 0 - beta_stud_1 * 0)\n#&gt; [1] 0\n\nund setzen sie in das Streudiagramm ein:\n\n\n\n\n\n\n\n\n\nWir können das studentisierte Problem auch wieder auf unser ursprüngliches zurück rechnen.\nDie Regressionsgerade im studentisierten Problem lautet:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761684 \\cdot x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot x^{stud}\n\\end{aligned}\n\\]\nRechnen wir nun mittels der Formel \\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\]\ndie Steigung um, so erhalten wir:\n\n(b1 &lt;- beta_stud_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822828\n\nUnd setzen wir das in unsere Gleichung zur Bestimmung von \\(\\hat\\beta_0\\) ein:\n\n(b0 &lt;- mean(dt$mpg) - b1 * mean(dt$hp))\n#&gt; [1] 30.09886\n\nso erhalten wir die Schätzwerte des ursprünglichen Problem.\n\n\nEin anderer Weg um die Regressionskoeffizenten zu bestimmen…\nGehen wir das Problem noch einmal neu an. Wir suchen \\(\\hat\\beta=(\\hat\\beta_0, \\hat\\beta_1)\\) welches \\(QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\\) minimiert.\nStatt es direkt, wie oben durch Null setzen der partiellen Ableitungen, zu bestimmen, wählen wir nun einen mathematisch-numerischen Ansatz und wollen \\(\\hat\\beta \\in \\mathbf{R}^2\\) als Optimierungsproblem mit Hilfe des Gradientenverfahrens lösen.\nBeim Gradientenverfahren wird versucht, ausgehend von einem Startwert \\(\\hat\\beta^0 \\in \\mathbf{R}^2\\), gemäß der Iterationsvorschrift\n\\[\n\\hat\\beta^{k+1} = \\hat\\beta^{k} + \\alpha^k \\cdot d^k\n\\]\nfür alle \\(k=0,1, ...\\) eine Näherungslösung für \\(\\hat\\beta\\) zu finden. Dabei ist \\(\\alpha^k &gt; 0\\) eine positive Schrittweite und \\(d^k\\in\\mathbf{R}^n\\) eine Abstiegsrichtung, welche wir in jedem Iterationsschritt \\(k\\) so bestimmen, dass die Folge \\(\\hat\\beta^k\\) zu einem stationären Punkt, unserer Näherungslösung, konvergiert.\nIm einfachsten Fall, dem Verfahren des steilsten Abstieges, wird der Abstiegsvektor \\(d^k\\) aus dem Gradienten \\(\\nabla QS\\) wie folgt bestimmt:\n\\[\nd^k = -\\nabla QS\\left(\\hat\\beta^k\\right)\n\\]\nWegen \\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot n \\cdot \\left(  \\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y} \\right)\n\\]\nund\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS = 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\right)\n\\]\ngilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot(\\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y})  \\\\\n\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{pmatrix}\n\\end{aligned}\n\\]\nWir wollen hier von Anfang an mit den studentisierten Werten arbeiten, weil diese numerisch viele Vorteile haben. Darum vereinfachen sich die beiden partiellen Ableitungen noch einmal zu:\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot v\n\\]\nund\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\\\\n&= 2 \\cdot (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{aligned}\n\\]\nSomit gilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot \\hat\\beta_0 \\\\\n(n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{pmatrix}\n\\end{aligned}\n\\]\nUm die Varianz und die Kovarianz nicht jedesmal neu zu berechnen, speichern wir die Ergebnisse vorab. Ebenso, damit der Quellcode kürzer wird, speichern wir in \\(x\\) und \\(y\\) die studentisierten Werte von \\(hp\\) und \\(mpg\\):\n\n# Vorbereitungen \nvar_x &lt;- var(~ hp_stud, data = dt)\ncov_xy &lt;- cov(mpg_stud ~ hp_stud, data = dt)\n\nn &lt;- length(dt$hp_stud)\n\nx &lt;- dt$hp_stud\ny &lt;- dt$mpg_stud\n\nNun erstellen wir die \\(QS\\) und \\(\\nabla QS\\) Funktionen: Wir definieren diese Funktion wie folgt in R:\n\nqs &lt;- function(b_0, b_1) {\n  sum((b_1 * x - y)**2)\n}\n\nnabla_qs &lt;- function(b_0, b_1) {\n  c(2 * n * b_0,\n    2 * (n - 1) * (b_1 * var_x - cov_xy)\n  )\n}\n\nDie Schrittweite \\(alpha\\) bestimmen wir mit Hilfe der Armijo-Bedingung und der Backtracking Liniensuche: Diese formalisiert das Konzept “genügend” in der geforderten Verringerung des Funktionswertes. Die Bedingung \\(f(x^k + \\alpha d^k) &lt; f(x^k)\\) wird modifiziert zu \\[f(x^k + \\alpha d^k) \\leq f(x^k) + \\sigma \\alpha \\left(\\nabla f(x^k)\\right)^T d^k,\\] mit \\(\\sigma\\in (0,1)\\). Die Armijo-Bedingung umgeht Konvergenzprobleme der einfachen Bedingung, indem sie fordert, dass die Verringerung zumindest proportional zur Schrittweite und zur Richtungsableitung \\(\\left(\\nabla f(x^k)\\right)^T d^k\\) ist, mit Hilfe der Proportionalitätskonstante \\(\\sigma\\). In der Praxis werden oft sehr kleine Werte verwendet, z.B. \\(\\sigma=0.0001\\).\nDie Backtracking-Liniensuche verringert die Schrittweite wiederholt um den Faktor \\(\\rho\\) (rho) , bis die Armijo-Bedingung erfüllt ist. Sie terminiert garantiert nach einer endlichen Anzahl von Schritten. Weshalb wir sie hier einsetzen:\n\nalpha_k &lt;- function(b_0, b_1, d_k, alpha = 1, sigma = 0.0001, rho = 0.5) {\n  d_0 &lt;- d_k[1]\n  d_1 &lt;- d_k[2]\n  nabla &lt;- nabla_qs(b_0, b_1)\n  n_0 &lt;- nabla[1]\n  n_1 &lt;- nabla[2]\n\n  lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n  rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n\n  while (lhs &gt; rhs) {\n    alpha &lt;- rho * alpha\n    lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n    rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n  }\n  return(alpha)\n}\n\nEin paar Einstellungen vorab:\n\n# maximale Anzahl an Iterationen\nmax_iter &lt;- 1000\niter &lt;- 0\n\n# Genauigkeit\neps &lt;- 10**-6\n\n# Startwerte\nb_0 &lt;- 0 \nb_1 &lt;- -1 \n\nFür eine vorgegebene Genauigkeit \\(eps=10^{-6}\\), den Startwerten \\(\\hat\\beta_0^0 = 0\\) und \\(\\hat\\beta_1^0 = -1\\) können wir somit das Verfahren starten:\n\nwhile (TRUE) {\n  iter &lt;- iter + 1\n\n  d_k &lt;- -nabla_qs(b_0, b_1)\n\n  ad_ &lt;- alpha_k(b_0, b_1, d_k) * d_k\n\n  x0 &lt;- b_0 + ad_[1]\n  x1 &lt;- b_1 + ad_[2]\n\n  if ((abs(b_0 - x0) &lt; eps) & (abs(b_1 - x1) &lt; eps) | (iter &gt; max_iter)) {\n    break\n  }\n  b_0 &lt;- x0\n  b_1 &lt;- x1\n}\n\nWir haben in \\(203\\) Iterationsschritten das folgende Ergebnis für die Regressionskoeffizienten:\n\\[\n\\hat\\beta_0^{stud} = 0 \\qquad \\hat\\beta_1^{stud} = -0.7761689\n\\]\nBetrachten wir die daraus erstellte Regressionsgerade:\n\n\n\n\n\n\n\n\n\nUm die Regressionskoeffizienten für unser ursprüngliches Problem zu erhalten müssen wir wie folgt zurück rechnen:\n\n(b1 &lt;- b_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822832\n(b0 &lt;- mean(dt$mpg) -  b1 * mean(dt$hp))\n#&gt; [1] 30.09887\n\nDie Geradengleichung für das ursprüngliches Problem lautet somit:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988668 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nDie R Funktion optim\nIn R gibt es bessere Optimierungsmethoden, als die hier verwendete. Zum Beispiel können wir die Funktion optim verwenden. Die Funktion optim benötigt die zu optimierende \\(f(x)\\) und ggf. die Gradientenfunktion \\(gf(x)\\) sowie einen Startpunkt \\(x^0\\):\n\nf &lt;- function(beta) {\n  qs(beta[1], beta[2])\n}\n\ngrf &lt;- function(beta) {\n  nabla_qs(beta[1], beta[2])\n}\n\n# Der eigentliche Aufruf von optim:\nergb &lt;- optim(c(0,-0.5),f ,grf, method = \"CG\")\n\n# Auslesen der Schätzer aus dem Ergebnis:\n(optim_beta_0 &lt;- ergb$par[1])\n#&gt; [1] 0\n(optim_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.7761683\n\nWir erhalten somit für das studentisierte Problem die Gerade:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta_0^{stud} + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761683 \\cdot  x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot  x^{stud}\n\\end{aligned}\n\\]\nFür das ursprüngliche Problem rechnen wir mittels\n\noptim_b1 &lt;- optim_beta_1 * sd(dt$mpg) / sd(dt$hp)\noptim_b0 &lt;- mean(dt$mpg) -  optim_b1 * mean(dt$hp)\n\num und erhalten:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988601 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "2. Idee: Summe der absoluten Abweichungen",
    "text": "2. Idee: Summe der absoluten Abweichungen\nWir ändern nun die Abweichungsmessfunktion von der Quadrat-Summe hin zu den Absolut-Summen:\n\\[\nAS = AS(\\hat\\beta) = AS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n |\\hat{y}_i - y_i|\n\\]\nAuch hier wollen wir mit den studentisierten Daten arbeiten und stellen die Funktion der Absolut-Summen auf:\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  return(sum(abs(b_0 + b_1 * x - y)))\n}\n\nDanach konstruieren wir die zu optimierende Funktion \\(f\\):\n\n# Zu optimierende Funktion\nf &lt;- function(beta) {\n  as(beta[1], beta[2])\n}\n\nDiesmal nutzen wir optim ohne eine Gradientenfunktion:\n\nergb &lt;- optim(c(0,-1), f)\n\n# Schätzer auslesen\n(opti_as_beta_0 &lt;- ergb$par[1])\n#&gt; [1] -0.1304518\n(opti_as_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.6844911\n\nSchauen wir uns nun die so erhaltene Gerade im Vergleich mit der ‘normalen’ Regressionsgerade an:\n\n\n\n\n\n\n\n\n\nIn grün und gestrichelt sehen wir die Gerade aus der Idee der quadratischen Abweichungssummen, in blau die aus der Idee der absoluten Abweichungssummen.\nFür unser ursprüngliches Problem rechnen wir um:\n\n# Umrechnen in die ursprüngliche Fragestellung\n(as_b1 &lt;- opti_as_beta_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06016948\n(as_b0 &lt;- (mean(dt$mpg) - as_b1 * mean(dt$hp)) + opti_as_beta_0 * sd(dt$mpg))\n#&gt; [1] 28.13051\n\nUnd die dazu gehörige Darstellung:\n\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 28.1305094 -0.0601695 \\cdot x \\\\\n          &\\approx 28.131 -0.06 \\cdot x\n\\end{aligned}\n\\]\nDiese Methode nennt sich Median-Regression und ein ein Spezialfall der Quantilsregression, die sich u.a. mit dem R-Paket quantreg unmittelbar umsetzen lässt:\n\nlibrary(quantreg)\nergmedianreg &lt;- rq(mpg ~ hp, data = dt)\ncoef(ergmedianreg)\n#&gt; (Intercept)          hp \n#&gt; 28.13050847 -0.06016949"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "1. Idee: Betrag der Summe der Abweichungen",
    "text": "1. Idee: Betrag der Summe der Abweichungen\nWenn wir die Summe der Abweichungen \\(\\sum\\limits_{i=1}^n \\hat{e}_i\\) minimieren wollen, dann ist es sinnvoll den Betrag davon zu minimieren. Wir suchen also die Schätzer \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\), so dass der Ausdruck\n\\[\n\\left| \\sum_{i=1}^n \\hat{e}_i \\right| = \\left| \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \\right|\n\\]\nminimal ist.\nWegen:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i)\n&= \\sum_{i=1}^n \\hat\\beta_0 + \\sum_{i=1}^n \\hat\\beta_1 \\cdot x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot n \\cdot \\bar{x} - n \\cdot \\bar{y} \\\\\n&= n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\bar{x} - \\bar{y} \\right) \\\\\n&= n \\cdot \\left( \\hat\\beta_0 - \\bar{y} + \\hat\\beta_1 \\cdot \\bar{x}  \\right)\n\\end{aligned}\n\\]\nkönnen wir das absolute Minimum bei \\(\\hat\\beta_0 - \\bar{y} =0\\) und \\(\\hat\\beta_1 \\cdot \\bar{x}=0\\) erreichen, was zur Lösung \\(\\hat\\beta_0 =\\bar{y}\\) und \\(\\hat\\beta_1 = 0\\) führt. Dies ist unser Nullmodel in dem die \\(x_i\\) keinen Einfluss auf die \\(y_i\\) haben und wir daher pauschal die \\(y_i\\) mit \\(\\hat{y}_i=\\bar{y}\\), also dem Mittelwert der \\(y_i\\) abschätzen."
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nAls Vergleich können wir uns die Quadratsumme \\(QS\\) und Absolutsumme \\(AS\\) der drei Modelle einmal ansehen:\n\n# Quadratische Abweichungssummen\nqs &lt;- function(b_0, b_1) {\n  sum(((b_0 + b_1 * dt$hp) - dt$mpg )**2)\n}\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  sum(abs((b_0 + b_1 * dt$hp) - dt$mpg))\n}\n\n\n# Quadratsummen:\nquad_sum &lt;- c(qs(b0, b1), qs(as_b0, as_b1), qs(mean_mpg, 0))\n\n# Absolutsummen:\nabs_sum &lt;- c(as(b0, b1), as(as_b0, as_b1), as(mean_mpg, 0))\n\ntab &lt;- tibble(\n  sums = c(quad_sum, abs_sum),\n  sum_type = rep(c(\"quad\", \"abs\"), each = 3),\n  methode = rep(c(\"Idee 3\", \"Idee 2\", \"Idee 1\"), 2)\n)\n\npivot_wider(tab, names_from=sum_type, values_from=sums, names_sort=T)\n#&gt; # A tibble: 3 × 3\n#&gt;   methode   abs  quad\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Idee 3   93.0  448.\n#&gt; 2 Idee 2   87.3  477.\n#&gt; 3 Idee 1  151.  1126."
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 quantreg_6.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nDas “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎"
  },
  {
    "objectID": "posts/Willkommen/index.html",
    "href": "posts/Willkommen/index.html",
    "title": "Willkommen in meinem Blog",
    "section": "",
    "text": "Das ist mein erster Blog-Eintrag! Ich werde nun hoffentlich (langsam) meinen Blog von Hugo aus Quarto umstellen. Ich arbeite viel mit R und eine extra Sprache nur für den Blog ist mir einfach auf Dauer zu umständlich. Aber mal sehen, wann ich Zeit finde dieses Projekt umzustezen."
  },
  {
    "objectID": "posts/2021-02-12-gedankenstütze-zu-wichtigen-funktionsbegriffen-in-der-statistik/index.html",
    "href": "posts/2021-02-12-gedankenstütze-zu-wichtigen-funktionsbegriffen-in-der-statistik/index.html",
    "title": "Gedankenstütze zu wichtigen Funktionsbegriffen in der Statistik",
    "section": "",
    "text": "Eine kleine Liste von fundermentalen Begriffen in der Statistik.\nGilt für eine reelle Funktion \\(f: \\mathbf{R} \\to \\mathbf{R}\\):\n\n\\(f(x)\\) ist nichtnegativ, d.h., \\(f(x) \\geq 0\\), für alle \\(x \\in \\mathbf{R}\\).\n\\(f(x)\\) ist integrierbar.\n\\(f(x)\\) ist normiert in dem Sinne, dass \\[\\int_{-\\infty}^\\infty f(x) \\,\\text{d}x = 1\\]\n\nDann nennen wir \\(f(x)\\) eine Wahrscheinlichkeitsdichtefunktion (engl. probability density funktion kurz pdf) oder kurz Dichte (engl. density).\nDurch \\[P([a, b]) := \\int_a^b f(x) \\text{d} x\\] definiert \\(f\\) eine Wahrscheinlichkeitsverteilung auf den reellen Zahlen.\nIst \\(X\\) eine reelwertige Zufallsvariable (kurz ZV) und existiert eine reelle Funktion \\(f_X(x)\\) der Art, dass für alle \\(a \\in \\mathbf{R}\\)\n\\[P(X \\leq a) = \\int_{-\\infty}^a f_X(x) \\text{d}x\\]\ngilt, so nennt man \\(f\\) die Wahrscheinlichkeitsdichtefunktion von \\(X\\).\nDie Funktion\n\\[F_X(a) = P(X \\leq a)\\]\nnenen wir (Wahrscheinlichkeits-)Verteilung(-sfunktion) (engl. cumulative distribution function kurz cdf aber auch nur distribution function) von \\(X\\).\nGenau dann ist eine Funktion \\(F: \\mathbf{R} \\to [0, 1]\\) eine Verteilungsfunktion, wenngilt:\n\nEs ist \\(\\lim_{t \\to -\\infty} F(t)=0\\) und \\(\\lim_{t \\to +\\infty} F(t)=1\\)\nDie Funktion \\(\\overline{F}(t)\\) ist monoton wachsend.\nDie Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig\n\nDie Funktion\n\\[\\overline{F}_X(a) = 1 - F_X(a)\\]\nnennen wir Überlebensfunktion (engl. survival function, complementarey cumulative distribution funktion kurz ccdf, tail distribution, exceedance oder reliability function).\nEs gilt \\(\\overline{F}_X(a) + F_X(a) = 1\\).\nGenau dann ist eine Funktion \\(\\overline{F}: \\mathbf{R} \\to [0, 1]\\) eine Überlebensfunktion, wenn gilt:\n\nEs ist \\(\\lim_\\limits{t \\to -\\infty} \\overline{F}(t)=1\\) und \\(\\lim_{t \\to + \\infty}\\overline{F}(t)=0\\)\nDie Funktion \\(\\overline{F}(t)\\) ist monoton fallend.\nDie Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig\n\nEin paar weitere Eigenschaften von Überlebensfunktionen:\n\nNicht-negative stetige ZV \\(X\\) mit Erwartungswert, also \\(\\int_0^\\infty x f(x) \\text{d} x = \\mu &lt; \\infty\\), erfüllen die Markov-Ungleichung\n\n\\[\\overline{F}_X(x) \\leq \\frac{\\operatorname{E}(X)}{x}\\]\n\nIst \\(X\\) eine ZV und \\(\\overline{F}_X\\) die zugehörige Überlebensfunktion. Existiert \\(E(X)\\), dann gilt \\(\\lim_{t \\to +\\infty}\\overline{F}(x)=0 = o\\left(\\frac{1}{x}\\right)\\).\n\nBeweisskizze: Sei \\(f_X\\) die Dichtefunktion von \\(F_X\\) zur ZV \\(X\\). Für jedes \\(c&gt;0\\) ist dann \\[\n\\begin{aligned}\n      E(X) = \\int_0^\\infty x \\cdot f_X(x) \\text{d}x &=  \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty x \\cdot f_X(x) \\text{d}x \\\\\n      &\\geq  \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty c \\cdot f_X(x) \\text{d}x \\\\\n      &=  \\int_0^cx \\cdot f_X(x) \\text{d}x + c \\cdot \\int_c^\\infty f_X(x) \\text{d}x \\\\\n      &=  \\int_0^cx \\cdot f_X(x) \\text{d}x +c \\cdot \\overline{F}(c)\n  \\end{aligned}\n\\]\nDamit gilt nun: \\[\n0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\n\\] Wegen \\(\\lim\\limits_{c \\to +\\infty} \\int_0^cx \\cdot f_X(x) \\text{d}x = E(X)\\) folgt:\n\\[0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\\to 0 \\text{ wenn } c \\to \\infty\n\\]\nFür nicht-negative ZV \\(X\\) gilt:\n\\[\nE(X) = \\int_0^\\infty \\overline{F}_X(x) \\,\\text{d} x\n\\]"
  },
  {
    "objectID": "posts/2025-07-27-Beweisverfahren/index.html",
    "href": "posts/2025-07-27-Beweisverfahren/index.html",
    "title": "Beweisverfahren in der Mathematik",
    "section": "",
    "text": "Trivial!\n\n\n\nDer Satz folgt sofort aus dem Fakt\n\\[\n  \\left|\n    \\oplus_{k \\in S}\n      \\left(\n       \\mathfrak{K}^{\\mathbb{F}^\\alpha(i)}  \n      \\right)_{i \\in \\mathcal{U}_k}\n  \\right|\n  \\preceq \\aleph_1\n  \\text{ wenn }\n  \\left[\n    \\mathfrak{H}_\\mathcal{W}\n  \\right]     \n  \\cap \\mathbb{F}^\\alpha(\\mathbb{N}) \\neq \\emptyset.\n\\]\n\n\n\nDer Satz ist ein einfaches Korrollat eines Ergebnis bewiesen in einer handgeschriebenen Anmerkung ausgehändigt in einer Vorlesung bei der Jugoslawischen Mathematischen Gesellschaft 1973.\n\n\n\nDer Beweis kann auf Seite 478 gefunden werden, in einem Buch, welches nur 396 Seiten hat.\n\n\n\nProposition 5.18 in [BL] ist ein einfaches Korollar zm Satz 7.18 in [C], welches wiederum auf Korollar 2.14 in [K] basiert. Dieses wiederum wird unter Bezugnahme auf Proposition 5.18 in [BL] hergeleitet.\n\n\n\nMein guter Kollege Andrew meinte, er glaube, er hätte dafür vor ein paar Jahren einen Beweis finden können …\n\n\n\nFür Interessierte ist das Ergebnis auf der Webseite zu diesem Buch zu sehen. – Die es leider nicht mehr gibt.\n\n\n\nKapitel 3: Der Beweis hierfür wird erst in Kapitel 7 erbracht, wenn wir die Theorie weiterentwickelt haben.\nKapitel 7: Der Einfachheit halber beweisen wir dies nur für den Fall z = 0. Der allgemeine Fall wird jedoch in Anhang C behandelt.\nAnhang C: Der formale Beweis geht über den Rahmen dieses Buches hinaus, aber unsere Intuition sagt uns natürlich, dass dies wahr ist."
  },
  {
    "objectID": "posts/2025-07-27-Beweisverfahren/index.html#allgemeine-beweistechniken",
    "href": "posts/2025-07-27-Beweisverfahren/index.html#allgemeine-beweistechniken",
    "title": "Beweisverfahren in der Mathematik",
    "section": "",
    "text": "Trivial!\n\n\n\nDer Satz folgt sofort aus dem Fakt\n\\[\n  \\left|\n    \\oplus_{k \\in S}\n      \\left(\n       \\mathfrak{K}^{\\mathbb{F}^\\alpha(i)}  \n      \\right)_{i \\in \\mathcal{U}_k}\n  \\right|\n  \\preceq \\aleph_1\n  \\text{ wenn }\n  \\left[\n    \\mathfrak{H}_\\mathcal{W}\n  \\right]     \n  \\cap \\mathbb{F}^\\alpha(\\mathbb{N}) \\neq \\emptyset.\n\\]\n\n\n\nDer Satz ist ein einfaches Korrollat eines Ergebnis bewiesen in einer handgeschriebenen Anmerkung ausgehändigt in einer Vorlesung bei der Jugoslawischen Mathematischen Gesellschaft 1973.\n\n\n\nDer Beweis kann auf Seite 478 gefunden werden, in einem Buch, welches nur 396 Seiten hat.\n\n\n\nProposition 5.18 in [BL] ist ein einfaches Korollar zm Satz 7.18 in [C], welches wiederum auf Korollar 2.14 in [K] basiert. Dieses wiederum wird unter Bezugnahme auf Proposition 5.18 in [BL] hergeleitet.\n\n\n\nMein guter Kollege Andrew meinte, er glaube, er hätte dafür vor ein paar Jahren einen Beweis finden können …\n\n\n\nFür Interessierte ist das Ergebnis auf der Webseite zu diesem Buch zu sehen. – Die es leider nicht mehr gibt.\n\n\n\nKapitel 3: Der Beweis hierfür wird erst in Kapitel 7 erbracht, wenn wir die Theorie weiterentwickelt haben.\nKapitel 7: Der Einfachheit halber beweisen wir dies nur für den Fall z = 0. Der allgemeine Fall wird jedoch in Anhang C behandelt.\nAnhang C: Der formale Beweis geht über den Rahmen dieses Buches hinaus, aber unsere Intuition sagt uns natürlich, dass dies wahr ist."
  },
  {
    "objectID": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html",
    "href": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html",
    "title": "Wie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert",
    "section": "",
    "text": "Am besten mit dem Raspberry Pi Installer von hier: https://www.raspberrypi.com/software/\nFür einen Raspberry Zero: Raspberry Pi OS (32-bit) Lite\nFür einen Raspberry Zero 2 W: Raspberry Pi OS (64-bit) Lite\nWichtig MIT SSH konfigurieren!\n\n\n\nAuf dem RaspberryPI Zero ist ein Waveshare 2-13inch-e-Paper HAT installiert. Alles Wichtige dazu findet sich unter: https://www.waveshare.com/wiki/2.13inch_e-Paper_HAT_Manual#Working_With_Raspberry_Pi\nDamit das Display richtig funktioniert sollte man in der config.txt die folgende Zeile anpassen:\ndtparam=spi=on\n\n\n\nEs gibt viele Weg die zum Ziel führen (können).\nZum Beispiel mit dem Befehl nmap:\n&gt; nmap -sn 192.168.2.0/24\nOder einfach den Hostnamen “raspberrpi.local” nutzen.\nMan kann sich somit ggf. mittels ssh und dem folgenden Kommando anmelden:\n&gt; ssh norman@raspberrypi.local\nEbenso kann R PI Connect benutzt werden, wenn man es vorher eingerichtet hat.\n\n\n\nIch habe bei GitHub ein kleines Programm geschrieben, welches die aktuelle IP Address (IP4 und IP6 sowie die aktuelle Uhrzeit) auf dem Display ausgibt.\nEs ist unter https://github.com/NMarkgraf/pi-startup-display zu finden und wird einfach in das Hauptverzeichnis des Nutzers “norman” unter “/home/norman” per\n&gt; git clone https://github.com/NMarkgraf/pi-startup-display \ninstalliert.\nSpäter kann dann im Verzeichnis pi-startup-display einfach das aktuelle Release von GutHUb durch den folgenden Befehl geladen werden:\n&gt; git pull\nDamit dieses Skript nach jedem Neustart automatisch (einmal) gestartet wird bin ich nach dem Beispiel von https://webnist.de/automatisches-starten-eines-python-skripts-auf-dem-raspberry-pi-mit-systemd/ vorgegangen.\nAber ich habe die service Datei “/etc/systemd/system/startup-display.service” wie folgt angepasst:\n[Unit]\nDescription=startup-display\nAfter=network.target\n \n[Service]\nExecStart=/usr/bin/python3 -u /home/norman/pi-startup-display/start-display.py \nWorkingDirectory=/home/norman/pi-startup-display/\n#StandardOutput=inherit\nStandardOutput=file:/tmp/start-display.log\n#StandardError=inherit\nStandardError=file:/tmp/start-display-error.log\n#Restart=always\nRestart=on-failture\nRestartSec=60\nUser=norman\n \n[Install]\nWantedBy=multi-user.target\nZum anlegen habe ich den Editor “nano” genutzt und die Datei gleich mit angelegt.\n&gt; sudo nano /etc/systemd/system/startup-display.service\nDie geänderte Datei muss nun initialisiert werden mit:\n&gt; sudo systemctl daemon-reload\n&gt; sudo systemctl enable startup-display.service\nDann können wir alles, zum Testen, von Hand starten:\n&gt; sudo systemctl start startup-display.service\nOb alles zu unserer Zufriedenheit funktioniert können wir mit der Status-Anzeige mit dem folgenden Befehl prüfen:\n&gt; sudo systemctl status startup-display.service\nUnd natürlich ggf. auch stoppen:\n&gt; sudo systemctl stop startup-display.service"
  },
  {
    "objectID": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#einrichten-des-raspberry-zero",
    "href": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#einrichten-des-raspberry-zero",
    "title": "Wie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert",
    "section": "",
    "text": "Am besten mit dem Raspberry Pi Installer von hier: https://www.raspberrypi.com/software/\nFür einen Raspberry Zero: Raspberry Pi OS (32-bit) Lite\nFür einen Raspberry Zero 2 W: Raspberry Pi OS (64-bit) Lite\nWichtig MIT SSH konfigurieren!\n\n\n\nAuf dem RaspberryPI Zero ist ein Waveshare 2-13inch-e-Paper HAT installiert. Alles Wichtige dazu findet sich unter: https://www.waveshare.com/wiki/2.13inch_e-Paper_HAT_Manual#Working_With_Raspberry_Pi\nDamit das Display richtig funktioniert sollte man in der config.txt die folgende Zeile anpassen:\ndtparam=spi=on\n\n\n\nEs gibt viele Weg die zum Ziel führen (können).\nZum Beispiel mit dem Befehl nmap:\n&gt; nmap -sn 192.168.2.0/24\nOder einfach den Hostnamen “raspberrpi.local” nutzen.\nMan kann sich somit ggf. mittels ssh und dem folgenden Kommando anmelden:\n&gt; ssh norman@raspberrypi.local\nEbenso kann R PI Connect benutzt werden, wenn man es vorher eingerichtet hat.\n\n\n\nIch habe bei GitHub ein kleines Programm geschrieben, welches die aktuelle IP Address (IP4 und IP6 sowie die aktuelle Uhrzeit) auf dem Display ausgibt.\nEs ist unter https://github.com/NMarkgraf/pi-startup-display zu finden und wird einfach in das Hauptverzeichnis des Nutzers “norman” unter “/home/norman” per\n&gt; git clone https://github.com/NMarkgraf/pi-startup-display \ninstalliert.\nSpäter kann dann im Verzeichnis pi-startup-display einfach das aktuelle Release von GutHUb durch den folgenden Befehl geladen werden:\n&gt; git pull\nDamit dieses Skript nach jedem Neustart automatisch (einmal) gestartet wird bin ich nach dem Beispiel von https://webnist.de/automatisches-starten-eines-python-skripts-auf-dem-raspberry-pi-mit-systemd/ vorgegangen.\nAber ich habe die service Datei “/etc/systemd/system/startup-display.service” wie folgt angepasst:\n[Unit]\nDescription=startup-display\nAfter=network.target\n \n[Service]\nExecStart=/usr/bin/python3 -u /home/norman/pi-startup-display/start-display.py \nWorkingDirectory=/home/norman/pi-startup-display/\n#StandardOutput=inherit\nStandardOutput=file:/tmp/start-display.log\n#StandardError=inherit\nStandardError=file:/tmp/start-display-error.log\n#Restart=always\nRestart=on-failture\nRestartSec=60\nUser=norman\n \n[Install]\nWantedBy=multi-user.target\nZum anlegen habe ich den Editor “nano” genutzt und die Datei gleich mit angelegt.\n&gt; sudo nano /etc/systemd/system/startup-display.service\nDie geänderte Datei muss nun initialisiert werden mit:\n&gt; sudo systemctl daemon-reload\n&gt; sudo systemctl enable startup-display.service\nDann können wir alles, zum Testen, von Hand starten:\n&gt; sudo systemctl start startup-display.service\nOb alles zu unserer Zufriedenheit funktioniert können wir mit der Status-Anzeige mit dem folgenden Befehl prüfen:\n&gt; sudo systemctl status startup-display.service\nUnd natürlich ggf. auch stoppen:\n&gt; sudo systemctl stop startup-display.service"
  },
  {
    "objectID": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#postgresql-installieren",
    "href": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#postgresql-installieren",
    "title": "Wie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert",
    "section": "PostgreSQL installieren",
    "text": "PostgreSQL installieren\nIch gehe wie folgt vor:\n\nSchritt 1: Installieren von PostgreSQL\nDa es ein deb Paket gibt, nehme ich den leichten Weg:\n&gt; sudo apt install postgresql\n\n\nSchritt 2: Wechseln auf Nutzer:in “postgres”\nWir kommunizieren mit PostgreSQL durch den Benutzer “postgres”, in den wir uns verwandelt:\n&gt; sudo su postgres\n\n\nSchritt 3: Herunterladen der Demo-Daten, entpacken und installieren\nDie erste Demo-Daten erhalten wir durch:\n&gt; curl -L -O https://github.com/lerocha/netflixdb/releases/download/v1.0.16/netflixdb-postgres.zip\n&gt; unzip netflixdb-postgres.zip\nDa Daten können wir dann in die Datenbank “netfix” einfügren:\n&gt; sudo su postgres\n&gt; psql -d netflix\n&gt; \\i /home/norman/netflixdb-postgres.sql\nJetzt ist die Datenbank “netflix” in PostgreSQL installiert.\nUnd wir können diese nun mit den folgenden Parametern ansprechen:\n  ip:       ???? \n  port:     5342\n  user:     postgres\n  password: postgresdemo \n  dbname:   netflix\n\n\nSchritt 4: AdminerNEO installieren\nAdminerNEO ist ein Webtool für die Verwaltung von PostgreSQL.\nWir installieren die aktuelle Version von der Website: https://www.adminneo.org/download"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu möchte mensch zwei (oder mehr) Grafiken neben- oder übereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zunächst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausführen oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu möchte mensch zwei (oder mehr) Grafiken neben- oder übereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zunächst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausführen oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse für n = 1 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‘import nqueens; nqueens.main()’\nerzeugt.\n\n\nDie Ergebnisse des Test vom 20.7.2025\n\nCPython:\n\n3.13.1: 1 loop, best of 50: 673 msec per loop\n3.13.5: 1 loop, best of 50: 661 msec per loop\n\nPyPy:\n\n3.10-7.3.17: 1 loops, average of 50: 75.6 +- 2.9 msec per loop (using standard deviation)\n3.11-7.3.19: 1 loops, average of 50: 74.9 +- 2.89 msec per loop (using standard deviation)\n\nGraalPython/GraalPy\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1):1 loop, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 1 loop, best of 50: 30 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 1 loop, best of 50: 24.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 1 loop, best of 50: 28.1 msec per loop"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup-1",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup-1",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse für n = 5 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‘import nqueens; nqueens.main()’\nerzeugt.\n\nDie Ergebnisse des Test vom 4.12.2025 (iMac 24.6.0 x86_64 i386)\n\nCPython:\n\nPython 3.13.5: 5 loops, best of 50: 656 msec per loop\nPython 3.13.10: 5 loops, best of 50: 647 msec per loop\nPython 3.14.0: 5 loops, best of 50: 665 msec per loop\nPython 3.14.1: 5 loops, best of 50: 701 msec per loop\n\nPyPy:\n\nPython 3.11.11 (0253c85bf5f8, Feb 26 2025, 10:43:06) [PyPy 7.3.19 with GCC Apple LLVM 15.0.0 (clang-1500.3.9.4)]: 5 loops, average of 50: 85.7 +- 2.08 msec per loop (using standard deviation)\nPython 3.11.13 (413c9b7f57f5, Jul 03 2025, 18:04:31) [PyPy 7.3.20 with GCC Apple LLVM 16.0.0 (clang-1600.0.26.6)]: 5 loops, average of 50: 87.6 +- 3.06 msec per loop (using standard deviation)\n\nGraalPy(thon):\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1): 5 loops, best of 50: 25.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 5 loops, best of 50: 33.6 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 5 loops, best of 50: 24.4 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 5 loops, best of 50: 32.2 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.2): 5 loops, best of 50: 24.4 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.2): 5 loops, best of 50: 32.3 msec per loop\nGraalPy 3.12.8 (Oracle GraalVM Native 25.0.1): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.12.8 (GraalVM CE Native 25.0.1): 5 loops, best of 50: 29.6 msec per loop"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#die-ergebnisse-des-test-vom-4.12.2025-macbookpro-24.6.0-arm64-arm",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#die-ergebnisse-des-test-vom-4.12.2025-macbookpro-24.6.0-arm64-arm",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Die Ergebnisse des Test vom 4.12.2025 (MacBookPro 24.6.0 arm64 arm)",
    "text": "Die Ergebnisse des Test vom 4.12.2025 (MacBookPro 24.6.0 arm64 arm)\n\nCPython:\n\nPython 3.13.5: 5 loops, best of 50: 661 msec per loop\nPython 3.13.10: 5 loops, best of 50: 653 msec per loop\nPython 3.14.0: 5 loops, best of 50: 469 msec per loop\nPython 3.14.1: 5 loops, best of 50: 469 msec per loop\n\nPyPy:\n\nPython 3.11.11 (0253c85bf5f8, Feb 26 2025, 10:42:49) [PyPy 7.3.19 with GCC Apple LLVM 15.0.0 (clang-1500.3.9.4)]: 5 loops, average of 50: 74.2 +- 0.681 msec per loop (using standard deviation)\nPython 3.11.13 (413c9b7f57f5, Jul 03 2025, 18:04:06) [PyPy 7.3.20 with GCC Apple LLVM 16.0.0 (clang-1600.0.26.6)]: 5 loops, average of 50: 73.3 +- 0.688 msec per loop (using standard deviation)\n\nGraalPy(thon):\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1): 5 loops, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 5 loops, best of 50: 29.1 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 5 loops, best of 50: 27.3 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.2): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.2): 5 loops, best of 50: 27.2 msec per loop\nGraalPy 3.12.8 (Oracle GraalVM Native 25.0.1): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.12.8 (GraalVM CE Native 25.0.1): 5 loops, best of 50: 24.9 msec per loop"
  },
  {
    "objectID": "publications/2004-09-XX_LEARN2CONTROL/index.html",
    "href": "publications/2004-09-XX_LEARN2CONTROL/index.html",
    "title": "LEARN2CONTROL",
    "section": "",
    "text": "LEARN2CONTROL ist eine computergestützte Lernumgebung, die es ermöglicht, vorhandenes Grundlagenwissen im Bereich der Regelungstechnik durch selbstständiges Lernen in einem projektorientierten Umfeld zu vertiefen. Der Lernschwerpunkt liegt dabei besonders auf der Vermittlung der inneren Abhängigkeiten und Wechselwirkungen der unterschiedlichen Verfahren und Methoden der Regelungstechnik"
  },
  {
    "objectID": "publications/2004-09-XX_LEARN2CONTROL/index.html#zusammenfassung",
    "href": "publications/2004-09-XX_LEARN2CONTROL/index.html#zusammenfassung",
    "title": "LEARN2CONTROL",
    "section": "",
    "text": "LEARN2CONTROL ist eine computergestützte Lernumgebung, die es ermöglicht, vorhandenes Grundlagenwissen im Bereich der Regelungstechnik durch selbstständiges Lernen in einem projektorientierten Umfeld zu vertiefen. Der Lernschwerpunkt liegt dabei besonders auf der Vermittlung der inneren Abhängigkeiten und Wechselwirkungen der unterschiedlichen Verfahren und Methoden der Regelungstechnik"
  },
  {
    "objectID": "projekte/FUMS/index.html",
    "href": "projekte/FUMS/index.html",
    "title": "FUMS - Fresh Up your Maths Skills",
    "section": "",
    "text": "Crash-Kurs durch die Mathematik für Studierende der Informatik / Wirtschaftsinformatik. Quasi ein kleiner Vorkurs mit den wichtistens Wiederholungen der Schulmathematik.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdrückliches und schriftliches Einverständnis benutzt werden.\nFür alle anderen gilt:\n© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "projekte/FUMS/index.html#fums---fresh-up-your-maths-skills",
    "href": "projekte/FUMS/index.html#fums---fresh-up-your-maths-skills",
    "title": "FUMS - Fresh Up your Maths Skills",
    "section": "",
    "text": "Crash-Kurs durch die Mathematik für Studierende der Informatik / Wirtschaftsinformatik. Quasi ein kleiner Vorkurs mit den wichtistens Wiederholungen der Schulmathematik.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdrückliches und schriftliches Einverständnis benutzt werden.\nFür alle anderen gilt:\n© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "projekte/WS-Quarto/index.html",
    "href": "projekte/WS-Quarto/index.html",
    "title": "Workshop zum Thema quarto",
    "section": "",
    "text": "Workshop rund um das Thema “quarto”. Mit den Themen:\n\nWas ist quarto und was ist es nicht\nWie installiere ich quarto\nAuswahl der Ausgabe-formate in quarto\nWie erzeuge ich die Ausgaben (aka rendern von Dokumenten)\nDie quarto CLI\nAufbau der YAML-Kopf eines quarto markdown documents (=.qmd)\nGrundlagen des markdowns\nImplizite und expliziete Formatierung mittels quarto\nErstellung von Webseiten, RevealJS-Präsentationen und PDF Dokumente"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Blog",
    "section": "",
    "text": "Statistik21\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2014\n\n\nKarsten Lübke, Matthias Gehrke, Norman Markgaf\n\n\n\n\n\n\n\n\n\n\n\n\nLEARN2CONTROL\n\n\nProjektorientiertes Lernen für ein besseres Gesamtverständnis in der regelungstechnischen Ausbildung\n\n\n\n\n\n\n\n\nSep 1, 2004\n\n\nAndreas Liefeld, Marten Völker, Manuel Remelhe, Kai Dadhe, Sebastian Engell, Carsten Fritsch, Norman Markgaf\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Datenschutzerklaerung.html",
    "href": "Datenschutzerklaerung.html",
    "title": "Datenschutzerklaerung",
    "section": "",
    "text": "Personenbezogene Daten (nachfolgend zumeist nur „Daten“ genannt) werden von uns nur im Rahmen der Erforderlichkeit sowie zum Zwecke der Bereitstellung eines funktionsfähigen und nutzerfreundlichen Internetauftritts, inklusive seiner Inhalte und der dort angebotenen Leistungen, verarbeitet. Gemäß Art. 4 Ziffer 1. der Verordnung (EU) 2016/679, also der Datenschutz-Grundverordnung (nachfolgend nur „DSGVO“ genannt), gilt als „Verarbeitung“ jeder mit oder ohne Hilfe automatisierter Verfahren ausgeführter Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten, wie das Erheben, das Erfassen, die Organisation, das Ordnen, die Speicherung, die Anpassung oder Veränderung, das Auslesen, das Abfragen, die Verwendung, die Offenlegung durch Übermittlung, Verbreitung oder eine andere Form der Bereitstellung, den Abgleich oder die Verknüpfung, die Einschränkung, das Löschen oder die Vernichtung. Mit der nachfolgenden Datenschutzerklärung informieren wir Sie insbesondere über Art, Umfang, Zweck, Dauer und Rechtsgrundlage der Verarbeitung personenbezogener Daten, soweit wir entweder allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung entscheiden. Zudem informieren wir Sie nachfolgend über die von uns zu Optimierungszwecken sowie zur Steigerung der Nutzungsqualität eingesetzten Fremdkomponenten, soweit hierdurch Dritte Daten in wiederum eigener Verantwortung verarbeiten.\nUnsere Datenschutzerklärung ist wie folgt gegliedert:\nI. Informationen über uns als Verantwortliche\n…"
  },
  {
    "objectID": "Datenschutzerklaerung.html#i.-informationen-über-uns-als-verantwortliche",
    "href": "Datenschutzerklaerung.html#i.-informationen-über-uns-als-verantwortliche",
    "title": "Datenschutzerklaerung",
    "section": "I. Informationen über uns als Verantwortliche",
    "text": "I. Informationen über uns als Verantwortliche\nVerantwortlicher Anbieter dieses Internetauftritts im datenschutzrechtlichen Sinne ist:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\nDeutschland\nTelefon: +49-176-20077335\nE-Mail: admin(at)sefiroth.net\n\nDatenschutzbeauftragte/r beim Anbieter ist:\nNorman Markgraf"
  },
  {
    "objectID": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "href": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "title": "Datenschutzerklaerung",
    "section": "III. Informationen zur Datenverarbeitung",
    "text": "III. Informationen zur Datenverarbeitung\nIhre bei Nutzung unseres Internetauftritts verarbeiteten Daten werden gelöscht oder gesperrt, sobald der Zweck der Speicherung entfällt, der Löschung der Daten keine gesetzlichen Aufbewahrungspflichten entgegenstehen und nachfolgend keine anderslautenden Angaben zu einzelnen Verarbeitungsverfahren gemacht werden.\n\nCookies\n\nSitzungs-Cookies/Session-Cookies\n\nWir verwenden mit unserem Internetauftritt sog. Cookies. Cookies sind kleine Textdateien oder andere Speichertechnologien, die durch den von Ihnen eingesetzten Internet-Browser auf Ihrem Endgerät ablegt und gespeichert werden. Durch diese Cookies werden im individuellen Umfang bestimmte Informationen von Ihnen, wie beispielsweise Ihre Browser- oder Standortdaten oder Ihre IP-Adresse, verarbeitet. Durch diese Verarbeitung wird unser Internetauftritt benutzerfreundlicher, effektiver und sicherer, da die Verarbeitung bspw. die Wiedergabe unseres Internetauftritts in unterschiedlichen Sprachen oder das Angebot einer Warenkorbfunktion ermöglicht. Rechtsgrundlage dieser Verarbeitung ist Art. 6 Abs. 1 lit b.) DSGVO, sofern diese Cookies Daten zur Vertragsanbahnung oder Vertragsabwicklung verarbeitet werden. Falls die Verarbeitung nicht der Vertragsanbahnung oder Vertragsabwicklung dient, liegt unser berechtigtes Interesse in der Verbesserung der Funktionalität unseres Internetauftritts. Rechtsgrundlage ist in dann Art. 6 Abs. 1 lit. f) DSGVO. Mit Schließen Ihres Internet-Browsers werden diese Session-Cookies gelöscht.\n\nDrittanbieter-Cookies\n\nGegebenenfalls werden mit unserem Internetauftritt auch Cookies von Partnerunternehmen, mit denen wir zum Zwecke der Werbung, der Analyse oder der Funktionalitäten unseres Internetauftritts zusammenarbeiten, verwendet. Die Einzelheiten hierzu, insbesondere zu den Zwecken und den Rechtsgrundlagen der Verarbeitung solcher Drittanbieter-Cookies, entnehmen Sie bitte den nachfolgenden Informationen.\n\nBeseitigungsmöglichkeit\n\nSie können die Installation der Cookies durch eine Einstellung Ihres Internet-Browsers verhindern oder einschränken. Ebenfalls können Sie bereits gespeicherte Cookies jederzeit löschen. Die hierfür erforderlichen Schritte und Maßnahmen hängen jedoch von Ihrem konkret genutzten Internet-Browser ab. Bei Fragen benutzen Sie daher bitte die Hilfefunktion oder Dokumentation Ihres Internet-Browsers oder wenden sich an dessen Hersteller bzw. Support. Bei sog. Flash-Cookies kann die Verarbeitung allerdings nicht über die Einstellungen des Browsers unterbunden werden. Stattdessen müssen Sie insoweit die Einstellung Ihres Flash-Players ändern. Auch die hierfür erforderlichen Schritte und Maßnahmen hängen von Ihrem konkret genutzten Flash-Player ab. Bei Fragen benutzen Sie daher bitte ebenso die Hilfefunktion oder Dokumentation Ihres Flash-Players oder wenden sich an den Hersteller bzw. Benutzer-Support. Sollten Sie die Installation der Cookies verhindern oder einschränken, kann dies allerdings dazu führen, dass nicht sämtliche Funktionen unseres Internetauftritts vollumfänglich nutzbar sind.\n\n\nGoogle Analytics\nIn unserem Internetauftritt setzen wir Google Analytics ein. Hierbei handelt es sich um einen Webanalysedienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Der Dienst Google Analytics dient zur Analyse des Nutzungsverhaltens unseres Internetauftritts. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Analyse, Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Nutzungs- und nutzerbezogene Informationen, wie bspw. IP-Adresse, Ort, Zeit oder Häufigkeit des Besuchs unseres Internetauftritts, werden dabei an einen Server von Google in den USA übertragen und dort gespeichert. Allerdings nutzen wir Google Analytics mit der sog. Anonymisierungsfunktion. Durch diese Funktion kürzt Google die IP-Adresse schon innerhalb der EU bzw. des EWR. Die so erhobenen Daten werden wiederum von Google genutzt, um uns eine Auswertung über den Besuch unseres Internetauftritts sowie über die dortigen Nutzungsaktivitäten zur Verfügung zu stellen. Auch können diese Daten genutzt werden, um weitere Dienstleistungen zu erbringen, die mit der Nutzung unseres Internetauftritts und der Nutzung des Internets zusammenhängen. Google gibt an, Ihre IP-Adresse nicht mit anderen Daten zu verbinden. Zudem hält Google unter https://www.google.com/intl/de/policies/privacy/partners weitere datenschutzrechtliche Informationen für Sie bereit, so bspw. auch zu den Möglichkeiten, die Datennutzung zu unterbinden. Zudem bietet Google unter https://tools.google.com/dlpage/gaoptout?hl=de ein sog. Deaktivierungs-Add-on nebst weiteren Informationen hierzu an. Dieses Add-on lässt sich mit den gängigen Internet-Browsern installieren und bietet Ihnen weitergehende Kontrollmöglichkeit über die Daten, die Google bei Aufruf unseres Internetauftritts erfasst. Dabei teilt das Add-on dem JavaScript (ga.js) von Google Analytics mit, dass Informationen zum Besuch unseres Internetauftritts nicht an Google Analytics übermittelt werden sollen. Dies verhindert aber nicht, dass Informationen an uns oder an andere Webanalysedienste übermittelt werden. Ob und welche weiteren Webanalysedienste von uns eingesetzt werden, erfahren Sie natürlich ebenfalls in dieser Datenschutzerklärung.\n\n\nGoogle-Maps\nIn unserem Internetauftritt setzen wir Google Maps zur Darstellung unseres Standorts sowie zur Erstellung einer Anfahrtsbeschreibung ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Sofern Sie die in unseren Internetauftritt eingebundene Komponente Google Maps aufrufen, speichert Google über Ihren Internet-Browser ein Cookie auf Ihrem Endgerät. Um unseren Standort anzuzeigen und eine Anfahrtsbeschreibung zu erstellen, werden Ihre Nutzereinstellungen und -daten verarbeitet. Hierbei können wir nicht ausschließen, dass Google Server in den USA einsetzt. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung der Funktionalität unseres Internetauftritts. Durch die so hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Anfahrtsbeschreibung zu übermitteln ist. Sofern Sie mit dieser Verarbeitung nicht einverstanden sind, haben Sie die Möglichkeit, die Installation der Cookies durch die entsprechenden Einstellungen in Ihrem Internet-Browser zu verhindern. Einzelheiten hierzu finden Sie vorstehend unter dem Punkt „Cookies“. Zudem erfolgt die Nutzung von Google Maps sowie der über Google Maps erlangten Informationen nach den Google-Nutzungsbedingungen https://policies.google.com/terms?gl=DE&hl=de und den Geschäftsbedingungen für Google Maps https://www.google.com/intl/de_de/help/terms_maps.html. Überdies bietet Google unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitergehende Informationen an.\n\n\nGoogle reCAPTCHA\nIn unserem Internetauftritt setzen wir Google reCAPTCHA zur Überprüfung und Vermeidung von Interaktionen auf unserer Internetseite durch automatisierte Zugriffe, bspw. durch sog. Bots, ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Durch diesen Dienst kann Google ermitteln, von welcher Webseite eine Anfrage gesendet wird sowie von welcher IP-Adresse aus Sie die sog. reCAPTCHA-Eingabebox verwenden. Neben Ihrer IP-Adresse werden womöglich noch weitere Informationen durch Google erfasst, die für das Angebot und die Gewährleistung dieses Dienstes notwendig sind. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Sicherheit unseres Internetauftritts sowie in der Abwehr unerwünschter, automatisierter Zugriffe in Form von Spam o.ä.. Google bietet unter https://policies.google.com/privacy weitergehende Informationen zu dem allgemeinen Umgang mit Ihren Nutzerdaten an.\n\n\nGoogle Fonts\nIn unserem Internetauftritt setzen wir Google Fonts zur Darstellung externer Schriftarten ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Durch die bei Aufruf unseres Internetauftritts hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Darstellung der Schrift zu übermitteln ist. Google bietet unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitere Informationen an und zwar insbesondere zu den Möglichkeiten der Unterbindung der Datennutzung.\n\n\n„Facebook“-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Facebook ein. Bei Facebook handelt es sich um einen Internetservice der facebook Inc., 1601 S. California Ave, Palo Alto, CA 94304, USA. In der EU wird dieser Service wiederum von der Facebook Ireland Limited, 4 Grand Canal Square, Dublin 2, Irland, betrieben, nachfolgend beide nur „Facebook“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000GnywAAC&status=Active garantiert Facebook, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Weitergehende Informationen über die möglichen Plug-ins sowie über deren jeweilige Funktionen hält Facebook unter https://developers.facebook.com/docs/plugins/ für Sie bereit.\nSofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Facebook in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Facebook Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Facebook eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Facebook erkannt. Die so gesammelten Informationen weist Facebook womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Gefällt mir“-Button von Facebook benutzen, werden diese Informationen in Ihrem Facebook-Nutzerkonto gespeichert und ggf. über die Plattform von Facebook veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Facebook ausloggen oder durch den Einsatz eines Add-ons für Ihren Internetbrowser verhindern, dass das Laden des Facebook-Plug-in blockiert wird. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Facebook in den unter https://www.facebook.com/policy.php abrufbaren Datenschutzhinweisen bereit.\n\n\nX-(ehemals „Twitter“)-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Twitter ein. Bei X handelt es sich um einen Internetservice der X.AI Corp., 795 Folsom St., Suite 600, San Francisco, CA 94107, USA, nachfolgend nur „xAI“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000TORzAAO&status=Active garantiert Twitter, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Sofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Twitter in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Twitter Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Twitter eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Twitter erkannt. Die so gesammelten Informationen weist Twitter womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Teilen“-Button von Twitter benutzen, werden diese Informationen in Ihrem Twitter-Nutzerkonto gespeichert und ggf. über die Plattform von Twitter veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Twitter ausloggen oder die entsprechenden Einstellungen in Ihrem Twitter-Benutzerkonto vornehmen. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Twitter in den unter https://twitter.com/privacy abrufbaren Datenschutzhinweisen bereit.\nMuster-Datenschutzerklärung der Anwaltskanzlei Weiß & Partner"
  }
]