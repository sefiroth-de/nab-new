[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Norman’s Academic Blog",
    "section": "",
    "text": "Willkommen in meinem Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nGrafiken nebeneinander setzen mit ggplot2 oder ggformula\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nCSV Dateien bearbeiten mit Miller\n\n\n\nAllgemeines\n\nMiller\n\nCSV\n\nDatenjudo\n\nDatenformate\n\n\n\n\n\n\n\n\n\nAug 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDatenjudo für Fragebögen\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDinge die man in zwei Dimensionen machen kann - Multiple lineare Regression\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 24, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nRegression mit studentisierten Daten\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nInteraktionseffekte leichter interpretieren durch Transformationen\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nWege zur Normalverteilung\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 11, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nÜber die Koeffizienten einer linearen Regression\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 9, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nGraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest\n\n\n\nPython\n\nPyPy\n\nGraalPython\n\nGraalPy\n\nSpeedtest\n\n\n\n\n\n\n\n\n\nApr 8, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nÜber die t-Verteilung mit einem bzw. zwei Freiheitsgraden\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 17, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nBehäbige Funktionen aka slowly varying function\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nEin paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nGedankenstütze zu wichtigen Funktionsbegriffen in der Statistik\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nCook Abstand\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDer Zentrale Grenzwertsatz\n\n\n\nStatistik\n\n\n\n\n\n\n\n\n\nApr 5, 2017\n\n\nNorman Markgraf\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Norman Markgraf",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\nNorman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer. Als Lehrbeauftragter hat er an der Hochschule Bochum, Hochschule Rhein-Waal (am Campus Kleve) und der FOM Hochschule für Oekonomie und Management an den Standorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss, Wuppertal und München verschiedene Lehrveranstaltungen abgehalten."
  },
  {
    "objectID": "index.html#biografie",
    "href": "index.html#biografie",
    "title": "Norman Markgraf",
    "section": "",
    "text": "Norman Markgraf ist freiberuflicher Dozent für Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer. Als Lehrbeauftragter hat er an der Hochschule Bochum, Hochschule Rhein-Waal (am Campus Kleve) und der FOM Hochschule für Oekonomie und Management an den Standorten Aachen, Bonn, Dortmund, Duisburg, Düsseldorf, Gütersloh (Bertelsmann), Köln, Münster, Neuss, Wuppertal und München verschiedene Lehrveranstaltungen abgehalten."
  },
  {
    "objectID": "index.html#interessen",
    "href": "index.html#interessen",
    "title": "Norman Markgraf",
    "section": "Interessen",
    "text": "Interessen\n\nInteressen\nMathematik\nStatistik\nDatenanalyse\nData Literacy\nDatenkompetenz\nData Science\nFinanzmathematik\nWirtschaftsmathematik\nIngenieurmathematik\nDatenbanken\nBig Data"
  },
  {
    "objectID": "index.html#bildung",
    "href": "index.html#bildung",
    "title": "Norman Markgraf",
    "section": "Bildung",
    "text": "Bildung\n\nWissenschaftlicher Mitarbeiter am Lehrstuhl für Prozessinformatik der Fakultät für Elektro- und Informationstechnik, 2006 Ruhr-Universität Bochum\nDiplom-Mathematiker mit dem Schwerpunkt Informatik und dem Nebenfach Wirtschaftsinformatik, 1992 Ruhr-Universität Bochum"
  },
  {
    "objectID": "index.html#kontakt-impressum",
    "href": "index.html#kontakt-impressum",
    "title": "Norman Markgraf",
    "section": "Kontakt / Impressum",
    "text": "Kontakt / Impressum\n\nAngabe gemäß §5 TMG:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\n\n\nHaftungsausschluss - Disclaimer\n\nHaftung für Inhalte\nAlle Inhalte unseres Internetauftritts wurden mit größter Sorgfalt und nach bestem Gewissen erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt.\nEine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverzüglich entfernen.\n\n\nHaftungsbeschränkung für externe Links\nUnsere Webseite enthält Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher können wir für die „externen Links“ auch keine Gewähr auf Richtigkeit der Inhalte übernehmen. Für die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverstöße überprüft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine ständige inhaltliche Überprüfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht möglich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die außerhalb unseres Verantwortungsbereichs liegen, würde eine Haftungsverpflichtung ausschließlich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch möglich und zumutbar wäre, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\nDiese Haftungsausschlusserklärung gilt auch innerhalb des eigenen Internetauftrittes „Name Ihrer Domain“ gesetzten Links und Verweise von Fragestellern, Blogeinträgern, Gästen des Diskussionsforums. Für illegale, fehlerhafte oder unvollständige Inhalte und insbesondere für Schäden, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der über Links auf die jeweilige Veröffentlichung lediglich verweist.\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverzüglich entfernt.\n\n\nUrheberrecht\nDie auf unserer Webseite veröffentlichen Inhalte und Werke unterliegen dem deutschen Urheberrecht (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external)) . Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers außerhalb der Grenzen des Urheberrechtes bedürfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external) ). Downloads und Kopien dieser Seite sind nur für den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverzüglich entfernen.\nDieses Impressum wurde freundlicherweise von (www.jurarat.de) zur Verfügung gestellt."
  },
  {
    "objectID": "index.html#impressum",
    "href": "index.html#impressum",
    "title": "Norman Markgraf",
    "section": "Impressum",
    "text": "Impressum\n\nAngabe gemäß §5 TMG:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\n\n\nHaftungsausschluss - Disclaimer\n\nHaftung für Inhalte\nAlle Inhalte unseres Internetauftritts wurden mit größter Sorgfalt und nach bestem Gewissen erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt.\nEine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverzüglich entfernen.\n\n\nHaftungsbeschränkung für externe Links\nUnsere Webseite enthält Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher können wir für die „externen Links“ auch keine Gewähr auf Richtigkeit der Inhalte übernehmen. Für die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverstöße überprüft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine ständige inhaltliche Überprüfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht möglich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die außerhalb unseres Verantwortungsbereichs liegen, würde eine Haftungsverpflichtung ausschließlich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch möglich und zumutbar wäre, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\nDiese Haftungsausschlusserklärung gilt auch innerhalb des eigenen Internetauftrittes „Norman’s Academic Blog“ gesetzten Links und Verweise von Fragestellern, Blogeinträgern, Gästen des Diskussionsforums. Für illegale, fehlerhafte oder unvollständige Inhalte und insbesondere für Schäden, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der über Links auf die jeweilige Veröffentlichung lediglich verweist.\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverzüglich entfernt.\n\n\nUrheberrecht\nDie auf unserer Webseite veröffentlichen Inhalte und Werke unterliegen dem deutschen Urheberrecht (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external)) . Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers außerhalb der Grenzen des Urheberrechtes bedürfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external) ). Downloads und Kopien dieser Seite sind nur für den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverzüglich entfernen.\nDieses Impressum wurde freundlicherweise von (www.jurarat.de) zur Verfügung gestellt."
  },
  {
    "objectID": "posts/Willkommen/index.html",
    "href": "posts/Willkommen/index.html",
    "title": "Willkommen in meinem Blog",
    "section": "",
    "text": "Das ist mein erster Blog-Eintrag! Ich werde nun hoffentlich (langsam) meinen Blog von Hugo aus Quarto umstellen. Ich arbeite viel mit R und eine extra Sprache nur für den Blog ist mir einfach auf Dauer zu umständlich. Aber mal sehen, wann ich Zeit finde dieses Projekt umzustezen."
  },
  {
    "objectID": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "href": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "title": "CSV Dateien bearbeiten mit Miller",
    "section": "",
    "text": "Miller beschreibt sich selbst folgendermaßen:\n\nMiller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. You get to work with your data using named fields, without needing to count positional column indices.\n\nMiller kombiniert die Funktionalität von awk, sed und cut und eignet sich besonders für feldbasierte Datenmanipulation.\n\nBeispiel 1: Erstellung einer Markdown-Tabelle\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --omd cat\nMit diesem Befehl wird eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, anschließend werden alle Kommata durch Punkte ersetzt und schließlich wird daraus eine Markdown-Tabelle erzeugt.\n\n\nBeispiel 2: Gerahmte Darstellung\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --opprint --barred cat\nMit diesem Befehl wird ebenfalls eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, alle Kommata werden durch Punkte ersetzt und am Ende wird eine eingerahmte Tabelle erzeugt."
  },
  {
    "objectID": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html",
    "href": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "href": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#neues-setup",
    "href": "posts/2021-04-08-graalvm-21-0-0-pypy-3-7-7-3-3-und-python3-9-2-im-kurzen-test/index.html#neues-setup",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse für n = 11 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‘import nqueens; nqueens.main()’\nerzeugt.\n\n\nDie Ergebnisse des Test vom 20.7.2025\n\nCPython:\n\n3.13.1: 1 loop, best of 50: 673 msec per loop\n3.13.5: 1 loop, best of 50: 661 msec per loop\n\nPyPy:\n\n3.10-7.3.17: 1 loops, average of 50: 75.6 +- 2.9 msec per loop (using standard deviation)\n3.11-7.3.19: 1 loops, average of 50: 74.9 +- 2.89 msec per loop (using standard deviation)\n\nGraalPython/GraalPy\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1):1 loop, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 1 loop, best of 50: 30 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 1 loop, best of 50: 24.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 1 loop, best of 50: 28.1 msec per loop"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgeführt.\nAls Testfall diente das klassische n-Damen-Problem, dessen Lösungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse für n = 11 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‘import nqueens; nqueens.main()’\nerzeugt.\n\n\nDie Ergebnisse des Test vom 20.7.2025\n\nCPython:\n\n3.13.1: 1 loop, best of 50: 673 msec per loop\n3.13.5: 1 loop, best of 50: 661 msec per loop\n\nPyPy:\n\n3.10-7.3.17: 1 loops, average of 50: 75.6 +- 2.9 msec per loop (using standard deviation)\n3.11-7.3.19: 1 loops, average of 50: 74.9 +- 2.89 msec per loop (using standard deviation)\n\nGraalPython/GraalPy\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1):1 loop, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 1 loop, best of 50: 30 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 1 loop, best of 50: 24.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 1 loop, best of 50: 28.1 msec per loop"
  },
  {
    "objectID": "Datenschutzerklaerung.html",
    "href": "Datenschutzerklaerung.html",
    "title": "Datenschutzerklaerung",
    "section": "",
    "text": "Personenbezogene Daten (nachfolgend zumeist nur „Daten“ genannt) werden von uns nur im Rahmen der Erforderlichkeit sowie zum Zwecke der Bereitstellung eines funktionsfähigen und nutzerfreundlichen Internetauftritts, inklusive seiner Inhalte und der dort angebotenen Leistungen, verarbeitet. Gemäß Art. 4 Ziffer 1. der Verordnung (EU) 2016/679, also der Datenschutz-Grundverordnung (nachfolgend nur „DSGVO“ genannt), gilt als „Verarbeitung“ jeder mit oder ohne Hilfe automatisierter Verfahren ausgeführter Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten, wie das Erheben, das Erfassen, die Organisation, das Ordnen, die Speicherung, die Anpassung oder Veränderung, das Auslesen, das Abfragen, die Verwendung, die Offenlegung durch Übermittlung, Verbreitung oder eine andere Form der Bereitstellung, den Abgleich oder die Verknüpfung, die Einschränkung, das Löschen oder die Vernichtung. Mit der nachfolgenden Datenschutzerklärung informieren wir Sie insbesondere über Art, Umfang, Zweck, Dauer und Rechtsgrundlage der Verarbeitung personenbezogener Daten, soweit wir entweder allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung entscheiden. Zudem informieren wir Sie nachfolgend über die von uns zu Optimierungszwecken sowie zur Steigerung der Nutzungsqualität eingesetzten Fremdkomponenten, soweit hierdurch Dritte Daten in wiederum eigener Verantwortung verarbeiten.\nUnsere Datenschutzerklärung ist wie folgt gegliedert:\nI. Informationen über uns als Verantwortliche\n…"
  },
  {
    "objectID": "Datenschutzerklaerung.html#i.-informationen-über-uns-als-verantwortliche",
    "href": "Datenschutzerklaerung.html#i.-informationen-über-uns-als-verantwortliche",
    "title": "Datenschutzerklaerung",
    "section": "I. Informationen über uns als Verantwortliche",
    "text": "I. Informationen über uns als Verantwortliche\nVerantwortlicher Anbieter dieses Internetauftritts im datenschutzrechtlichen Sinne ist:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\nDeutschland\nTelefon: +49-176-20077335\nE-Mail: admin(at)sefiroth.net\n\nDatenschutzbeauftragte/r beim Anbieter ist:\nNorman Markgraf"
  },
  {
    "objectID": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "href": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "title": "Datenschutzerklaerung",
    "section": "III. Informationen zur Datenverarbeitung",
    "text": "III. Informationen zur Datenverarbeitung\nIhre bei Nutzung unseres Internetauftritts verarbeiteten Daten werden gelöscht oder gesperrt, sobald der Zweck der Speicherung entfällt, der Löschung der Daten keine gesetzlichen Aufbewahrungspflichten entgegenstehen und nachfolgend keine anderslautenden Angaben zu einzelnen Verarbeitungsverfahren gemacht werden.\n\nCookies\n\nSitzungs-Cookies/Session-Cookies\n\nWir verwenden mit unserem Internetauftritt sog. Cookies. Cookies sind kleine Textdateien oder andere Speichertechnologien, die durch den von Ihnen eingesetzten Internet-Browser auf Ihrem Endgerät ablegt und gespeichert werden. Durch diese Cookies werden im individuellen Umfang bestimmte Informationen von Ihnen, wie beispielsweise Ihre Browser- oder Standortdaten oder Ihre IP-Adresse, verarbeitet. Durch diese Verarbeitung wird unser Internetauftritt benutzerfreundlicher, effektiver und sicherer, da die Verarbeitung bspw. die Wiedergabe unseres Internetauftritts in unterschiedlichen Sprachen oder das Angebot einer Warenkorbfunktion ermöglicht. Rechtsgrundlage dieser Verarbeitung ist Art. 6 Abs. 1 lit b.) DSGVO, sofern diese Cookies Daten zur Vertragsanbahnung oder Vertragsabwicklung verarbeitet werden. Falls die Verarbeitung nicht der Vertragsanbahnung oder Vertragsabwicklung dient, liegt unser berechtigtes Interesse in der Verbesserung der Funktionalität unseres Internetauftritts. Rechtsgrundlage ist in dann Art. 6 Abs. 1 lit. f) DSGVO. Mit Schließen Ihres Internet-Browsers werden diese Session-Cookies gelöscht.\n\nDrittanbieter-Cookies\n\nGegebenenfalls werden mit unserem Internetauftritt auch Cookies von Partnerunternehmen, mit denen wir zum Zwecke der Werbung, der Analyse oder der Funktionalitäten unseres Internetauftritts zusammenarbeiten, verwendet. Die Einzelheiten hierzu, insbesondere zu den Zwecken und den Rechtsgrundlagen der Verarbeitung solcher Drittanbieter-Cookies, entnehmen Sie bitte den nachfolgenden Informationen.\n\nBeseitigungsmöglichkeit\n\nSie können die Installation der Cookies durch eine Einstellung Ihres Internet-Browsers verhindern oder einschränken. Ebenfalls können Sie bereits gespeicherte Cookies jederzeit löschen. Die hierfür erforderlichen Schritte und Maßnahmen hängen jedoch von Ihrem konkret genutzten Internet-Browser ab. Bei Fragen benutzen Sie daher bitte die Hilfefunktion oder Dokumentation Ihres Internet-Browsers oder wenden sich an dessen Hersteller bzw. Support. Bei sog. Flash-Cookies kann die Verarbeitung allerdings nicht über die Einstellungen des Browsers unterbunden werden. Stattdessen müssen Sie insoweit die Einstellung Ihres Flash-Players ändern. Auch die hierfür erforderlichen Schritte und Maßnahmen hängen von Ihrem konkret genutzten Flash-Player ab. Bei Fragen benutzen Sie daher bitte ebenso die Hilfefunktion oder Dokumentation Ihres Flash-Players oder wenden sich an den Hersteller bzw. Benutzer-Support. Sollten Sie die Installation der Cookies verhindern oder einschränken, kann dies allerdings dazu führen, dass nicht sämtliche Funktionen unseres Internetauftritts vollumfänglich nutzbar sind.\n\n\nGoogle Analytics\nIn unserem Internetauftritt setzen wir Google Analytics ein. Hierbei handelt es sich um einen Webanalysedienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Der Dienst Google Analytics dient zur Analyse des Nutzungsverhaltens unseres Internetauftritts. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Analyse, Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Nutzungs- und nutzerbezogene Informationen, wie bspw. IP-Adresse, Ort, Zeit oder Häufigkeit des Besuchs unseres Internetauftritts, werden dabei an einen Server von Google in den USA übertragen und dort gespeichert. Allerdings nutzen wir Google Analytics mit der sog. Anonymisierungsfunktion. Durch diese Funktion kürzt Google die IP-Adresse schon innerhalb der EU bzw. des EWR. Die so erhobenen Daten werden wiederum von Google genutzt, um uns eine Auswertung über den Besuch unseres Internetauftritts sowie über die dortigen Nutzungsaktivitäten zur Verfügung zu stellen. Auch können diese Daten genutzt werden, um weitere Dienstleistungen zu erbringen, die mit der Nutzung unseres Internetauftritts und der Nutzung des Internets zusammenhängen. Google gibt an, Ihre IP-Adresse nicht mit anderen Daten zu verbinden. Zudem hält Google unter https://www.google.com/intl/de/policies/privacy/partners weitere datenschutzrechtliche Informationen für Sie bereit, so bspw. auch zu den Möglichkeiten, die Datennutzung zu unterbinden. Zudem bietet Google unter https://tools.google.com/dlpage/gaoptout?hl=de ein sog. Deaktivierungs-Add-on nebst weiteren Informationen hierzu an. Dieses Add-on lässt sich mit den gängigen Internet-Browsern installieren und bietet Ihnen weitergehende Kontrollmöglichkeit über die Daten, die Google bei Aufruf unseres Internetauftritts erfasst. Dabei teilt das Add-on dem JavaScript (ga.js) von Google Analytics mit, dass Informationen zum Besuch unseres Internetauftritts nicht an Google Analytics übermittelt werden sollen. Dies verhindert aber nicht, dass Informationen an uns oder an andere Webanalysedienste übermittelt werden. Ob und welche weiteren Webanalysedienste von uns eingesetzt werden, erfahren Sie natürlich ebenfalls in dieser Datenschutzerklärung.\n\n\nGoogle-Maps\nIn unserem Internetauftritt setzen wir Google Maps zur Darstellung unseres Standorts sowie zur Erstellung einer Anfahrtsbeschreibung ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Sofern Sie die in unseren Internetauftritt eingebundene Komponente Google Maps aufrufen, speichert Google über Ihren Internet-Browser ein Cookie auf Ihrem Endgerät. Um unseren Standort anzuzeigen und eine Anfahrtsbeschreibung zu erstellen, werden Ihre Nutzereinstellungen und -daten verarbeitet. Hierbei können wir nicht ausschließen, dass Google Server in den USA einsetzt. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung der Funktionalität unseres Internetauftritts. Durch die so hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Anfahrtsbeschreibung zu übermitteln ist. Sofern Sie mit dieser Verarbeitung nicht einverstanden sind, haben Sie die Möglichkeit, die Installation der Cookies durch die entsprechenden Einstellungen in Ihrem Internet-Browser zu verhindern. Einzelheiten hierzu finden Sie vorstehend unter dem Punkt „Cookies“. Zudem erfolgt die Nutzung von Google Maps sowie der über Google Maps erlangten Informationen nach den Google-Nutzungsbedingungen https://policies.google.com/terms?gl=DE&hl=de und den Geschäftsbedingungen für Google Maps https://www.google.com/intl/de_de/help/terms_maps.html. Überdies bietet Google unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitergehende Informationen an.\n\n\nGoogle reCAPTCHA\nIn unserem Internetauftritt setzen wir Google reCAPTCHA zur Überprüfung und Vermeidung von Interaktionen auf unserer Internetseite durch automatisierte Zugriffe, bspw. durch sog. Bots, ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Durch diesen Dienst kann Google ermitteln, von welcher Webseite eine Anfrage gesendet wird sowie von welcher IP-Adresse aus Sie die sog. reCAPTCHA-Eingabebox verwenden. Neben Ihrer IP-Adresse werden womöglich noch weitere Informationen durch Google erfasst, die für das Angebot und die Gewährleistung dieses Dienstes notwendig sind. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Sicherheit unseres Internetauftritts sowie in der Abwehr unerwünschter, automatisierter Zugriffe in Form von Spam o.ä.. Google bietet unter https://policies.google.com/privacy weitergehende Informationen zu dem allgemeinen Umgang mit Ihren Nutzerdaten an.\n\n\nGoogle Fonts\nIn unserem Internetauftritt setzen wir Google Fonts zur Darstellung externer Schriftarten ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur „Google“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu ermöglichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Durch die bei Aufruf unseres Internetauftritts hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Darstellung der Schrift zu übermitteln ist. Google bietet unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitere Informationen an und zwar insbesondere zu den Möglichkeiten der Unterbindung der Datennutzung.\n\n\n„Facebook“-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Facebook ein. Bei Facebook handelt es sich um einen Internetservice der facebook Inc., 1601 S. California Ave, Palo Alto, CA 94304, USA. In der EU wird dieser Service wiederum von der Facebook Ireland Limited, 4 Grand Canal Square, Dublin 2, Irland, betrieben, nachfolgend beide nur „Facebook“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000GnywAAC&status=Active garantiert Facebook, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Weitergehende Informationen über die möglichen Plug-ins sowie über deren jeweilige Funktionen hält Facebook unter https://developers.facebook.com/docs/plugins/ für Sie bereit.\nSofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Facebook in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Facebook Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Facebook eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Facebook erkannt. Die so gesammelten Informationen weist Facebook womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Gefällt mir“-Button von Facebook benutzen, werden diese Informationen in Ihrem Facebook-Nutzerkonto gespeichert und ggf. über die Plattform von Facebook veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Facebook ausloggen oder durch den Einsatz eines Add-ons für Ihren Internetbrowser verhindern, dass das Laden des Facebook-Plug-in blockiert wird. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Facebook in den unter https://www.facebook.com/policy.php abrufbaren Datenschutzhinweisen bereit.\n\n\nX-(ehemals „Twitter“)-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Twitter ein. Bei X handelt es sich um einen Internetservice der X.AI Corp., 795 Folsom St., Suite 600, San Francisco, CA 94107, USA, nachfolgend nur „xAI“ genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild („EU-US Privacy Shield“) https://www.privacyshield.gov/participant?id=a2zt0000000TORzAAO&status=Active garantiert Twitter, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualitätsverbesserung unseres Internetauftritts. Sofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, lädt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Twitter in den USA herunter. Aus technischen Gründen ist es dabei notwendig, dass Twitter Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Twitter eingeloggt sein, während Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Twitter erkannt. Die so gesammelten Informationen weist Twitter womöglich Ihrem dortigen persönlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. „Teilen“-Button von Twitter benutzen, werden diese Informationen in Ihrem Twitter-Nutzerkonto gespeichert und ggf. über die Plattform von Twitter veröffentlicht. Wenn Sie das verhindern möchten, müssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Twitter ausloggen oder die entsprechenden Einstellungen in Ihrem Twitter-Benutzerkonto vornehmen. Weitergehende Informationen über die Erhebung und Nutzung von Daten sowie Ihre diesbezüglichen Rechte und Schutzmöglichkeiten hält Twitter in den unter https://twitter.com/privacy abrufbaren Datenschutzhinweisen bereit.\nMuster-Datenschutzerklärung der Anwaltskanzlei Weiß & Partner"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabhängige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nFür die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt für die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie für wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ für }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "Ein Beispiel:",
    "text": "Ein Beispiel:\nNehmen wir drei Verteilungen mit Zufallsvariable \\(U\\), \\(X\\), \\(Y\\) und jeweils \\(n\\) Realisationen \\(u_1,\\dots, u_n\\), \\(x_1,\\dots, x_n\\), \\(y_1,\\dots, y_n\\).\nWählen wir zunächst \\(n=5\\):\n\nu\n\n[1] 19.726 69.683 60.790  0.955 42.901\n\nx\n\n[1]  7.942 15.905 12.917  6.818  4.434\n\ny\n\n[1] 59.961 56.552 51.094 75.288 47.985\n\n\nStandardisieren wir die Werte:\n\nlibrary(mosaic)\nzscore(u)\n\n[1] -0.6695256  1.0830283  0.7710507 -1.3280357  0.1434823\n\nzscore(x)\n\n[1] -0.3543069  1.3440714  0.7067796 -0.5940379 -1.1025063\n\nzscore(y)\n\n[1]  0.1677971 -0.1526624 -0.6657361  1.6085958 -0.9579944\n\n\nDie Behauptung des Zentralengrenzwertsatzes ist nun, dass mit steigender Anzahl an Werten \\(n\\) die standardisierten Werte in der empirischen Verteilungsfunktion sich immer mehr der Verteilungsfunktion der Standardnormalverteilung annähern:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeiterführende Literatur und Quellen dieses Eintrags:"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu möchte mensch zwei (oder mehr) Grafiken neben- oder übereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zunächst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausführen oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu möchte mensch zwei (oder mehr) Grafiken neben- oder übereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zunächst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausführen oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "",
    "text": "Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine möglichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt. Dabei tolerieren wir eine (kleine) Abweichung \\(e_i\\).\nBei einer einfachen linearen Regression gehen wir davon aus, dass die Datenpunkte (im wesentlichen) auf einer Geraden liegen. Mit \\(g(x)=\\beta_0 + \\beta1 \\cdot x\\) ergibt sich dann für die Datenpunkte die Gleichung:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i\\]\nUnsere Aufgabe besteht nun darin die Parameter \\(\\beta_0\\) (y-Achsenabschnitt) und \\(\\beta_1\\) (Steigung) an Hand der \\(n\\) Datenpunkte zu schätzen. Alle unsere Schätzungen kennzeichnen wir mit einem Dach (\\(\\hat{.}\\)), um sie von den (in der Regel unbekannten) Parametern besser zu unterscheiden.\nWir suchen somit nach \\(\\hat\\beta= \\left(\\hat\\beta_0,\\, \\hat\\beta_1\\right)\\), so dass die Gerade \\(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x\\) zu gegebenem \\(x_i\\) eine möglichst gute Schätzung von \\(y_i\\) (genannt \\(\\hat{y}_i\\)) hat:\n\\[\n\\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i\n\\]\nDie Abweichung \\(\\hat{e_i}\\) unserer Schätzung \\(\\hat{y}_i\\) von dem gegebenen Wert \\(y_i\\) lässt sich schreiben als:\n\\[\n\\hat{e_i} =  \\hat{y_i} - y_i =  \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i\n\\]\nWenn wir diese Abweichung über alle \\(i\\) minimieren, finden wir unser \\(\\hat\\beta\\).\nDoch das wirft eine Frage auf: Wie genau messen wir die möglichst kleinste Abweichung der \\(\\hat{e_i}\\) konkret?\nWir betrachten zunächst drei einfache Ideen:\nGewöhnlich nutzen wir die quadratischen Abweichungen, weshalb wir die drei Ideen ebenso in umgekehrter Reihenfolge betrachten wollen:"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "3. Idee: Summe der quadratischen Abweichungen",
    "text": "3. Idee: Summe der quadratischen Abweichungen\nWir bezeichnen mit\n\\[\\begin{aligned}\nQS &= QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n  &= \\sum\\limits_{i=1}^n \\hat{e_i}^2 = \\sum\\limits_{i=1}^n \\left(\\hat{y_i} - y_i \\right)^2 \\\\\n  &= \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\n\\end{aligned}\\]\ndie Quadrat-Summe der Abweichungen.\nGesucht wird \\(\\hat\\beta=\\left(\\hat\\beta_0,\\,\\hat\\beta_1\\right)\\), so das \\(QS\\) minimiert wird.\nDies ist ein Minimierungsproblem, bei dem wir zu mindestens eine (exakte) mathematisch-algebraisch Lösung in Form eines stationären Punktes finden können. Dazu berechnen wir die Nullstelle der ersten partiellen Ableitung von \\(QS\\) nach \\(\\hat\\beta_0\\) bzw. \\(\\hat\\beta_1\\).\n\nVorbemerkungen\nWegen \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i\\) ist \\(n \\cdot \\bar{x} =\\sum\\limits_{i=1}^n x_i\\) und analog \\(n \\cdot \\bar{y} =\\sum\\limits_{i=1}^n y_i\\)\n\n\nSchätzen des y-Achenabschnitts \\(\\hat\\beta_0\\)\nEs ist:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot 1 \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 + \\sum\\limits_{i=1}^n\\hat\\beta_1 \\cdot x_i - \\sum\\limits_{i=1}^n y_i\\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot\\sum\\limits_{i=1}^n x_i - \\sum\\limits_{i=1}^n y_i \\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot n \\cdot \\bar{x} - n \\cdot\\bar{y} \\right) \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right)\n\\end{aligned}\\]\nUm stationäre Punkte zu ermitteln, müssen wir den Ausdruck nun gleich Null setzen und erhalten:\n\\[\\begin{aligned}\n  0 &= \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\qquad | : (2 \\cdot n) \\\\\n  &= \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y}\n\\end{aligned}\\]\nStellen wir nach \\(\\hat\\beta_0\\) um, erhalten wir:\n\\[\\begin{aligned}\n  \\hat\\beta_0 &= - \\hat\\beta_1\\cdot\\bar{x} + \\bar{y} \\\\\n  \\hat\\beta_0 &= \\bar{y} - \\hat\\beta_1\\cdot\\bar{x}\n\\end{aligned}\\]\nUm \\(\\hat\\beta_0\\) zu bestimmen, benötigen wir \\(\\hat\\beta_1\\).\n\n\nSchätzen der Steigung \\(\\hat\\beta_1\\)\nEs ist:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot x_i \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 \\cdot x_i + \\sum\\limits_{i=1}^n \\hat\\beta_1 \\cdot x_i\\cdot x_i- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot \\sum\\limits_{i=1}^n  x_i + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right)\n\\end{aligned}\\]\nWir ersetzen nun \\(\\hat\\beta_0\\) durch \\(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\) und erhalten:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  &=\n  2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(\\left(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\right) \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - n \\cdot \\hat\\beta_1 \\cdot  \\bar{x}^2  + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n\\end{aligned}\\]\nMit Hilfe des Verschiebesatzes von Steiner (zweimal angewendet) erhalten wir:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  \n    &=2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)+ \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(\\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)\n\\end{aligned}\\]\nWir setzen nun wieder den Ausdruck gleich Null:\n\\[\\begin{aligned}\n0 &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)  \\qquad | : 2\\\\\n   &= \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{aligned}\\]\nUnd stellen dann nach \\(\\hat\\beta_1\\) um:\n\\[\\begin{aligned}\n  \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\n    &= \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\\\\n  \\hat\\beta_1\n    &= \\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\n\\end{aligned}\\]\nWir können nun Zähler und Nenner der rechten Seite mit \\(\\frac{1}{n}\\) erweitern und erhalten so:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\\\\n\\end{aligned}\\]\nOder aber wir erweitern mit \\(\\frac{1}{n-1}\\) und erhalten:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{s_{x,y}}{s^2_{x}}\n\\end{aligned}\\]\nDamit können wir zur Berechnung sowohl die Kovarianz der Grundgesamtheit \\(\\sigma_{x,y}\\) und die Varianz \\(\\sigma^2_x\\) von \\(x\\), als auch deren Schätzer \\(s_{x,y}\\) und \\(s^2_x\\) verwendet werden!\nDiese Methode nennt sich Methode der kleinsten Quadrate (engl. ordenary least square method) und wir sprechen dann auch von den Kleinste-Quadrate-Schätzern (oder kurz KQ-Schätzer bzw. OLS-Schätzer) \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\).\nErweitern wir den Ausdruck mit Standardabweichung \\(\\sigma_y\\) bzw. \\(s_y\\), so erhalten wir:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n&= \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\end{aligned}\\]\nund analog für die Schätzer:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{s_{x,y}}{s^2_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_y} \\cdot \\frac{s_y}{s_x} \\\\\n&= r_{x,y} \\cdot \\frac{s_y}{s_x} \\\\\n\\end{aligned}\\]\nDie Steigung \\(\\hat\\beta_1\\) hat somit eine direkte Beziehung mit dem Korrelationskoeffizenten \\(\\rho\\) (der Grundgesamtheit) bzw. \\(r\\) (der Stichprobe).\nFür eine Berechnung in R heißt dies: wir können die Regressionskoeffizienten \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\) direkt algebraisch ausrechnen, wenn wir\n\ndie Standardabweichungen von \\(x\\) und \\(y\\) und den Korrelationskoeffizienten oder\ndie Varianz von \\(x\\) und Kovarianz von \\(x\\) und \\(y\\)\n\nhaben.\n\n\nEin Beispiel in R:\nAuf Grundlage der Datentabelle mtcars wollen wir Prüfen wie ein linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdestärke hp) modelliert werden kann.1\n\nlibrary(mosaic)\n\n# Wir nehmen die Datentabelle 'mtcars':\nmtcars %&gt;%\n  select(hp, mpg) -&gt; dt\n\n# Ein kurzer Blick auf die Daten:\nfavstats(~ hp, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  146.6875 68.56287\nfavstats(~ mpg, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  20.09062 6.026948\n\n# Wir vergleichen den Verbrauch (mpg, miles per gallon) \n# mit den Pferdestärken (hp) mit Hilfe eines Streudiagramms:\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir zunächst die Mittelwerte von \\(x\\) (also ‘hp’) und \\(y\\) (also ‘mpg’)\n\n(mean_hp &lt;- mean(~ hp, data = dt))\n#&gt; [1] 146.6875\n(mean_mpg &lt;- mean(~ mpg, data = dt))\n#&gt; [1] 20.09062\n\nund zeichnen die Punkt \\((\\bar{x}, \\bar{y}) = (146.69, 20.09)\\) in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir nun die Schätzwerte für die Regressionsgerade\n\n(beta_1 &lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt))\n#&gt; [1] -0.06822828\n(beta_0 &lt;- mean_mpg - beta_1 * mean_hp)\n#&gt; [1] 30.09886\n\nund zeichnen diese in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \"dodgerblue\") %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988605 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nStudentisieren – einmal hin und einmal zurück\nWas passiert eigentlich, wenn wir unsere \\(x\\) und \\(y\\) Werte studentisieren (aka standardisieren oder z-transformieren)?\nZur Erinnerung, studentisieren geht so: \\[\nx^{stud} = \\frac{x - \\bar{x}}{s_x}\n\\]\nIn R können wir das mit der Funktion ‘zscore’ wie folgt machen:\n\ndt %&gt;%\n  mutate(\n    hp_stud = zscore(hp),\n    mpg_stud = zscore(mpg)\n  ) -&gt; dt\n\nNatürlich sind die Mittelwerte nun Null und die Standardabweichungen Eins:\n\nfavstats(~ hp_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;           mean sd\n#&gt;  -4.857226e-17  1\nfavstats(~ mpg_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;          mean sd\n#&gt;  4.336809e-17  1\n\nDer Grund für die kleinen Abweichungen von der Null bei den Mittelwerten sind unumgängliche Rundungsfehler, die der Computer macht!\nSchauen wir uns nun das Streudiagramm an, zusammen mit dem Mittelpunkt \\((0,0)\\)\n\ngf_point(mpg_stud ~ hp_stud, data = dt) %&gt;%\n  gf_point(0 ~ 0, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(-2, 2))\n\n\n\n\n\n\n\n\nAuch wenn die Skalierungen sich geändert haben, die Diagramme sind sehr ähnlich.\nBestimmen wir die Koeffizienten der Regressionsgerade\n\n(beta_stud_1 &lt;- cov(mpg_stud ~ hp_stud, data = dt))\n#&gt; [1] -0.7761684\n(beta_stud_0 &lt;- 0 - beta_stud_1 * 0)\n#&gt; [1] 0\n\nund setzen sie in das Streudiagramm ein:\n\n\n\n\n\n\n\n\n\nWir können das studentisierte Problem auch wieder auf unser ursprüngliches zurück rechnen.\nDie Regressionsgerade im studentisierten Problem lautet:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761684 \\cdot x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot x^{stud}\n\\end{aligned}\n\\]\nRechnen wir nun mittels der Formel \\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\]\ndie Steigung um, so erhalten wir:\n\n(b1 &lt;- beta_stud_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822828\n\nUnd setzen wir das in unsere Gleichung zur Bestimmung von \\(\\hat\\beta_0\\) ein:\n\n(b0 &lt;- mean(dt$mpg) - b1 * mean(dt$hp))\n#&gt; [1] 30.09886\n\nso erhalten wir die Schätzwerte des ursprünglichen Problem.\n\n\nEin anderer Weg um die Regressionskoeffizenten zu bestimmen…\nGehen wir das Problem noch einmal neu an. Wir suchen \\(\\hat\\beta=(\\hat\\beta_0, \\hat\\beta_1)\\) welches \\(QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\\) minimiert.\nStatt es direkt, wie oben durch Null setzen der partiellen Ableitungen, zu bestimmen, wählen wir nun einen mathematisch-numerischen Ansatz und wollen \\(\\hat\\beta \\in \\mathbf{R}^2\\) als Optimierungsproblem mit Hilfe des Gradientenverfahrens lösen.\nBeim Gradientenverfahren wird versucht, ausgehend von einem Startwert \\(\\hat\\beta^0 \\in \\mathbf{R}^2\\), gemäß der Iterationsvorschrift\n\\[\n\\hat\\beta^{k+1} = \\hat\\beta^{k} + \\alpha^k \\cdot d^k\n\\]\nfür alle \\(k=0,1, ...\\) eine Näherungslösung für \\(\\hat\\beta\\) zu finden. Dabei ist \\(\\alpha^k &gt; 0\\) eine positive Schrittweite und \\(d^k\\in\\mathbf{R}^n\\) eine Abstiegsrichtung, welche wir in jedem Iterationsschritt \\(k\\) so bestimmen, dass die Folge \\(\\hat\\beta^k\\) zu einem stationären Punkt, unserer Näherungslösung, konvergiert.\nIm einfachsten Fall, dem Verfahren des steilsten Abstieges, wird der Abstiegsvektor \\(d^k\\) aus dem Gradienten \\(\\nabla QS\\) wie folgt bestimmt:\n\\[\nd^k = -\\nabla QS\\left(\\hat\\beta^k\\right)\n\\]\nWegen \\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot n \\cdot \\left(  \\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y} \\right)\n\\]\nund\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS = 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\right)\n\\]\ngilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot(\\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y})  \\\\\n\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{pmatrix}\n\\end{aligned}\n\\]\nWir wollen hier von Anfang an mit den studentisierten Werten arbeiten, weil diese numerisch viele Vorteile haben. Darum vereinfachen sich die beiden partiellen Ableitungen noch einmal zu:\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot v\n\\]\nund\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\\\\n&= 2 \\cdot (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{aligned}\n\\]\nSomit gilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot \\hat\\beta_0 \\\\\n(n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{pmatrix}\n\\end{aligned}\n\\]\nUm die Varianz und die Kovarianz nicht jedesmal neu zu berechnen, speichern wir die Ergebnisse vorab. Ebenso, damit der Quellcode kürzer wird, speichern wir in \\(x\\) und \\(y\\) die studentisierten Werte von \\(hp\\) und \\(mpg\\):\n\n# Vorbereitungen \nvar_x &lt;- var(~ hp_stud, data = dt)\ncov_xy &lt;- cov(mpg_stud ~ hp_stud, data = dt)\n\nn &lt;- length(dt$hp_stud)\n\nx &lt;- dt$hp_stud\ny &lt;- dt$mpg_stud\n\nNun erstellen wir die \\(QS\\) und \\(\\nabla QS\\) Funktionen: Wir definieren diese Funktion wie folgt in R:\n\nqs &lt;- function(b_0, b_1) {\n  sum((b_1 * x - y)**2)\n}\n\nnabla_qs &lt;- function(b_0, b_1) {\n  c(2 * n * b_0,\n    2 * (n - 1) * (b_1 * var_x - cov_xy)\n  )\n}\n\nDie Schrittweite \\(alpha\\) bestimmen wir mit Hilfe der Armijo-Bedingung und der Backtracking Liniensuche: Diese formalisiert das Konzept “genügend” in der geforderten Verringerung des Funktionswertes. Die Bedingung \\(f(x^k + \\alpha d^k) &lt; f(x^k)\\) wird modifiziert zu \\[f(x^k + \\alpha d^k) \\leq f(x^k) + \\sigma \\alpha \\left(\\nabla f(x^k)\\right)^T d^k,\\] mit \\(\\sigma\\in (0,1)\\). Die Armijo-Bedingung umgeht Konvergenzprobleme der einfachen Bedingung, indem sie fordert, dass die Verringerung zumindest proportional zur Schrittweite und zur Richtungsableitung \\(\\left(\\nabla f(x^k)\\right)^T d^k\\) ist, mit Hilfe der Proportionalitätskonstante \\(\\sigma\\). In der Praxis werden oft sehr kleine Werte verwendet, z.B. \\(\\sigma=0.0001\\).\nDie Backtracking-Liniensuche verringert die Schrittweite wiederholt um den Faktor \\(\\rho\\) (rho) , bis die Armijo-Bedingung erfüllt ist. Sie terminiert garantiert nach einer endlichen Anzahl von Schritten. Weshalb wir sie hier einsetzen:\n\nalpha_k &lt;- function(b_0, b_1, d_k, alpha = 1, sigma = 0.0001, rho = 0.5) {\n  d_0 &lt;- d_k[1]\n  d_1 &lt;- d_k[2]\n  nabla &lt;- nabla_qs(b_0, b_1)\n  n_0 &lt;- nabla[1]\n  n_1 &lt;- nabla[2]\n\n  lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n  rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n\n  while (lhs &gt; rhs) {\n    alpha &lt;- rho * alpha\n    lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n    rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n  }\n  return(alpha)\n}\n\nEin paar Einstellungen vorab:\n\n# maximale Anzahl an Iterationen\nmax_iter &lt;- 1000\niter &lt;- 0\n\n# Genauigkeit\neps &lt;- 10**-6\n\n# Startwerte\nb_0 &lt;- 0 \nb_1 &lt;- -1 \n\nFür eine vorgegebene Genauigkeit \\(eps=10^{-6}\\), den Startwerten \\(\\hat\\beta_0^0 = 0\\) und \\(\\hat\\beta_1^0 = -1\\) können wir somit das Verfahren starten:\n\nwhile (TRUE) {\n  iter &lt;- iter + 1\n\n  d_k &lt;- -nabla_qs(b_0, b_1)\n\n  ad_ &lt;- alpha_k(b_0, b_1, d_k) * d_k\n\n  x0 &lt;- b_0 + ad_[1]\n  x1 &lt;- b_1 + ad_[2]\n\n  if ((abs(b_0 - x0) &lt; eps) & (abs(b_1 - x1) &lt; eps) | (iter &gt; max_iter)) {\n    break\n  }\n  b_0 &lt;- x0\n  b_1 &lt;- x1\n}\n\nWir haben in \\(203\\) Iterationsschritten das folgende Ergebnis für die Regressionskoeffizienten:\n\\[\n\\hat\\beta_0^{stud} = 0 \\qquad \\hat\\beta_1^{stud} = -0.7761689\n\\]\nBetrachten wir die daraus erstellte Regressionsgerade:\n\n\n\n\n\n\n\n\n\nUm die Regressionskoeffizienten für unser ursprüngliches Problem zu erhalten müssen wir wie folgt zurück rechnen:\n\n(b1 &lt;- b_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822832\n(b0 &lt;- mean(dt$mpg) -  b1 * mean(dt$hp))\n#&gt; [1] 30.09887\n\nDie Geradengleichung für das ursprüngliches Problem lautet somit:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988668 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nDie R Funktion optim\nIn R gibt es bessere Optimierungsmethoden, als die hier verwendete. Zum Beispiel können wir die Funktion optim verwenden. Die Funktion optim benötigt die zu optimierende \\(f(x)\\) und ggf. die Gradientenfunktion \\(gf(x)\\) sowie einen Startpunkt \\(x^0\\):\n\nf &lt;- function(beta) {\n  qs(beta[1], beta[2])\n}\n\ngrf &lt;- function(beta) {\n  nabla_qs(beta[1], beta[2])\n}\n\n# Der eigentliche Aufruf von optim:\nergb &lt;- optim(c(0,-0.5),f ,grf, method = \"CG\")\n\n# Auslesen der Schätzer aus dem Ergebnis:\n(optim_beta_0 &lt;- ergb$par[1])\n#&gt; [1] 0\n(optim_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.7761683\n\nWir erhalten somit für das studentisierte Problem die Gerade:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta_0^{stud} + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761683 \\cdot  x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot  x^{stud}\n\\end{aligned}\n\\]\nFür das ursprüngliche Problem rechnen wir mittels\n\noptim_b1 &lt;- optim_beta_1 * sd(dt$mpg) / sd(dt$hp)\noptim_b0 &lt;- mean(dt$mpg) -  optim_b1 * mean(dt$hp)\n\num und erhalten:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988601 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "2. Idee: Summe der absoluten Abweichungen",
    "text": "2. Idee: Summe der absoluten Abweichungen\nWir ändern nun die Abweichungsmessfunktion von der Quadrat-Summe hin zu den Absolut-Summen:\n\\[\nAS = AS(\\hat\\beta) = AS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n |\\hat{y}_i - y_i|\n\\]\nAuch hier wollen wir mit den studentisierten Daten arbeiten und stellen die Funktion der Absolut-Summen auf:\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  return(sum(abs(b_0 + b_1 * x - y)))\n}\n\nDanach konstruieren wir die zu optimierende Funktion \\(f\\):\n\n# Zu optimierende Funktion\nf &lt;- function(beta) {\n  as(beta[1], beta[2])\n}\n\nDiesmal nutzen wir optim ohne eine Gradientenfunktion:\n\nergb &lt;- optim(c(0,-1), f)\n\n# Schätzer auslesen\n(opti_as_beta_0 &lt;- ergb$par[1])\n#&gt; [1] -0.1304518\n(opti_as_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.6844911\n\nSchauen wir uns nun die so erhaltene Gerade im Vergleich mit der ‘normalen’ Regressionsgerade an:\n\n\n\n\n\n\n\n\n\nIn grün und gestrichelt sehen wir die Gerade aus der Idee der quadratischen Abweichungssummen, in blau die aus der Idee der absoluten Abweichungssummen.\nFür unser ursprüngliches Problem rechnen wir um:\n\n# Umrechnen in die ursprüngliche Fragestellung\n(as_b1 &lt;- opti_as_beta_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06016948\n(as_b0 &lt;- (mean(dt$mpg) - as_b1 * mean(dt$hp)) + opti_as_beta_0 * sd(dt$mpg))\n#&gt; [1] 28.13051\n\nUnd die dazu gehörige Darstellung:\n\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 28.1305094 -0.0601695 \\cdot x \\\\\n          &\\approx 28.131 -0.06 \\cdot x\n\\end{aligned}\n\\]\nDiese Methode nennt sich Median-Regression und ein ein Spezialfall der Quantilsregression, die sich u.a. mit dem R-Paket quantreg unmittelbar umsetzen lässt:\n\nlibrary(quantreg)\nergmedianreg &lt;- rq(mpg ~ hp, data = dt)\ncoef(ergmedianreg)\n#&gt; (Intercept)          hp \n#&gt; 28.13050847 -0.06016949"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "1. Idee: Betrag der Summe der Abweichungen",
    "text": "1. Idee: Betrag der Summe der Abweichungen\nWenn wir die Summe der Abweichungen \\(\\sum\\limits_{i=1}^n \\hat{e}_i\\) minimieren wollen, dann ist es sinnvoll den Betrag davon zu minimieren. Wir suchen also die Schätzer \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\), so dass der Ausdruck\n\\[\n\\left| \\sum_{i=1}^n \\hat{e}_i \\right| = \\left| \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \\right|\n\\]\nminimal ist.\nWegen:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i)\n&= \\sum_{i=1}^n \\hat\\beta_0 + \\sum_{i=1}^n \\hat\\beta_1 \\cdot x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot n \\cdot \\bar{x} - n \\cdot \\bar{y} \\\\\n&= n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\bar{x} - \\bar{y} \\right) \\\\\n&= n \\cdot \\left( \\hat\\beta_0 - \\bar{y} + \\hat\\beta_1 \\cdot \\bar{x}  \\right)\n\\end{aligned}\n\\]\nkönnen wir das absolute Minimum bei \\(\\hat\\beta_0 - \\bar{y} =0\\) und \\(\\hat\\beta_1 \\cdot \\bar{x}=0\\) erreichen, was zur Lösung \\(\\hat\\beta_0 =\\bar{y}\\) und \\(\\hat\\beta_1 = 0\\) führt. Dies ist unser Nullmodel in dem die \\(x_i\\) keinen Einfluss auf die \\(y_i\\) haben und wir daher pauschal die \\(y_i\\) mit \\(\\hat{y}_i=\\bar{y}\\), also dem Mittelwert der \\(y_i\\) abschätzen."
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nAls Vergleich können wir uns die Quadratsumme \\(QS\\) und Absolutsumme \\(AS\\) der drei Modelle einmal ansehen:\n\n# Quadratische Abweichungssummen\nqs &lt;- function(b_0, b_1) {\n  sum(((b_0 + b_1 * dt$hp) - dt$mpg )**2)\n}\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  sum(abs((b_0 + b_1 * dt$hp) - dt$mpg))\n}\n\n\n# Quadratsummen:\nquad_sum &lt;- c(qs(b0, b1), qs(as_b0, as_b1), qs(mean_mpg, 0))\n\n# Absolutsummen:\nabs_sum &lt;- c(as(b0, b1), as(as_b0, as_b1), as(mean_mpg, 0))\n\ntab &lt;- tibble(\n  sums = c(quad_sum, abs_sum),\n  sum_type = rep(c(\"quad\", \"abs\"), each = 3),\n  methode = rep(c(\"Idee 3\", \"Idee 2\", \"Idee 1\"), 2)\n)\n\npivot_wider(tab, names_from=sum_type, values_from=sums, names_sort=T)\n#&gt; # A tibble: 3 × 3\n#&gt;   methode   abs  quad\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Idee 3   93.0  448.\n#&gt; 2 Idee 2   87.3  477.\n#&gt; 3 Idee 1  151.  1126."
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 quantreg_6.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "href": "posts/2021-06-09-über-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nDas “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "",
    "text": "Bei einer multiplen linearen Regression kann man den Einfluss einer unabhägigen Variable auf das Verhalten einer anderen unabhägigen Variable in Bezug auf die abhägige Variable mit modellieren.\nWir wollen das einmal an dem Beispiel der folgenden Datentabelle Impact of Beauty on Instructor’s Teaching Ratings und der Fragestellung in wie weit das Alter und das Geschlecht einen Einfluss auf das Evaluationsergebnis haben.\nDazu stellen laden wir die Daten aus dem Internet:\n\nlibrary(mosaic)\nurl &lt;- paste0(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/\",\n              \"TeachingRatings.csv\")\nteacherratings &lt;- read.csv(url)\n\nund betrachten das Streudiagramm:\n\ngf_point(eval ~ age, color = ~gender, data = teacherratings)"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "Nachtrag und Danksagung",
    "text": "Nachtrag und Danksagung\nDie Idee zu diesem Blog-Post verdanke ich dem Blog von Prof. Dr. Sebastian Sauer. Hier der Link zum Orginal-Blog: https://data-se.netlify.app/2021/06/17/beispiel-zur-interpretation-des-interaktionseffekts/\nDanke auch für die kritische Durchsicht und die hilfreichen Anmerkungen."
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#reproduzierbarkeitsinformationen",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "",
    "text": "Wir wollen den Fall unterschen bei dem wir mit zwei statistischen Variabeln (\\(X\\) und \\(Y\\)) eine dritte Variable (\\(Z\\)) mittels einer multiplen linearen Regression modellieren.\nEs seien die Datenpunkte \\((x_1, y_1, z_1), \\dots, (x_n, y_n, z_n)\\) gegeben und wir wollen eine lineare Funktion \\(g(x,y)\\) finden, so dass\n\\[\nz_i = g(x_i,y_i)+ \\epsilon_i =\\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot y_i + \\epsilon_i\n\\]\ngilt und der Abweichungsterm \\(\\epsilon_i\\) möglichst klein ist.\nAuf Grundlage unserer Datenpunkt wollen wir die Koeffizienten so schätzen, dass die Summe der quadratische Abweichungen minimal ist. \\[\n  QS = QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\n  = \\sum\\limits_{i=1}^n (z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i )^2\n\\]\nDas führt zu der folgenden, notwendigen Bedingen (für stationäre Punkte):\n\\[\n\\nabla QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\end{pmatrix}\n\\]\nIm einzelnen heißt das:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot\\sum\\limits_{i=1}^n \\left(z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i \\right) \\\\\n&= -2 \\cdot n \\cdot \\left(\\bar{z} - \\hat\\beta_0 - \\hat\\beta_1 \\cdot\\bar{x} - \\hat\\beta_2 \\cdot\\bar{y} \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot x_i - \\hat\\beta_0 \\cdot x_i - \\hat\\beta_1 \\cdot x_i\\cdot x_i - \\hat\\beta_2 \\cdot y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot y_i - \\hat\\beta_0 \\cdot y_i - \\hat\\beta_1 \\cdot x_i\\cdot y_i - \\hat\\beta_2 \\cdot y_i\\cdot y_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die 1. Gleichung gleich Null und stellen nach \\(\\hat\\beta_0\\) um:\n\\[\n  \\hat\\beta_0 = \\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}\n\\] Nun ersetzen wir \\(\\hat\\beta_0\\) in den verbleibenden Gleichungen durch \\(z_i - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i\\) und nutzen den Verschiebesatz:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (x_i -\\bar{x})^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i -\\bar{y})^2 - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die beiden Gleichungen nun gleich Null und formen nach \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\) um:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2}\n\\end{aligned}\n\\]\nDurch Erweiterung von Zähler nun Nenner mit \\(\\frac{1}{n-1}\\) erhalten wir:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  &= \\frac{s_{x,z}-\\hat\\beta_2\\cdot s_{x,y}}{s^2_{x}} = \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i - \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2} \\\\\n  &= \\frac{s_{y,z}-\\hat\\beta_1\\cdot s_{x,y}}{s^2_{y}} = \\frac{s_{y,z}}{s^2_y}-\\hat\\beta_1 \\frac{s_{x,y}}{s^2_{y}} \\\\\n\\end{aligned}\n\\]\nwir setzen nun die erste in die zweite Gleichung ein und erhalten:\n\\[\n\\begin{aligned}\n\\hat\\beta_2\n  &= \\frac{s_{y,z}}{s^2_y} - \\left(\\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\right) \\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}} + \\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{\\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}}}{1-\\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}}} \\\\\n  &= \\frac{\\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x\\cdot s^2_y}}{\\frac{s^2_x s^2_y-(s_{x,y})^2}{s^2_x \\cdot s^2_y}}\n  = \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2}\n\\end{aligned}\n\\]\nUnd damit weiter:\n\\[\n\\begin{aligned}\n\\hat\\beta_1  \n  &= \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z}}{s^2_x} - \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2} \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z} (s^2_x s^2_y - (s_{x,y})^2) - s_{y,z}s_{x,y}s^2_x + s_{x,z}s_{x,y}s_{x,y}}{s^2_x (s^2_x s^2_y - (s_{x,y})^2)} \\\\\n    &= \\frac{s_{x,z}s^2_x s^2_y - s_{x,z}(s_{x,y})^2 - s_{y,z}s_{x,y}s^2_x + s_{x,z}(s_{x,y})^2}{s^2_x s^2_x s^2_y- s^2_x(s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nlibrary(mosaic)\n\nmtcars %&gt;%\n  select(mpg, hp, wt) -&gt; dt\n\n# Von R berechnete Koeffizienten:\ncoef(lm(mpg ~ hp + wt, data = dt))\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074\n\nmean_x = mean( ~ hp, data = dt)\nmean_y = mean( ~ wt, data = dt)\nmean_z = mean( ~ mpg, data = dt)\n\ns_xy &lt;- cov(hp ~ wt, data = dt)\ns_xz &lt;- cov(hp ~ mpg, data = dt)\ns_yz &lt;- cov(wt ~ mpg, data = dt)\n\nvar_x &lt;- var(~ hp, data = dt)\nvar_y &lt;- var(~ wt, data = dt)\nb1z &lt;- s_xz*var_x*var_y - s_xz*(s_xy)**2 - s_yz*s_xy*var_x + s_xz*s_xy**2\nb1n &lt;- var_x*var_x*var_y - var_x*s_xy**2\nb1 &lt;- b1z / b1n\nb2 &lt;- (s_yz*var_x - s_xz*s_xy) / (var_x * var_y - s_xy*s_xy)\nb0 &lt;- mean_z - b1 * mean_x - b2 * mean_y\n\n# Koeffizienten zur Ausgabe aufbereiten:\nmy_coef &lt;- c(b0, b1, b2)\nnames(my_coef) &lt;- c(\"(Intercept)\", \"hp\", \"wt\")\n\n# Von Hand berechnete Koeffizienten:\nmy_coef\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074"
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "Was passiert, wenn wir alle Datenpunkte studentisieren?",
    "text": "Was passiert, wenn wir alle Datenpunkte studentisieren?\nWir rechnen um in:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x}; \\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}; \\quad z_i^{\\text{stud}} = \\frac{z_i-\\bar{z}}{s_z}\n\\]\nDamit ist\n\\[\n\\bar{x_i}^\\text{stud} = 0; \\quad \\bar{y_i}^\\text{stud} = 0;\\quad \\bar{z_i}^\\text{stud} = 0\n\\] und\n\\[\ns_{{x_i}^\\text{stud}} = 1; \\quad s_{{y_i}^\\text{stud}} = 1;\\quad s_{{z_i}^\\text{stud}} = 1\n\\]\nZur Vereinfachung lassen wir die Kennzeichnung “stud” weg. Damit ist dann:\n\\[\n\\begin{aligned}\n\\hat\\beta_0\n  &= 0\\\\\n\\\\\n\\hat\\beta_1\n  &= \\frac{s_{x,z} \\cdot s^2_x \\cdot s^2_y - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y}s^2_x + s_{x,z} \\cdot (s_{x,y})^2}{s^2_x \\cdot s^2_x s^2_y- s^2_x \\cdot (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} \\cdot 1 \\cdot 1 -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} \\cdot 1 + s_{x,z} \\cdot (s_{x,y})^2}{1 \\cdot 1 \\cdot 1 - 1 \\cdot (s_{x,y})^2}\\\\\n  &= \\frac{s_{x,z} -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} + s_{x,z} \\cdot (s_{x,y})^2}{1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} - s_{y,z} \\cdot s_{x,y} }{1 - (s_{x,y})^2} \\\\\n\\\\\n\\hat\\beta_2\n  &= \\frac{s_{y,z} \\cdot s^2_x - s_{x,z} \\cdot s_{x,y}}{s^2_x \\cdot s^2_y - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} \\cdot 1 - s_{x,z} \\cdot s_{x,y}}{1 \\cdot 1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} - s_{x,z} \\cdot s_{x,y}}{1 - (s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nWir schauen uns ein paar Fälle genauer an:\n\nFall: \\(X\\) und \\(Y\\) sind unabhängig. Dann ist \\(s_{x,y}=0\\) und wir erhalten \\(\\hat\\beta_1=s_{x,z}\\in[-1;1]\\) und \\(\\hat\\beta_2=s_{y,z}\\in[-1;1]\\).\nFall: \\(X\\) und \\(Y\\) sind abhängig. Dann ist \\(|s_{x,y}|=1\\) und es gibt keine Lösung für \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\).\nFall: \\(0 &lt; |s_{x,y}| &lt; 1\\). …"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html",
    "href": "posts/2020-06-29-cook-abstand/index.html",
    "title": "Cook Abstand",
    "section": "",
    "text": "Frage: Was macht einen Wert zum Ausreißer?\nEine mögliche Antwort lautet: Ein Wert gilt als Ausreißer, wenn er deutlich von den übrigen Werten abweicht und einen (starken) Einfluss auf das Modell ausübt.\nEin Verfahren zur Identifikation solcher Ausreißer ist der Cook-Abstand (engl.: Cook’s distance). Die zugrunde liegende Idee besteht darin zu messen, wie stark ein einzelner Wert das Modell beeinflusst. Dazu vergleicht man das Modell einmal mit und einmal ohne diesen Wert.\nSehen wir uns den Cook-Abstand anhand eines einfachen linearen Regressionsmodells näher an:"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#unser-modell",
    "href": "posts/2020-06-29-cook-abstand/index.html#unser-modell",
    "title": "Cook Abstand",
    "section": "Unser Modell:",
    "text": "Unser Modell:\nZuerst ein Streudiagramm zur Visualisierung der Daten:\n\ngf_point(tip ~ total_bill, data = tips)\n\n\n\n\n\n\n\n\nDann erstellen wir ein lineares Regressionsmodell:\n\nerg_lm &lt;- lm(tip ~ total_bill, data = tips)\nsummary(erg_lm)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1982 -0.5652 -0.0974  0.4863  3.7434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.920270   0.159735   5.761 2.53e-08 ***\ntotal_bill  0.105025   0.007365  14.260  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.022 on 242 degrees of freedom\nMultiple R-squared:  0.4566,    Adjusted R-squared:  0.4544 \nF-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16\n\n\nVisualisierung der Regressionsgeraden:\n\ngf_point(tip ~ total_bill, data = tips) %&gt;%\n  gf_coefline(\n    model = erg_lm,\n    color = ~ \"Regressionsgerade\",\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nEinflussreiche Ausreißer können bei linearen Modellen problematisch sein. Was passiert, wenn wir einen potenziellen Ausreißer entfernen?\nBeispiel: Wir eliminieren die Beobachtung mit dem Index 173\n\ntips %&gt;% slice(173) -&gt; tips_removed\ntips_removed\n\n  total_bill  tip\n1       7.25 5.15\n\n\n\ntips %&gt;% slice(-173) -&gt; tips_red\nerg_lm_red &lt;- lm(tip ~ total_bill, data = tips_red)\nsummary(erg_lm_red)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips_red)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2136 -0.5351 -0.0818  0.4951  3.6869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.86065    0.15709   5.479 1.08e-07 ***\ntotal_bill   0.10731    0.00723  14.843  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9992 on 241 degrees of freedom\nMultiple R-squared:  0.4776,    Adjusted R-squared:  0.4754 \nF-statistic: 220.3 on 1 and 241 DF,  p-value: &lt; 2.2e-16\n\n\nGrafischer Vergleich:\n\ngf_point(tip ~ total_bill, data = tips_red) %&gt;%\n  gf_coefline(\n    model = erg_lm,\n    color = ~ \"Regressionsgerade\"\n    ) %&gt;%\n  gf_point(\n    tip ~ total_bill, \n    colour = ~ \"Entfernter Punkt\",\n    data = tips_removed)"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#berechnung-des-cook-abstands",
    "href": "posts/2020-06-29-cook-abstand/index.html#berechnung-des-cook-abstands",
    "title": "Cook Abstand",
    "section": "Berechnung des Cook-Abstands",
    "text": "Berechnung des Cook-Abstands\nWir vergleichen die Prognosen beider Modelle:\n\nnew_data &lt;- tibble(total_bill = tips$total_bill)\nprognose_lm &lt;- predict(erg_lm, newdata = new_data)\nprognose_lm_red &lt;- predict(erg_lm_red, newdata = new_data)\n\nBerechnung:\n\\[\nd_j = \\sum_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2\n\\] Dabei ist \\(\\hat{y}_i\\) die Prognose des Wertes \\(y_i\\) auf Basis von \\(x_i\\) mit dem Originalmodell und \\(\\hat{y}_{i(j)}\\) die Prognose wenn man die \\(j\\)-te Beobachtung aus dem Modell gestrichen hat.\n\nd_j &lt;- sum((prognose_lm - prognose_lm_red)^2)\nd_j\n\n[1] 0.1511406\n\n\nDer Cook Abstand \\(D_j\\) wird nun noch normiert durch \\[{\\text{var}_{\\text{cook}}} = p \\cdot s_{\\epsilon_i^2}^2\\] Dabei ist \\(s_{\\epsilon_i^2}^2\\) der erwartungstreue Schätzer der Varianz der Residuen und \\(p\\) die Anzahl aller erklärenden Variablen plus Eins, also: $ 1 + 1 = 2$.\nNormierung des Cook-Abstands:\n\\[\n  D_j = \\frac{d_j}{\\text{var}_{\\text{cook}}} = \\frac{\\sum\\limits_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2}{p \\cdot s_{\\epsilon_i^2}^2}\n\\]\n\n# Summary des Modells\nselm &lt;- summary(erg_lm)\n\n# Wir finden p als rank im Modell\np &lt;- erg_lm$rank \n\n# Wir finden den erwatungtreuen Schätzer im Summary des Modells\ns_quad_eps_quad &lt;- (selm$sigma)^2 \n\nvar_cook = p * s_quad_eps_quad\n\nD_j = d_j / var_cook\nD_j\n\n[1] 0.07234504\n\n\nAlternativ kann der Wert direkt mit cooks.distance() berechnet werden:\n\ncooks.distance(erg_lm)[173]\n\n       173 \n0.07234504 \n\n\nWann aber ist nun ein Wert ein einflussreicher Ausreißer?"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#entscheidungskriterien",
    "href": "posts/2020-06-29-cook-abstand/index.html#entscheidungskriterien",
    "title": "Cook Abstand",
    "section": "Entscheidungskriterien",
    "text": "Entscheidungskriterien\nCook selber gibt dafür die Bedingung \\(D_j &gt; 1\\) an. Andere Autor*innen schreiben \\(D_j &gt; 4/n\\), wobei \\(n\\) die Anzahl der Beobachtung ist.\nIn unserem Beispiel liefert die Variante \\(D_j &gt; 1\\)\n\ncooks &lt;- cooks.distance(erg_lm)\nnames(cooks) &lt;- NULL\nn &lt;- nrow(tips)\n\nany(cooks &gt; 1)\n\n[1] FALSE\n\n\nkeinen Ausreißer.\nWenn wir jedoch mit \\(D_j &gt; 4/n\\) suchen .\n\nany(cooks &gt; 4/n)\n\n[1] TRUE\n\n\ndann gibt es Ausreißer.\nDie Indices dieser finden wir mit:\n\nwhich(cooks &gt; 4/n)\n\n [1]  24  48  57 103 142 157 171 173 179 183 184 185 188 208 211 213 215 238\n\n\nDaten bereinigen:\n\nremove &lt;- which(cooks &gt; 4/n)\ntips %&gt;% slice(-remove) -&gt; tips_no_outliers\ntips %&gt;% slice(remove) -&gt; tips_removed\nerg_lm_no_outliers &lt;- lm(tip ~ total_bill, data = tips_no_outliers)\n\nErgebnis des Modells:\n\nsummary(erg_lm_no_outliers)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips_no_outliers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.22592 -0.48166 -0.06794  0.46992  2.31414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.773324   0.139435   5.546  8.2e-08 ***\ntotal_bill  0.111799   0.006958  16.069  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7778 on 224 degrees of freedom\nMultiple R-squared:  0.5355,    Adjusted R-squared:  0.5334 \nF-statistic: 258.2 on 1 and 224 DF,  p-value: &lt; 2.2e-16\n\n\nDaten bereinigen:\n\ngf_point(tip ~ total_bill, data = erg_lm_no_outliers) %&gt;%\n  gf_coefline(\n    model = erg_lm_no_outliers, \n    color = ~\"Regressionsgerade\"\n  )\n\n\n\n\n\n\n\n\nGrafischer Vergleich beider Modelle:\n\ngf_point(tip ~ total_bill, data = erg_lm) %&gt;%\n  gf_coefline(\n    model =  erg_lm,\n    color = ~ \"Regressionsgerade (Orginal)\"\n  ) %&gt;%\n  gf_coefline(\n    model = erg_lm_no_outliers,\n    color = ~ \"Regressionsgerade (No Outliers)\"\n  ) %&gt;%\n  gf_point(\n    tip ~ total_bill,\n    color = ~ \"Entfernte Punkte\",\n    data = tips_removed\n  )"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#modelle-in-gleichungsform",
    "href": "posts/2020-06-29-cook-abstand/index.html#modelle-in-gleichungsform",
    "title": "Cook Abstand",
    "section": "Modelle in Gleichungsform",
    "text": "Modelle in Gleichungsform\nDas ursprüngliche Modell:\n\\[\n  \\widehat{tips}_{lm} = 0.9202696 + 0.1050245 \\cdot total\\_bill\n\\]\nDas um pot. Ausreißer bereinigte Modell:\n\\[\n  \\widehat{tips}_{lm\\_no} 0.7733236 + 0.1117985 \\cdot total\\_bill\n\\]"
  },
  {
    "objectID": "posts/2021-02-12-gedankenstütze-zu-wichtigen-funktionsbegriffen-in-der-statistik/index.html",
    "href": "posts/2021-02-12-gedankenstütze-zu-wichtigen-funktionsbegriffen-in-der-statistik/index.html",
    "title": "Gedankenstütze zu wichtigen Funktionsbegriffen in der Statistik",
    "section": "",
    "text": "Eine kleine Liste von fundermentalen Begriffen in der Statistik.\nGilt für eine reelle Funktion \\(f: \\mathbf{R} \\to \\mathbf{R}\\):\n\n\\(f(x)\\) ist nichtnegativ, d.h., \\(f(x) \\geq 0\\), für alle \\(x \\in \\mathbf{R}\\).\n\\(f(x)\\) ist integrierbar.\n\\(f(x)\\) ist normiert in dem Sinne, dass \\[\\int_{-\\infty}^\\infty f(x) \\,\\text{d}x = 1\\]\n\nDann nennen wir \\(f(x)\\) eine Wahrscheinlichkeitsdichtefunktion (engl. probability density funktion kurz pdf) oder kurz Dichte (engl. density).\nDurch \\[P([a, b]) := \\int_a^b f(x) \\text{d} x\\] definiert \\(f\\) eine Wahrscheinlichkeitsverteilung auf den reellen Zahlen.\nIst \\(X\\) eine reelwertige Zufallsvariable (kurz ZV) und existiert eine reelle Funktion \\(f_X(x)\\) der Art, dass für alle \\(a \\in \\mathbf{R}\\)\n\\[P(X \\leq a) = \\int_{-\\infty}^a f_X(x) \\text{d}x\\]\ngilt, so nennt man \\(f\\) die Wahrscheinlichkeitsdichtefunktion von \\(X\\).\nDie Funktion\n\\[F_X(a) = P(X \\leq a)\\]\nnenen wir (Wahrscheinlichkeits-)Verteilung(-sfunktion) (engl. cumulative distribution function kurz cdf aber auch nur distribution function) von \\(X\\).\nGenau dann ist eine Funktion \\(F: \\mathbf{R} \\to [0, 1]\\) eine Verteilungsfunktion, wenngilt:\n\nEs ist \\(\\lim_{t \\to -\\infty} F(t)=0\\) und \\(\\lim_{t \\to +\\infty} F(t)=1\\)\nDie Funktion \\(\\overline{F}(t)\\) ist monoton wachsend.\nDie Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig\n\nDie Funktion\n\\[\\overline{F}_X(a) = 1 - F_X(a)\\]\nnennen wir Überlebensfunktion (engl. survival function, complementarey cumulative distribution funktion kurz ccdf, tail distribution, exceedance oder reliability function).\nEs gilt \\(\\overline{F}_X(a) + F_X(a) = 1\\).\nGenau dann ist eine Funktion \\(\\overline{F}: \\mathbf{R} \\to [0, 1]\\) eine Überlebensfunktion, wenn gilt:\n\nEs ist \\(\\lim_\\limits{t \\to -\\infty} \\overline{F}(t)=1\\) und \\(\\lim_{t \\to + \\infty}\\overline{F}(t)=0\\)\nDie Funktion \\(\\overline{F}(t)\\) ist monoton fallend.\nDie Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig\n\nEin paar weitere Eigenschaften von Überlebensfunktionen:\n\nNicht-negative stetige ZV \\(X\\) mit Erwartungswert, also \\(\\int_0^\\infty x f(x) \\text{d} x = \\mu &lt; \\infty\\), erfüllen die Markov-Ungleichung\n\n\\[\\overline{F}_X(x) \\leq \\frac{\\operatorname{E}(X)}{x}\\]\n\nIst \\(X\\) eine ZV und \\(\\overline{F}_X\\) die zugehörige Überlebensfunktion. Existiert \\(E(X)\\), dann gilt \\(\\lim_{t \\to +\\infty}\\overline{F}(x)=0 = o\\left(\\frac{1}{x}\\right)\\).\n\nBeweisskizze: Sei \\(f_X\\) die Dichtefunktion von \\(F_X\\) zur ZV \\(X\\). Für jedes \\(c&gt;0\\) ist dann \\[\n\\begin{aligned}\n      E(X) = \\int_0^\\infty x \\cdot f_X(x) \\text{d}x &=  \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty x \\cdot f_X(x) \\text{d}x \\\\\n      &\\geq  \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty c \\cdot f_X(x) \\text{d}x \\\\\n      &=  \\int_0^cx \\cdot f_X(x) \\text{d}x + c \\cdot \\int_c^\\infty f_X(x) \\text{d}x \\\\\n      &=  \\int_0^cx \\cdot f_X(x) \\text{d}x +c \\cdot \\overline{F}(c)\n  \\end{aligned}\n\\]\nDamit gilt nun: \\[\n0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\n\\] Wegen \\(\\lim\\limits_{c \\to +\\infty} \\int_0^cx \\cdot f_X(x) \\text{d}x = E(X)\\) folgt:\n\\[0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\\to 0 \\text{ wenn } c \\to \\infty\n\\]\nFür nicht-negative ZV \\(X\\) gilt:\n\\[\nE(X) = \\int_0^\\infty \\overline{F}_X(x) \\,\\text{d} x\n\\]"
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-für-r",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-für-r",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Vorbemerkungen und Notationen",
    "text": "Vorbemerkungen und Notationen\nDa alle t-Verteilungen symmetrisch sind, betrachten wir im wesendlichen nur den positiven Teil.\nZwei reelle Funktionen \\(f\\), \\(g\\) sind genau dann, im Sinne von de Bruijn1 (§1.4), asymptotisch äquivalent \\(f \\sim g\\), wenn\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{f(x)}{g(x)} = 1\n\\]\ngilt.\nIst \\(f \\sim g\\), so können wir auch\n\\[\nf(x) = g(x)\\cdot(1+o(1))\n\\]\ndafür schreiben. Dabei ist \\(h(x) = o(\\phi(x))\\) für \\(x \\to \\infty\\), falls \\(\\lim\\limits_{x \\to \\infty} \\frac{h(x)}{\\phi(x)} = 0\\) gilt. Aus der asymptotischen Äquivalenz von \\(f\\) und \\(g\\) folgt nun direkt:\n\\[\n\\lim\\limits_{x \\to \\infty}\\frac{f(x)}{g(x)}-1 =\\frac{f(x)-g(x)}{g(x)} = 0\n\\]\nMit \\(h(x) = \\frac{f(x)-g(x)}{g(x)}\\) ist \\(h(x) = o(1)\\) und daher \\(f(x)-g(x) = g(x)o(1)\\) und schliesslich \\(f(x) = g(x)+g(x)o(1)\\).\nEin wichtiges Korrolar sagt:\nIst \\(f \\sim g\\), so ist auch \\(\\log(f) \\sim \\log(g)\\)."
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung im Allgemeinen",
    "text": "Die t-Verteilung im Allgemeinen\nDie Dichtefunktion der t-Verteilung lauten im Allgemeinen:\n\\[\nf_n(x) = \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}}\\quad \\mathrm{für}\\quad -\\infty &lt; x &lt; +\\infty\n\\]\nwobei wir mit \\(\\Gamma(x)\\) die Gammafunktion\n\\[\n\\Gamma(x)=\\int\\limits_{0}^{+\\infty}t^{x-1}e^{-t}\\operatorname{d}t\n\\]\nbezeichnen. Für einige \\(x\\) nimmt die Gammafunktion leicht zu berechnende Werte an:\nSo ist für alle \\(n\\in\\mathbf{N_0}\\):\n\\(\\Gamma(n+1) = n!\\) und \\(\\Gamma\\left(n + \\frac{1}{2}\\right) = \\frac{(2n)!}{n!4^n}\\sqrt{\\pi}\\)\nmit der gewöhnlichen Fakultät \\(n! = \\prod_{i=0}^n i\\), wobei per Definition \\(0!=1\\) ist."
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung mit einem Freiheitsgrad",
    "text": "Die t-Verteilung mit einem Freiheitsgrad\nFür \\(f_1(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n&= \\frac{\\Gamma\\left(\\frac{2}{2}\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-\\frac{2}{2}} \\\\\n&= \\frac{\\Gamma\\left(1\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-1} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{1} {\\sqrt{\\pi} \\cdot \\sqrt{\\pi}} \\cdot \\left(1+x^{2}\\right)^{-1} \\\\\n       &= \\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\n\\end{align*}\n\\]\nDas ist die Dichtefunktion der standardisierten Cauchy-Verteilung\n\\[\nf_{(\\mu,\\lambda)}(x) = \\frac{1}{\\pi} \\cdot \\frac{\\lambda}{\\lambda^2+(x-\\mu)^2}\n\\]\nmit (\\(\\mu = 0\\) und \\(\\lambda=1\\)), welche – bekanntermaßen – keinen Erwartungwert hat.\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_1(k \\cdot x)}{f_1(x)}\n      &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\pi} \\cdot \\frac{1}{1+(kx)^{2}}}{\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}} = \\lim_{x \\to +\\infty} \\frac{1+x^2}{1+k^2x^2} \\\\\n      &=\\lim_{x \\to +\\infty} \\frac{\\frac{1}{x^2}+\\frac{x^2}{x^2}}{\\frac{1}{x^2}+k^2\\frac{x^2}{x^2}} =\\frac{1}{k^2}=k^{-2}\n\\end{align*}\n\\]\nfür alle reellen \\(k&gt;0\\) ist \\(f_1(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -2\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\overline{F}_1(x) = \\int_x^\\infty f_1(t) \\operatorname{d}t = \\frac{1}{\\pi} \\cdot \\int_x^\\infty  \\frac{1}{1+x^{2}} \\operatorname{d}t = \\frac{\\arctan(x)}{\\pi}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt nun:\n\\[\n\\arctan`(x)= \\frac{1}{1+x^2} \\to \\frac{1}{x^2} \\text{ für } x\\to \\infty\n\\]\nGenauer gilt wegen\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{\\frac{1}{1+x^2}}{\\frac{1}{x^2}}\n= \\lim\\limits_{x \\to \\infty} \\frac{x^2}{1+x^2} =1,\n\\]\ndass \\(\\frac{1}{1+x^2} \\sim \\frac{1}{x^2}\\), also asymptotisch äquivalent sind und somit auch \\(\\log\\left(\\frac{1}{1+x^2}\\right) \\sim \\log\\left(\\frac{1}{x^2}\\right)\\).\nZusammen gefasst gilt somit: \\[\n\\log\\left(\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\\right) \\to -2\\log(x) - \\log(\\pi) \\text{ für } x \\to \\infty\n\\]\nSei \\(f_1^*(x) = C \\cdot x^{-\\alpha}\\) mit \\(\\alpha = 2\\) und \\(C=\\frac{1}{\\pi} \\approx0.3183\\).\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 1\n\nf_star &lt;- function(x) {\n  alpha &lt;- 2\n  C &lt;- 1/pi\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie groß ist nun der (absolute) Fehler zwischen \\(f_1^*\\) und \\(f_1\\)?\nEine Grafik von \\(f_1^*-f_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(x**-2 - 1/(1+x**2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nGenauer gilt:\n\\[\nf_1^*(x) - f_1(x) = \\frac{1}{x^2+x^4}\n\\]\nWir können also für ein hinreichend großes \\(x &gt;&gt; 1\\) statt \\(f_1\\) auch \\(f_1^*\\) verwenden und erhalten somit:\n\\[\n\\begin{align*}\n\\overline{F}_1(x) &\\approx \\int_x^\\infty f_1^*(t) \\operatorname{d}t\n  = \\int_x^\\infty  C \\cdot t^{-\\alpha} \\operatorname{d}t \\\\\n  &= \\frac{1}{\\pi} \\cdot \\int_x^\\infty  t^{-2} \\operatorname{d}t\n  = \\frac{1}{\\pi}\\left[\\lim\\limits_{\\epsilon \\to \\infty} \\left(-\\epsilon^{-1}\\right) -\\left(-x^{-1}\\right)\\right]\\\\\n  &= \\frac{1}{\\pi}\\cdot\\left[0 + \\frac{1}{x}\\right] = \\frac{1}{\\pi \\cdot x}\n\\end{align*}\n\\]\nWie hinreichend ist hier hinreichend groß?\nTaleb schreibt an dieser Stelle gerne, dass man jenseits des Karamata-Punktes die Karamata-Konstante anwenden kann. Beides Begriffe, zu denen ich zunächst keine echte Definition gefunden habe.\nJovan Karamata ist der Begründer der langsam variierend Funktionen. 1930 zeite er, dass eine positive stetige Funktion \\(L\\) auf den positiven reellen Zahlen genau dann langsam variierend ist, also für alle \\(t &gt; 0\\) die Bedingung\n\\[\nL(t\\,x)/L(x) \\to 1 \\qquad\\text{für}\\qquad x \\to \\infty\n\\]\nerfüllt, wenn sie für ein \\(a &gt; 0\\) für \\(x &gt; a\\) in der Form\n\\[\nL(x) = c(x) \\cdot \\exp \\left(\\int_a^x\\!\\varepsilon(t)/t \\ dt \\right)\n\\] mit \\(c(x) \\to c &gt; 0\\) und \\(\\varepsilon(x) \\to 0\\) für \\(x \\to \\infty\\) geschrieben werden kann.\nIch vermute also, dass wir \\(L(x)\\) ab dem Punkt \\(a\\) näherungsweise durch \\(c(x)\\) besser sogar durch die Konstante \\(c\\) ersetzen könnten.\nDie Karamata-Konstante ist \\(\\rho = -\\alpha\\), also \\(\\rho = c\\)?\nDer Karamata-Punkt bleibt etwas nebulöser. Es könnte sich hier um den Punkt \\(a\\) handeln und vermutlich könnte man hier so argumentieren:\nWenn die Fehler zwischen \\(f\\) und \\(f^*\\) hinreichend klein ist.\nHierfür könnte man einen absoluten Fehler oder einen relativen Fehler als Maßstab ansehen.\nFür einen relativen Fehler vielleicht \\(\\frac{f^*-f}{x} &lt; k\\)?\nOder man betrachtet hier gleich \\(\\frac{f^*-f}{\\log(x)} &lt; k^*\\)?"
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "t-Verteilung mit zwei Freiheitsgeraden",
    "text": "t-Verteilung mit zwei Freiheitsgeraden\nFür \\(f_2(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(\\frac{2}{2}\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(1\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{3}{2}\\right)=\\frac{\\sqrt{\\pi}}{2}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{1}{2\\sqrt{2}} \\cdot \\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n  &= \\frac{1}{\\sqrt[2]{2^3} \\cdot \\sqrt[2]{\\left(1+\\frac{x^{2}}{2}\\right)^3}}  \\\\\n  &= \\frac{1}{(x^2+2)^{\\frac{3}{2}}} \\\\\n  &= \\frac{1}{\\sqrt{(x^2+2)^3}}\n\\end{align*}\n\\]\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_2(k \\cdot x)}{f_2(x)}\n    &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\sqrt{((k\\cdot x)^2+2)^3}}}{\\frac{1}{\\sqrt{(x^2+2)^3}}} = \\lim_{x \\to +\\infty} \\frac{\\sqrt{(x^2+2)^3}}{\\sqrt{((k\\cdot x)^2+2)^3}} \\\\\n    &= \\lim_{x \\to +\\infty} \\left(\\frac{x^2+2}{k^2x^2+2}\\right)^\\frac{3}{2}=\\lim_{x \\to +\\infty} \\left(\\frac{\\frac{x^2}{x^2}+\\frac{2}{x^2}}{k^2\\frac{x^2}{x^2}+\\frac{2}{x^2}}\\right)^\\frac{3}{2} \\\\\n    &=\\left(\\frac{1}{k^2}\\right)^\\frac{3}{2}=\\frac{1}{k^3}=k^{-3}\n\\end{align*}\n\\]\nfür alle reellen \\(k&gt;0\\) ist \\(f_2(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -3\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\begin{align*}\n\\overline{F}_2(x) &= \\int_x^\\infty f_2(t) \\operatorname{d}t = \\int_x^\\infty \\frac{1}{\\sqrt{(t^2+2)^3}} \\operatorname{d}t \\\\\n  &= \\frac{x}{2 \\cdot \\sqrt{x^2+2}}\n\\end{align*}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt für jedes feste \\(k&gt;0\\):\n\\[\n\\begin{align*}\n\\lim\\limits_{x \\to \\infty} \\frac{\\overline{F}_2(k x)}{\\overline{F}_2(x)} &= \\lim\\limits_{x \\to \\infty}k \\cdot \\sqrt{\\frac{x^2+2}{k^2x^2+2}} \\\\\n  &=  k \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{\\frac{1}{k^2} \\cdot \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} \\\\\n  &= \\frac{k}{k} \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{ \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} = 1\\end{align*}\n\\]\nWegen\n\\[\n\\lim_{x \\to \\infty} \\frac{\\frac{1}{\\sqrt{(x^2+2)^3}}}{\\frac{1}{x^3}} =\\lim\\limits_{x \\to \\infty} \\frac{x^3}{(\\sqrt{x^2+2})^3} = \\lim\\limits_{x \\to \\infty} \\left(\\frac{x}{\\sqrt{x^2+2}}\\right)^3= 1\n\\]\nist \\(f_2 \\sim f^*_2\\) und somit auch \\(\\log(f_2) \\sim \\log(f^*_2)\\).\nAus \\(\\log\\left(\\frac{1}{x^3}\\right) = \\log(1)- 3\\cdot\\log(x)\\) können wir daher auf \\(\\alpha = 3\\) und \\(C=1\\) schliesse und schreiben:\n\\[\nf_2^*(x) = C \\cdot x^{-\\alpha} = x^{-3}\n\\]\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 2\n\nf_star &lt;- function(x) {\n  alpha &lt;- 3\n  C &lt;- 1\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie groß ist nun der absolute Fehler zwischen \\(f_2^*\\) und \\(f_2\\) genau?\nEine Grafik zeigt von \\(f_2^*-2_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(f_star(x) - dt(x,df=2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )"
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fussnoten",
    "text": "Fussnoten"
  },
  {
    "objectID": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "href": "posts/2021-02-14-über-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nde Bruijn, N. G. (1981), Asymptotic Methods in Analysis, Dover Publications, ISBN 9780486642215↩︎"
  },
  {
    "objectID": "posts/2021-06-27-datenjudo-für-fragebögen/index.html",
    "href": "posts/2021-06-27-datenjudo-für-fragebögen/index.html",
    "title": "Datenjudo für Fragebögen",
    "section": "",
    "text": "Ab und zu bekomme ich die Frage, wie man einen Fragebogen mit Likert-Scalen-Items auswerten kann.\nDazu kann etwas gezieltes Datenjudo helfen. Wir schauen uns das folgende generierte Mini-Beispiel an:\n\nlibrary(mosaic)  # Basis Paket\nlibrary(tibble)  # Eine modernere Variante der data.frames!\nset.seed(2009)   # Reproduzierbarkeit\n\nN &lt;- 25  # Anzahl der Testzeileneinträge in den \"testdaten\"!\n\n# Wir wollen eine Likert-Scale \nminLikert &lt;- 1  # bis\nmaxLikert &lt;- 6  # erstellen.\n\n# Zum späteren Umrechnen der inversen Items:\nmaxInvItem &lt;- maxLikert + 1\n\n# Wir bauen uns eine Testumfrage mit zwei Itemserien \n# (AS1-AS6 und BS1-BS6) und N Beobachtungen.\n# Die Items AS3, AS4  und BS1 und BS5 sind dabei \n# inverse Items, welche später umgerechnet werden:\ntestdaten &lt;- tibble(\n    ID = 1:N,\n    # AS1-AS6 bilden ein Itemset:\n    AS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS6 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # BS1-BS5 bilden ein Itemset:\n    BS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # Geschlecht als sex mit (1 für Frauen und 2 für Männer)\n    sex = sample(1:2, N, replace = TRUE)\n)\n\n# Orinal testdaten einmal ausgeben:\nhead(testdaten)\n#&gt; # A tibble: 6 × 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     4     4     1     1     2     4     5     5     3     2     3     2\n#&gt; 2     2     2     1     2     2     5     6     4     5     2     2     6     1\n#&gt; 3     3     4     4     6     3     3     4     3     3     4     1     5     1\n#&gt; 4     4     2     6     1     4     5     4     6     4     5     1     3     1\n#&gt; 5     5     3     1     3     5     5     6     6     1     2     6     5     1\n#&gt; 6     6     6     4     1     3     6     6     4     6     5     3     3     1\n\nDie Spalten AS3, AS4 und BS1, BS5 waren inverse Items, die wir noch umrechnen müssen:\n\n# Inverse Item umrechnen:\ntestdaten |&gt;\n    mutate(\n        AS3 = maxInvItem - AS3,\n        AS4 = maxInvItem - AS4,\n        BS1 = maxInvItem - BS1,\n        BS5 = maxInvItem - BS5\n    ) -&gt; testdaten_korrigiert \n\n# Die Daten mit den umgerechnetern inversen Items:\nhead(testdaten_korrigiert)\n#&gt; # A tibble: 6 × 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     1     4     4     6     6     2     4     2     5     3     2     4     2\n#&gt; 2     2     2     1     5     5     5     6     3     5     2     2     1     1\n#&gt; 3     3     4     4     1     4     3     4     4     3     4     1     2     1\n#&gt; 4     4     2     6     6     3     5     4     1     4     5     1     4     1\n#&gt; 5     5     3     1     4     2     5     6     1     1     2     6     2     1\n#&gt; 6     6     6     4     6     4     6     6     3     6     5     3     4     1\n\nDie jeweiligen Itemsets werden nun zur einem Wert (Gesamtscore) zusammengefasst, in dem wir jeweils den Mittelwert von AS1-AS6 und BS1-BS5 bildenund in AS bzw. BS speichern:\n\n# Wir fassen nun die AS1-AS6 und die BS1-BS5 zusammen \n# und bilden die jeweiligen Mittelwerte:\ntestdaten_korrigiert |&gt;\n    group_by(ID, sex) |&gt;  # Damit wird für jede Zeile die Zusammenfassung gemacht!\n    summarise(\n        AS = mean(c(AS1, AS2, AS3, AS4, AS5, AS6)),\n        BS = mean(c(BS1, BS2, BS3, BS4, BS5))\n    ) -&gt; testdaten_sum\n\n# Ausgabe der Mittelwerte der AS und BS\nhead(testdaten_sum)\n#&gt; # A tibble: 6 × 4\n#&gt; # Groups:   ID [6]\n#&gt;      ID   sex    AS    BS\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2  4.33   3.2\n#&gt; 2     2     1  4      2.6\n#&gt; 3     3     1  3.33   2.8\n#&gt; 4     4     1  4.33   3  \n#&gt; 5     5     1  3.5    2.4\n#&gt; 6     6     1  5.33   4.2\n\nDie Datentabelle testdaten_sum enthält nun die Spalten AS und BS mit den entsprechenden Mittelwerten der einzelnen Items AS1-AS6 sowieso BS1- BS5.\nWir wollen nun die Ergebnisse als Boxplots anzeigen lassen. Dafür benennen wir die Geschlechter von 1,2 auf “Frau”, “Mann” um:\n\ntestdaten_sum |&gt;\n    mutate(sex = factor(sex, levels = c(1, 2),\n                             labels = c(\"Frau\", \"Mann\"))\n    ) -&gt; testdaten_sex \n\nNun können wir die Boxplots erstellen:\n\n# Darstellung der Ergebnisse als Boxplot AS ~ sex:\ngf_boxplot(AS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von AS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item AS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(2.5, 4.5)  # Gibt den Bereich von 2.5 bis 4.5 aus!\n    )  \n  )\n\n# Darstellung der Ergebnisse als Boxplot BS ~ sex:\ngf_boxplot(BS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von BS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item BS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(1, 6)  # Gibt den ganzen Bereich von 1 bis 6 aus!\n    )  \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Kennzahlen dazu erhalten wir mit favstats. Dabei wählen wir die ersten sechs Einträge (Variabelbezeichnung und Q0 bis Q4) aus:\n\nfavstats(AS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex      min       Q1   median       Q3      max\n#&gt; 1 Frau 2.166667 3.333333 3.666667 4.166667 5.333333\n#&gt; 2 Mann 3.666667 4.000000 4.333333 4.333333 4.500000\nfavstats(BS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex min  Q1 median  Q3 max\n#&gt; 1 Frau 2.4 2.8    3.4 4.2 4.6\n#&gt; 2 Mann 2.6 3.2    3.3 3.8 4.2\n\nUnter der Verwendung des Pakets likert (https://github.com/jbryer/likert) können wir die Ausgaben auch noch etwas schöner gestalten:\n\nlibrary(likert)\n\n# Wir wählen nur den Itemset BS aus und speichern in in items2:\ntestdaten_korrigiert |&gt;\n  select(\n    starts_with(\"BS\")\n  ) -&gt; items2\n\n# Leider mag likert tibbels nicht so gerne, daher:\nitems2 &lt;- as.data.frame(items2)\n\n# Wir geben den Items noch ein paar Buzzwords:\nnames(items2) &lt;- c(\"Gesundheit\", \"Familie\", \"Geld\", \"Freunde\", \"Langes Leben\")\n\n# Vorbereitung:\nl2 &lt;- likert(items2, nlevels = 5)\n\n# Zusammenfassung\nsummary(l2)\n#&gt;           Item      low   neutral     high     mean       sd\n#&gt; 3         Geld 28.57143 23.809524 47.61905 3.380952 1.359272\n#&gt; 5 Langes Leben 57.14286  4.761905 38.09524 2.619048 1.532194\n#&gt; 1   Gesundheit 38.09524 28.571429 33.33333 2.857143 1.492840\n#&gt; 2      Familie 38.09524 28.571429 33.33333 2.952381 1.499206\n#&gt; 4      Freunde 42.85714 23.809524 33.33333 2.857143 1.236354\n\n# Graphische Ausgaben:\nplot(l2)\n\nplot(l2,\"bar\")\n\nplot(l2,\"heat\")\n\nplot(l2,\"density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoilà!"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html",
    "title": "Regression mit studentisierten Daten",
    "section": "",
    "text": "Bei einer einfachen linearen Regression versuchen wir zu vorgegebenen Datenpunkten \\((x_1, y_1), \\cdots (x_n, y_n)\\) die Parameter einer möglichst passenden Gerade \\(g(x)=\\beta_0 + \\beta_1 \\cdot x\\) zu schätzen.\nDie Schätzung des y-Achsenabschnitts \\(\\hat\\beta_0\\) und der Steigung \\(\\hat\\beta_1\\) erfolgt dabei algebraisch exakt mittels:\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1 \\cdot \\bar{x} \\quad\\text{und}\\quad \\hat\\beta_1 = \\frac{s_x}{s_y}\\cdot r_{x,y}\n\\]\nDabei sind \\(\\bar{x}\\) bzw. \\(\\bar{y}\\) die Mittelwerte und \\(s_x\\) bzw. \\(s_y\\) die Standardabweichungen der Datenpunkte \\(x_i\\) bzw. \\(y_i\\); darüberhinaus ist \\(r_{x,y}\\) der Korrelationskoeffizient der Datenpunkte.\nBeim studentisieren werden die Datenpunkte bzgl. des Mittelwertes zentriert und bzgl der Standardabweichung normiert:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x} \\quad\\text{bzw.}\\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}\n\\]\nWas passiert nun durch eine solche Studentisierung (oft auch z-Transformation genannt) mit den geschätzen Parametern?\nDie Mittelwerte \\(\\bar{x}^{stud}\\) und \\(\\bar{y}^{stud}\\) werden zu Null. Die Standardabweichungen \\(s_{x^{stud}}\\) und \\(s_{y^stud}\\) werden zur Eins:\n\\[\n\\bar{x}^{stud}=0=\\bar{y}^{stud} \\qquad s_{x^{stud}}= 1 = s_{y^{stud}}\n\\]\nDer y-Achsenabschnitt wird nun durch\n\\[\n\\hat\\beta_0^{stud}\n= \\bar{y}^{stud} - \\hat\\beta_1^{stud} \\cdot \\bar{x}^{stud}\n= 0 - \\hat\\beta_1^{stud} \\cdot 0 = 0\n\\]\nund die Steigung durch\n\\[\n\\hat\\beta_1^{stud}\n= \\frac{s_{x^{stud}}}{s_{y^{stud}}}\\cdot r_{x^{stud},y^{stud}}\n= \\frac{1}{1}\\cdot r_{x^{stud},y^{stud}} = r_{x^{stud},y^{stud}}\n\\]\ngeschätzt.\nFür den Korrelationskoeffienten gilt nun\n\\[\nr_{x^{stud},y^{stud}}\n= \\frac{s_{x^{stud},y^{stud}}}{s_{x^{stud}}\\cdot_{y^{stud}}}\n= \\frac{s_{x^{stud},y^{stud}}}{1 \\cdot 1}\n= s_{x^{stud},y^{stud}}.\n\\]\nDamit Schätzen wir unsere Steigung \\(\\hat\\beta_1^{stud}\\) direkt aus der Kovarianz \\(s_{x^{stud},y^{stud}}\\).\nDamit gilt:\n\\[\n\\hat\\beta_1^{stud} = r_{x^{stud},y^{stud}} = s_{x^{stud},y^{stud}} \\in [-1, 1]\n\\]\nIn Worten zusammengefasst: Im studentisierten Fall ist"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#reproduzierbarkeitsinformationen",
    "title": "Regression mit studentisierten Daten",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#footnotes",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#footnotes",
    "title": "Regression mit studentisierten Daten",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nDas “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎\nSie können hier auch die Funktion scale() verwenden!↩︎"
  },
  {
    "objectID": "posts/2021-06-11-wege-zur-normalverteilung/index.html",
    "href": "posts/2021-06-11-wege-zur-normalverteilung/index.html",
    "title": "Wege zur Normalverteilung",
    "section": "",
    "text": "Der fairen Wurf einer fairen Münze, also eine Münze bei der Kopf und Zahl gleich wahrscheinlich geworfen wird, sei der Ausgang des ersten Weges.\nWir können den Münzwurf mit R simulieren:\nlibrary(mosaic)\nset.seed(2009)\n\nrflip(1)\n\n\nFlipping 1 coin [ Prob(Heads) = 0.5 ] ...\n\nT\n\nNumber of Heads: 0 [Proportion Heads: 0]\nGenauso wie den Wurf zweier Münzen:\nrflip(2)\n\n\nFlipping 2 coins [ Prob(Heads) = 0.5 ] ...\n\nH H\n\nNumber of Heads: 2 [Proportion Heads: 1]\nOder auch von 20 Münzwürfen:\nrflip(20)\n\n\nFlipping 20 coins [ Prob(Heads) = 0.5 ] ...\n\nT T T T H H T T T H H H H T H T T H H H\n\nNumber of Heads: 10 [Proportion Heads: 0.5]\nWir wollen uns dafür interessieren, wie der Zufall auf jeweils \\(n\\) Münzwürfe einwirkt\nUnd wieder holen dafür die drei Experiment jeweils \\(N=10^{4}\\) mal und schauen uns danach anwie die Anzahl der Kopf Würfe variiert:\nN &lt;- 10000\nn &lt;- 1\nvrtlg_1 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_1) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5,n + 0.5)))\n\\(n=2\\)\nn &lt;- 2\nvrtlg_2 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_2) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\n\\(n=20\\)\nn &lt;- 20\nvrtlg_20 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_20) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\n\\(n=50\\)\nn &lt;- 50\nvrtlg_50 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_50) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\nDer faire Wurf eine fairen Münze \\(X\\) ist aber vorallem ein Gedanken-Expermiment, bei dem wir davon ausgehen, dass die Wahrscheinlichkeit für Kopf gleich der Wahrscheinlichkeit für Zahl ist:\n\\[\nP(X = \\text{\"Kopf\"}) = P(X = \\text{\"Zahl\"}) = 50\\,\\% = 0{,}5\n\\]\nWir wollen die beiden Ergebnisse kodieren: \\(\\text{\"Kopf\"}\\) mit \\(1\\) und \\(\\text{\"Zahl\"}\\) mit \\(0\\). Somit können wir schreiben:\n\\[\nP(X = 0)  = 0{,}5 = P(X = 1)\n\\]\nFür denn Fall, dass die Münze nicht mehr fair ist wollen wir vereinbaren, dass wir mit \\(q = P(X = 0)\\) und \\(p = P(X = 1)\\) die jeweiligen Wahrscheinlichkeiten bezeichnen wollen. Es gilt aber immer, dass \\(q+p = 1\\) ist!\nEine Variable \\(X\\) die dem Zufall ein Wert \\(x\\) zuweist, wollen wir Zufallsvariable nennen.1\nAusgehen von der Annahme können wir uns diese theoretischen Verteilungen auch einmal ansehen:\np = 0.5\nn &lt;- 1\ngf_dist(\"binom\", size = n, prob = p)\nn &lt;- 2\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_2)\ndist_2 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_2, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 20\ngf_dist(\"binom\", size = n, prob = p)\nn &lt;- 20\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_20)\ndist_20 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_20, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 50\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_50)\ndist_50 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = 0.5) %&gt;%\n  gf_point(density ~ x, data = dist_50, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 100\nN &lt;- 5 * n**2\nvrtlg_100 &lt;- do(N) * rflip(n)\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_100)\ndist_100 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_100, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 300\nN &lt;- 5 * n**2\nvrtlg_300 &lt;- do(N) * rflip(n)\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_300)\ndist_300 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_300, color = \"lightgreen\", alpha = 0.7)\nZeichnen wir nun die Gauß’sche Glockenkurve in rot dazu:\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_300, color = \"lightgreen\", alpha = 0.7) %&gt;%\n  gf_dist(\"norm\", mean &lt;- n*p, sd = sqrt(n*p*(1 - p)), color = \"red\")\nDie Gauß’sche Glockenkurve ist die Dichtefunktion der Normalverteilung und ist definiert durch:\n\\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\nMit den beiden Parameter \\(\\mu\\) und \\(\\sigma^2\\) kann man den Mittelwert der Verteilung (auch Erwartungswert genannt) und die Varianz der Verteilung einstellen.\nWir haben diese Werte oben mit den theoretischen Werten der Binomialverteilung \\(\\mu = E[X] = p \\cdot n\\) und \\(\\sigma = \\sqrt{\\sigma^2}= \\sqrt{Var[X]} = \\sqrt{n \\cdot p \\cdot (1-p)}\\) belegt.\nWir sehen, die (simulierten) relativen Häufigkeiten der Münzwürfe streben mit steigendem \\(N\\) mehr und mehr in Richtung der (theoretischen) Wahrscheinlichkeiten der Binomialverteilung und diese (mit steigendem \\(n\\)) gegen die Gauß’sche Glockenkurve der Normalverteilung.\nEine Verteilungsfunktion \\(F(x)\\) gibt an, wie wahrscheinlich es ist, einen Wert \\(\\leq x\\) zu beobachten:\n\\[\nF(x) = P(X \\leq x)\n\\]\nNatürlich ist damit immer \\(0 \\leq F(x) \\leq 1\\).\nEine empirische Verteilungsfunktion \\(F_n(x)\\)gibt an, wie groß die relative Häufigkeit des eintretens von Werten \\(\\leq x\\) bei einem Stichprobenumfang von \\(n\\) waren: \\[F_n(x) = \\frac{\\text{Anzahl der Werte} \\leq x}{n}\\]\nBetrachen wir nun empirische Verteilungsfunktion unserer Experimente:\nn = 2\ngf_ecdf( ~ heads, data=vrtlg_2)\nn = 20\ngf_ecdf( ~ heads, data=vrtlg_20)\nn = 50\ngf_ecdf( ~ heads, data=vrtlg_50)\nn = 300\ngf_ecdf( ~ heads, data=vrtlg_300)\nTragen wir zur empirische Verteilungsfunktion auch die Verteilungsfunktion der Normalverteilung von oben ein:\nn &lt;- 300\ngf_ecdf( ~ heads, data = vrtlg_300) %&gt;%\n  gf_dist(\"norm\", mean &lt;- n*p, sd = sqrt(n*p*(1 - p)), kind=\"cdf\", color = \"red\")\nDie empirische Verteilungsfunktion strebt also gegen die (theoretisch) Verteilungsfunktion der Normalverteilung. Sie lautet:\n\\[\nF(x) = \\int_{-\\infty}^x f(u)\\, \\text{d} u = \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "posts/2021-06-11-wege-zur-normalverteilung/index.html#footnotes",
    "href": "posts/2021-06-11-wege-zur-normalverteilung/index.html#footnotes",
    "title": "Wege zur Normalverteilung",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nEigentlich handelt es sich damit strenggenommen um eine Funktion. Und es müssen noch weitere Eigenschaften erfüllt sein. Aber darauf gehen wir hier nicht weiter ein.↩︎"
  },
  {
    "objectID": "posts/2021-02-12-ein-paar-gedanken-über-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "href": "posts/2021-02-12-ein-paar-gedanken-über-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "title": "Ein paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)",
    "section": "",
    "text": "Eine Funktion \\(f(x)\\) heißt potenzgesetzlich, falls\n\\[f(x) = C \\cdot x^a\\]\ngilt, für mindestens alle reellen \\(x &gt; x_{min}\\).\nGewöhnlich setzt man \\(\\alpha = -a\\) und schreibt\n\\[f(x) = C \\cdot x^{-\\alpha}.\\]\nDamit ergibt sich für \\(f'(x)\\) die Form:\n\\[\nf'(x) = -C \\cdot \\alpha \\cdot x^{-\\alpha -1} = C^* \\cdot x^{-(\\alpha + 1)}\n\\] mit \\(C^* = -C \\cdot \\alpha\\).\nEine (streng) potenzgesetzliche Verteilungen (engl. (strong) power-law probability distribution) zur ZV \\(X\\) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}_X(x)=P(X &gt; x)\\) die folgende Gestalt hat: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha}\\]\nMit der Dichte \\(f_X\\) ergibt sich: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha} =\\int_x^\\infty f_X(t) \\text{d}t = C^* \\cdot \\int_x^\\infty  t^{-(\\alpha+1)} \\text{d}t = C^* \\cdot \\int_x^\\infty t^{-\\alpha} \\text{d}t\\]\nAnstelle der Konstanten \\(C\\) tritt oft eine langsam variierende Funktion (engl. slowly varying funktion). Wir erhalten somit die folgende, allgemeinere Definition:\nEine potenzgesetzliche Verteilungen (engl. power-law probability distribution) (zu einer Zufallsvariable \\(X\\)) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}(x)=P(X &gt; x)\\) die folgende Gestalt hat:\n\\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha}\\]\nDabei ist \\(L(x):(x_{\\min}, +\\infty) \\to (x_{\\min}, +\\infty)\\) eine langsam variierende Funktion, also gilt für alle \\(t&gt;0\\):\n\\[\n\\lim_{x \\to +\\infty} \\frac{L(t \\cdot x)}{L(x)} = 1\n\\]\nIst nun wieder \\(f_X\\) die Dichte, so erhalten wir: \\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha} = \\int_x^\\infty f_X(t) dt\\]\n\\[f'(x) = [L(x)x^{-\\alpha}]'\\] \\[\\int_x^\\infty f_X(t) dt= \\int_x^\\infty [L(t)t^{-\\alpha}]' dt\\]\n\\(\\Delta x_0 = h = x_1 - x_0\\) \\(x_1 = x_0 + \\Delta x_0 = x_0 + h\\) \\(x_1 = c \\cdot x_0\\) \\(x_0 + h = c \\cdot x_0 &lt;=&gt; c = 1 + \\frac{h}{x_0}\\) \\(L(x_1) = L(x_0+h) = L(c \\cdot x_0)\\) \\(L(x_1) - L(x_0) = L(c \\cdot x_0) - L(x_0) = L(x_0 + h) - L(x_0)}\\) $\nFakten:\n\nSinnvoll nur, wenn \\(\\alpha &gt; 0\\).\nIst \\(\\alpha &lt; 3\\), dann ist die Varianz und die Schiefe (engl. skewness) (mathematisch) nicht definiert.\nFür \\(k &gt; \\alpha-1\\) ist das k. Moment unendlich.\n\nLogarithmiert man \\(y=f(x)=C \\cdot x^{-\\alpha}\\), so erhält mensch:\n\\[\\log(y) = \\log(C) -\\alpha \\cdot \\log(x)\\]\nIst eine Verteilung potenzgesetzlich, dann kann man \\(\\alpha\\), wie folgt abschätzen:\nSeien \\(x_0, x_1 &gt; x_{min}\\) zwei reelle Zahlen, \\(y_0=f(x_0)\\) bzw. \\(y_1 = f(y_1)\\).\nDann kann mensch wegen\n\\[\\begin{align*}\n  \\log(y_1) - \\log(y_0) &= \\log(C) - \\alpha \\cdot\\log(x_1) - \\log(C) + \\alpha \\cdot \\log(x_0) \\\\\n                        &= \\alpha \\cdot\\left(\\log(x_0)- \\log(x_1) \\right)\n\\end{align*}\\]\nden Wert für \\(\\alpha\\), so kann man mittels\n\\[\\alpha = \\frac{\\log(y_1) - \\log(y_0)}{\\log(x_0)- \\log(x_1)}\\]\nden Wert für \\(\\alpha\\) bestimmen.\nMit dem so ermittelten \\(\\alpha\\), können wir \\(C\\) wegen\n\\[\\log(C) = \\log(y)+ \\alpha\\log(x)\\]\nmit Hilfe von\n\\[C = y \\cdot x_0^\\alpha = f(x_0) \\cdot x_0^\\alpha\\]\nfür ein \\(x_0 &gt; x_{min}\\) abschätzen.\nBeispiel:\nNehmen wir die t-Verteilung mit \\(n=2\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{2}(x)\\).\nDann dann können wir \\(\\alpha=3\\) und \\(C=1\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nNoch ein Beispiel:\nNehmen wir die t-Verteilung mit \\(n=1\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{1}(x)\\).\nDann dann können wir \\(\\alpha=2\\) und \\(C=0.32\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nEin ’Gegen-’Beispiel:\nBetrachten wir nun die (rechte Seite – \\(x&gt;1=x_{min}\\)) einer Gauß’schen Standardnormalverteilung.\nMit den Stützstellen \\(x_0 = 1\\) und \\(x_1 = 5\\) können wir \\(\\alpha=7.46\\) und \\(C=0.24\\) abschätzen. Schauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"norm\",\n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWir erkennen, dass hier etwas nicht passt. Die Standardnormalverteilung ist (vielleicht) keine potenzgesetzich Verteilung?\nEin oft verwendetes Kriterium ist, dass sich die Funktion in der doppelt-logarithmischen Darstellung als Gerade offenbart.\nSchauen wir daher einmal nach:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\n** — **\nWeiter gilt für potenzgesetzliche Verteilungen wegen\n\\[\\frac{f(x)}{f(c\\cdot x)} = \\frac{C \\cdot x^{-\\alpha}}{C \\cdot (c\\cdot x)^{-\\alpha}}\n= c^\\alpha\\]\n\\(f(x)\\) und \\(f(c\\cdot x)\\) für alle (beliebig aber festen) \\(c&gt;0\\) proportional, was man gerne als \\(f(x) \\propto f(c \\cdot x)\\) schreibt.\n** — **\nDie Wahrscheinlichkeit für ein (mindestens) \\(8-\\sigma\\) Ereignis liegt bei einer Standardnormalverteilung bei etwa \\(6.66133814775094\\times 10^{-16}\\).\nBei einer t-Verteilung mit 2 Freiheitsgeraden bei etwa 0.00763403608266899$.\nWährend die Eintrittschance eines (mindestens) \\(8-\\sigma\\) Ereignisses bei der Standardnormalverteilung bei etwa \\(1 : round(1/(1-pnorm(8)),0)\\) liegt, ist diese der t-Verteilung mit einem Freiheitsgrad bei etwa $1 : 131"
  },
  {
    "objectID": "posts/2021-02-13-behäbige-funktionen-aka-slowly-varying-function/index.html",
    "href": "posts/2021-02-13-behäbige-funktionen-aka-slowly-varying-function/index.html",
    "title": "Behäbige Funktionen aka slowly varying function",
    "section": "",
    "text": "Reelle Funktionen, die ihren Funktionswert kaum ändern, kann man mit Fug und Recht durchaus behäbig nennen, korrekter wäre aber von langsam variierenden Funktionen zu sprechen\nIm Kontext von potenzgesetzlichen Verteilungen kommt der Begriff slowly varying function vor, der Funktionen beschreibt die nur sehr gering auf Änderungen ihres Parameters reagieren.\nDie Definition dieser behäbigen besser langsam variierenden Funktionen stammt von Jovan Karamata:\nEine positive stetige Funktion \\(L\\)1 auf den positiven reelen Zahlen ist langsam variierend (im unendlichen), falls für alle reellen \\(t&gt;0\\)\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = 1\\]\ngilt.\nBeispiele:\nBeweisskizze: Mit \\(L(x) = c\\) ist \\(L(x) = L(t x) = c\\) und damit \\(\\frac{L(t x)}{L(x)}= 1\\).\nBeweisskizze: Da \\(\\lim\\limits_{x \\to +\\infty} L(x) = b = \\lim\\limits_{x \\to +\\infty} L(t\\cdot x)\\) ist \\(\\lim\\limits_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = \\frac{\\lim\\limits_{x \\to +\\infty} L(t\\cdot x)}{\\lim\\limits_{x \\to +\\infty} L(x)} = \\frac{b}{b} = 1\\)\nBeweisskizze: Es gilt: - Für jede reelle Zahl \\(x&gt;0\\) ist \\(\\log_x(x) = 1\\). - Für reelle Zahlen \\(a, b\\) gilt: \\(\\frac{log(a)}{\\log(b)} = \\log_b(a)\\) - Für reelle Zahlen \\(a, b\\) gilt. \\(\\log(a \\cdot b) = log(a) + log(b)\\) - Für jede Konstante \\(k\\) gilt \\(\\lim_{x \\to +\\infty} \\log_x (k) = 0\\) Somit gilt \\(\\frac{\\log_\\beta(k \\cdot x)}{\\log_\\beta(x)} = \\log_x(k\\cdot x) = \\log_x(k) + \\log_x(x) = \\log_x(k) +1 \\to 1\\) wenn \\(x \\to +\\infty\\)\nEine regulär variierende Funktion \\(L:(0,+\\infty) \\to (0,+\\infty)\\) ist eine Funktion für die der Term\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = g(t)\\]\nmit \\(g(t)\\) für alle \\(t&gt;0\\) einen endlichen aber nicht verschwindenen Wert (m.a.W.: \\(g(t) \\neq 0\\)) hat .\nKaramata hat die regulär variierenden Funktionen nun wie folgt charaterisiert:"
  },
  {
    "objectID": "posts/2021-02-13-behäbige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "href": "posts/2021-02-13-behäbige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "title": "Behäbige Funktionen aka slowly varying function",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\n\\(L\\) oder auch \\(l\\) wird (angeblich) hier für den Begriff lente (serb. für faul) verwendet. Behäbig ist also doch nicht so falsch. ;-)↩︎"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-für-r",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-für-r",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "Für die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Vorbemerkungen und Notationen",
    "text": "Vorbemerkungen und Notationen\nDa alle t-Verteilungen symmetrisch sind, betrachten wir im wesendlichen nur den positiven Teil.\nZwei reelle Funktionen \\(f\\), \\(g\\) sind genau dann, im Sinne von de Bruijn1 (§1.4), asymptotisch äquivalent \\(f \\sim g\\), wenn\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{f(x)}{g(x)} = 1\n\\]\ngilt.\nIst \\(f \\sim g\\), so können wir auch\n\\[\nf(x) = g(x)\\cdot(1+o(1))\n\\]\ndafür schreiben. Dabei ist \\(h(x) = o(\\phi(x))\\) für \\(x \\to \\infty\\), falls \\(\\lim\\limits_{x \\to \\infty} \\frac{h(x)}{\\phi(x)} = 0\\) gilt. Aus der asymptotischen Äquivalenz von \\(f\\) und \\(g\\) folgt nun direkt:\n\\[\n\\lim\\limits_{x \\to \\infty}\\frac{f(x)}{g(x)}-1 =\\frac{f(x)-g(x)}{g(x)} = 0\n\\]\nMit \\(h(x) = \\frac{f(x)-g(x)}{g(x)}\\) ist \\(h(x) = o(1)\\) und daher \\(f(x)-g(x) = g(x)o(1)\\) und schliesslich \\(f(x) = g(x)+g(x)o(1)\\).\nEin wichtiges Korrolar sagt:\nIst \\(f \\sim g\\), so ist auch \\(\\log(f) \\sim \\log(g)\\)."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung im Allgemeinen",
    "text": "Die t-Verteilung im Allgemeinen\nDie Dichtefunktion der t-Verteilung lauten im Allgemeinen:\n\\[\nf_n(x) = \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}}\\quad \\mathrm{für}\\quad -\\infty &lt; x &lt; +\\infty\n\\]\nwobei wir mit \\(\\Gamma(x)\\) die Gammafunktion\n\\[\n\\Gamma(x)=\\int\\limits_{0}^{+\\infty}t^{x-1}e^{-t}\\operatorname{d}t\n\\]\nbezeichnen. Für einige \\(x\\) nimmt die Gammafunktion leicht zu berechnende Werte an:\nSo ist für alle \\(n\\in\\mathbf{N_0}\\):\n\\(\\Gamma(n+1) = n!\\) und \\(\\Gamma\\left(n + \\frac{1}{2}\\right) = \\frac{(2n)!}{n!4^n}\\sqrt{\\pi}\\)\nmit der gewöhnlichen Fakultät \\(n! = \\prod_{i=0}^n i\\), wobei per Definition \\(0!=1\\) ist."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung mit einem Freiheitsgrad",
    "text": "Die t-Verteilung mit einem Freiheitsgrad\nFür \\(f_1(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n&= \\frac{\\Gamma\\left(\\frac{2}{2}\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-\\frac{2}{2}} \\\\\n&= \\frac{\\Gamma\\left(1\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-1} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{1} {\\sqrt{\\pi} \\cdot \\sqrt{\\pi}} \\cdot \\left(1+x^{2}\\right)^{-1} \\\\\n       &= \\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\n\\end{align*}\n\\]\nDas ist die Dichtefunktion der standardisierten Cauchy-Verteilung\n\\[\nf_{(\\mu,\\lambda)}(x) = \\frac{1}{\\pi} \\cdot \\frac{\\lambda}{\\lambda^2+(x-\\mu)^2}\n\\]\nmit (\\(\\mu = 0\\) und \\(\\lambda=1\\)), welche – bekanntermaßen – keinen Erwartungwert hat.\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_1(k \\cdot x)}{f_1(x)}\n      &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\pi} \\cdot \\frac{1}{1+(kx)^{2}}}{\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}} = \\lim_{x \\to +\\infty} \\frac{1+x^2}{1+k^2x^2} \\\\\n      &=\\lim_{x \\to +\\infty} \\frac{\\frac{1}{x^2}+\\frac{x^2}{x^2}}{\\frac{1}{x^2}+k^2\\frac{x^2}{x^2}} =\\frac{1}{k^2}=k^{-2}\n\\end{align*}\n\\]\nfür alle reellen \\(k&gt;0\\) ist \\(f_1(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -2\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\overline{F}_1(x) = \\int_x^\\infty f_1(t) \\operatorname{d}t = \\frac{1}{\\pi} \\cdot \\int_x^\\infty  \\frac{1}{1+x^{2}} \\operatorname{d}t = \\frac{\\arctan(x)}{\\pi}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt nun:\n\\[\n\\arctan`(x)= \\frac{1}{1+x^2} \\to \\frac{1}{x^2} \\text{ für } x\\to \\infty\n\\]\nGenauer gilt wegen\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{\\frac{1}{1+x^2}}{\\frac{1}{x^2}}\n= \\lim\\limits_{x \\to \\infty} \\frac{x^2}{1+x^2} =1,\n\\]\ndass \\(\\frac{1}{1+x^2} \\sim \\frac{1}{x^2}\\), also asymptotisch äquivalent sind und somit auch \\(\\log\\left(\\frac{1}{1+x^2}\\right) \\sim \\log\\left(\\frac{1}{x^2}\\right)\\).\nZusammen gefasst gilt somit: \\[\n\\log\\left(\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\\right) \\to -2\\log(x) - \\log(\\pi) \\text{ für } x \\to \\infty\n\\]\nSei \\(f_1^*(x) = C \\cdot x^{-\\alpha}\\) mit \\(\\alpha = 2\\) und \\(C=\\frac{1}{\\pi} \\approx0.3183\\).\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 1\n\nf_star &lt;- function(x) {\n  alpha &lt;- 2\n  C &lt;- 1/pi\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie groß ist nun der (absolute) Fehler zwischen \\(f_1^*\\) und \\(f_1\\)?\nEine Grafik von \\(f_1^*-f_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(x**-2 - 1/(1+x**2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nGenauer gilt:\n\\[\nf_1^*(x) - f_1(x) = \\frac{1}{x^2+x^4}\n\\]\nWir können also für ein hinreichend großes \\(x &gt;&gt; 1\\) statt \\(f_1\\) auch \\(f_1^*\\) verwenden und erhalten somit:\n\\[\n\\begin{align*}\n\\overline{F}_1(x) &\\approx \\int_x^\\infty f_1^*(t) \\operatorname{d}t\n  = \\int_x^\\infty  C \\cdot t^{-\\alpha} \\operatorname{d}t \\\\\n  &= \\frac{1}{\\pi} \\cdot \\int_x^\\infty  t^{-2} \\operatorname{d}t\n  = \\frac{1}{\\pi}\\left[\\lim\\limits_{\\epsilon \\to \\infty} \\left(-\\epsilon^{-1}\\right) -\\left(-x^{-1}\\right)\\right]\\\\\n  &= \\frac{1}{\\pi}\\cdot\\left[0 + \\frac{1}{x}\\right] = \\frac{1}{\\pi \\cdot x}\n\\end{align*}\n\\]\nWie hinreichend ist hier hinreichend groß?\nTaleb schreibt an dieser Stelle gerne, dass man jenseits des Karamata-Punktes die Karamata-Konstante anwenden kann. Beides Begriffe, zu denen ich zunächst keine echte Definition gefunden habe.\nJovan Karamata ist der Begründer der langsam variierend Funktionen. 1930 zeite er, dass eine positive stetige Funktion \\(L\\) auf den positiven reellen Zahlen genau dann langsam variierend ist, also für alle \\(t &gt; 0\\) die Bedingung\n\\[\nL(t\\,x)/L(x) \\to 1 \\qquad\\text{für}\\qquad x \\to \\infty\n\\]\nerfüllt, wenn sie für ein \\(a &gt; 0\\) für \\(x &gt; a\\) in der Form\n\\[\nL(x) = c(x) \\cdot \\exp \\left(\\int_a^x\\!\\varepsilon(t)/t \\ dt \\right)\n\\] mit \\(c(x) \\to c &gt; 0\\) und \\(\\varepsilon(x) \\to 0\\) für \\(x \\to \\infty\\) geschrieben werden kann.\nIch vermute also, dass wir \\(L(x)\\) ab dem Punkt \\(a\\) näherungsweise durch \\(c(x)\\) besser sogar durch die Konstante \\(c\\) ersetzen könnten.\nDie Karamata-Konstante ist \\(\\rho = -\\alpha\\), also \\(\\rho = c\\)?\nDer Karamata-Punkt bleibt etwas nebulöser. Es könnte sich hier um den Punkt \\(a\\) handeln und vermutlich könnte man hier so argumentieren:\nWenn die Fehler zwischen \\(f\\) und \\(f^*\\) hinreichend klein ist.\nHierfür könnte man einen absoluten Fehler oder einen relativen Fehler als Maßstab ansehen.\nFür einen relativen Fehler vielleicht \\(\\frac{f^*-f}{x} &lt; k\\)?\nOder man betrachtet hier gleich \\(\\frac{f^*-f}{\\log(x)} &lt; k^*\\)?"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "t-Verteilung mit zwei Freiheitsgeraden",
    "text": "t-Verteilung mit zwei Freiheitsgeraden\nFür \\(f_2(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(\\frac{2}{2}\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(1\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{3}{2}\\right)=\\frac{\\sqrt{\\pi}}{2}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{1}{2\\sqrt{2}} \\cdot \\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n  &= \\frac{1}{\\sqrt[2]{2^3} \\cdot \\sqrt[2]{\\left(1+\\frac{x^{2}}{2}\\right)^3}}  \\\\\n  &= \\frac{1}{(x^2+2)^{\\frac{3}{2}}} \\\\\n  &= \\frac{1}{\\sqrt{(x^2+2)^3}}\n\\end{align*}\n\\]\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_2(k \\cdot x)}{f_2(x)}\n    &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\sqrt{((k\\cdot x)^2+2)^3}}}{\\frac{1}{\\sqrt{(x^2+2)^3}}} = \\lim_{x \\to +\\infty} \\frac{\\sqrt{(x^2+2)^3}}{\\sqrt{((k\\cdot x)^2+2)^3}} \\\\\n    &= \\lim_{x \\to +\\infty} \\left(\\frac{x^2+2}{k^2x^2+2}\\right)^\\frac{3}{2}=\\lim_{x \\to +\\infty} \\left(\\frac{\\frac{x^2}{x^2}+\\frac{2}{x^2}}{k^2\\frac{x^2}{x^2}+\\frac{2}{x^2}}\\right)^\\frac{3}{2} \\\\\n    &=\\left(\\frac{1}{k^2}\\right)^\\frac{3}{2}=\\frac{1}{k^3}=k^{-3}\n\\end{align*}\n\\]\nfür alle reellen \\(k&gt;0\\) ist \\(f_2(x)\\) eine regulär variierende Funktion mit Variationsindex \\(\\rho = -3\\).\nDie Überlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\begin{align*}\n\\overline{F}_2(x) &= \\int_x^\\infty f_2(t) \\operatorname{d}t = \\int_x^\\infty \\frac{1}{\\sqrt{(t^2+2)^3}} \\operatorname{d}t \\\\\n  &= \\frac{x}{2 \\cdot \\sqrt{x^2+2}}\n\\end{align*}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen dürfen.\nEs gilt für jedes feste \\(k&gt;0\\):\n\\[\n\\begin{align*}\n\\lim\\limits_{x \\to \\infty} \\frac{\\overline{F}_2(k x)}{\\overline{F}_2(x)} &= \\lim\\limits_{x \\to \\infty}k \\cdot \\sqrt{\\frac{x^2+2}{k^2x^2+2}} \\\\\n  &=  k \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{\\frac{1}{k^2} \\cdot \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} \\\\\n  &= \\frac{k}{k} \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{ \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} = 1\\end{align*}\n\\]\nWegen\n\\[\n\\lim_{x \\to \\infty} \\frac{\\frac{1}{\\sqrt{(x^2+2)^3}}}{\\frac{1}{x^3}} =\\lim\\limits_{x \\to \\infty} \\frac{x^3}{(\\sqrt{x^2+2})^3} = \\lim\\limits_{x \\to \\infty} \\left(\\frac{x}{\\sqrt{x^2+2}}\\right)^3= 1\n\\]\nist \\(f_2 \\sim f^*_2\\) und somit auch \\(\\log(f_2) \\sim \\log(f^*_2)\\).\nAus \\(\\log\\left(\\frac{1}{x^3}\\right) = \\log(1)- 3\\cdot\\log(x)\\) können wir daher auf \\(\\alpha = 3\\) und \\(C=1\\) schliesse und schreiben:\n\\[\nf_2^*(x) = C \\cdot x^{-\\alpha} = x^{-3}\n\\]\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 2\n\nf_star &lt;- function(x) {\n  alpha &lt;- 3\n  C &lt;- 1\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie groß ist nun der absolute Fehler zwischen \\(f_2^*\\) und \\(f_2\\) genau?\nEine Grafik zeigt von \\(f_2^*-2_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(f_star(x) - dt(x,df=2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fussnoten",
    "text": "Fussnoten"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "title": "Über die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nde Bruijn, N. G. (1981), Asymptotic Methods in Analysis, Dover Publications, ISBN 9780486642215↩︎"
  },
  {
    "objectID": "posts/2021-02-12-ein-paar-gedanken-ueber-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "href": "posts/2021-02-12-ein-paar-gedanken-ueber-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "title": "Ein paar Gedanken über potenzgesetzliche Verteilungen (power law distributions)",
    "section": "",
    "text": "Eine Funktion \\(f(x)\\) heißt potenzgesetzlich, falls\n\\[f(x) = C \\cdot x^a\\]\ngilt, für mindestens alle reellen \\(x &gt; x_{min}\\).\nGewöhnlich setzt man \\(\\alpha = -a\\) und schreibt\n\\[f(x) = C \\cdot x^{-\\alpha}.\\]\nDamit ergibt sich für \\(f'(x)\\) die Form:\n\\[\nf'(x) = -C \\cdot \\alpha \\cdot x^{-\\alpha -1} = C^* \\cdot x^{-(\\alpha + 1)}\n\\] mit \\(C^* = -C \\cdot \\alpha\\).\nEine (streng) potenzgesetzliche Verteilungen (engl. (strong) power-law probability distribution) zur ZV \\(X\\) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}_X(x)=P(X &gt; x)\\) die folgende Gestalt hat: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha}\\]\nMit der Dichte \\(f_X\\) ergibt sich: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha} =\\int_x^\\infty f_X(t) \\text{d}t = C^* \\cdot \\int_x^\\infty  t^{-(\\alpha+1)} \\text{d}t = C^* \\cdot \\int_x^\\infty t^{-\\alpha} \\text{d}t\\]\nAnstelle der Konstanten \\(C\\) tritt oft eine langsam variierende Funktion (engl. slowly varying funktion). Wir erhalten somit die folgende, allgemeinere Definition:\nEine potenzgesetzliche Verteilungen (engl. power-law probability distribution) (zu einer Zufallsvariable \\(X\\)) ist eine Verteilung deren Überlebensfunktion \\(\\overline{F}(x)=P(X &gt; x)\\) die folgende Gestalt hat:\n\\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha}\\]\nDabei ist \\(L(x):(x_{\\min}, +\\infty) \\to (x_{\\min}, +\\infty)\\) eine langsam variierende Funktion, also gilt für alle \\(t&gt;0\\):\n\\[\n\\lim_{x \\to +\\infty} \\frac{L(t \\cdot x)}{L(x)} = 1\n\\]\nIst nun wieder \\(f_X\\) die Dichte, so erhalten wir: \\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha} = \\int_x^\\infty f_X(t) dt\\]\n\\[f'(x) = [L(x)x^{-\\alpha}]'\\] \\[\\int_x^\\infty f_X(t) dt= \\int_x^\\infty [L(t)t^{-\\alpha}]' dt\\]\n\\(\\Delta x_0 = h = x_1 - x_0\\) \\(x_1 = x_0 + \\Delta x_0 = x_0 + h\\) \\(x_1 = c \\cdot x_0\\) \\(x_0 + h = c \\cdot x_0 &lt;=&gt; c = 1 + \\frac{h}{x_0}\\) \\(L(x_1) = L(x_0+h) = L(c \\cdot x_0)\\) \\(L(x_1) - L(x_0) = L(c \\cdot x_0) - L(x_0) = L(x_0 + h) - L(x_0)}\\) $\nFakten:\n\nSinnvoll nur, wenn \\(\\alpha &gt; 0\\).\nIst \\(\\alpha &lt; 3\\), dann ist die Varianz und die Schiefe (engl. skewness) (mathematisch) nicht definiert.\nFür \\(k &gt; \\alpha-1\\) ist das k. Moment unendlich.\n\nLogarithmiert man \\(y=f(x)=C \\cdot x^{-\\alpha}\\), so erhält mensch:\n\\[\\log(y) = \\log(C) -\\alpha \\cdot \\log(x)\\]\nIst eine Verteilung potenzgesetzlich, dann kann man \\(\\alpha\\), wie folgt abschätzen:\nSeien \\(x_0, x_1 &gt; x_{min}\\) zwei reelle Zahlen, \\(y_0=f(x_0)\\) bzw. \\(y_1 = f(y_1)\\).\nDann kann mensch wegen\n\\[\\begin{align*}\n  \\log(y_1) - \\log(y_0) &= \\log(C) - \\alpha \\cdot\\log(x_1) - \\log(C) + \\alpha \\cdot \\log(x_0) \\\\\n                        &= \\alpha \\cdot\\left(\\log(x_0)- \\log(x_1) \\right)\n\\end{align*}\\]\nden Wert für \\(\\alpha\\), so kann man mittels\n\\[\\alpha = \\frac{\\log(y_1) - \\log(y_0)}{\\log(x_0)- \\log(x_1)}\\]\nden Wert für \\(\\alpha\\) bestimmen.\nMit dem so ermittelten \\(\\alpha\\), können wir \\(C\\) wegen\n\\[\\log(C) = \\log(y)+ \\alpha\\log(x)\\]\nmit Hilfe von\n\\[C = y \\cdot x_0^\\alpha = f(x_0) \\cdot x_0^\\alpha\\]\nfür ein \\(x_0 &gt; x_{min}\\) abschätzen.\nBeispiel:\nNehmen wir die t-Verteilung mit \\(n=2\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{2}(x)\\).\nDann dann können wir \\(\\alpha=3\\) und \\(C=1\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nNoch ein Beispiel:\nNehmen wir die t-Verteilung mit \\(n=1\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{1}(x)\\).\nDann dann können wir \\(\\alpha=2\\) und \\(C=0.32\\) abschätzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nEin ’Gegen-’Beispiel:\nBetrachten wir nun die (rechte Seite – \\(x&gt;1=x_{min}\\)) einer Gauß’schen Standardnormalverteilung.\nMit den Stützstellen \\(x_0 = 1\\) und \\(x_1 = 5\\) können wir \\(\\alpha=7.46\\) und \\(C=0.24\\) abschätzen. Schauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"norm\",\n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWir erkennen, dass hier etwas nicht passt. Die Standardnormalverteilung ist (vielleicht) keine potenzgesetzich Verteilung?\nEin oft verwendetes Kriterium ist, dass sich die Funktion in der doppelt-logarithmischen Darstellung als Gerade offenbart.\nSchauen wir daher einmal nach:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\n** — **\nWeiter gilt für potenzgesetzliche Verteilungen wegen\n\\[\\frac{f(x)}{f(c\\cdot x)} = \\frac{C \\cdot x^{-\\alpha}}{C \\cdot (c\\cdot x)^{-\\alpha}}\n= c^\\alpha\\]\n\\(f(x)\\) und \\(f(c\\cdot x)\\) für alle (beliebig aber festen) \\(c&gt;0\\) proportional, was man gerne als \\(f(x) \\propto f(c \\cdot x)\\) schreibt.\n** — **\nDie Wahrscheinlichkeit für ein (mindestens) \\(8-\\sigma\\) Ereignis liegt bei einer Standardnormalverteilung bei etwa \\(6.66133814775094\\times 10^{-16}\\).\nBei einer t-Verteilung mit 2 Freiheitsgeraden bei etwa 0.00763403608266899$.\nWährend die Eintrittschance eines (mindestens) \\(8-\\sigma\\) Ereignisses bei der Standardnormalverteilung bei etwa \\(1 : round(1/(1-pnorm(8)),0)\\) liegt, ist diese der t-Verteilung mit einem Freiheitsgrad bei etwa $1 : 131"
  },
  {
    "objectID": "posts/2021-06-27-datenjudo-fuer-frageboegen/index.html",
    "href": "posts/2021-06-27-datenjudo-fuer-frageboegen/index.html",
    "title": "Datenjudo für Fragebögen",
    "section": "",
    "text": "Ab und zu bekomme ich die Frage, wie man einen Fragebogen mit Likert-Scalen-Items auswerten kann.\nDazu kann etwas gezieltes Datenjudo helfen. Wir schauen uns das folgende generierte Mini-Beispiel an:\n\nlibrary(mosaic)  # Basis Paket\nlibrary(tibble)  # Eine modernere Variante der data.frames!\nset.seed(2009)   # Reproduzierbarkeit\n\nN &lt;- 25  # Anzahl der Testzeileneinträge in den \"testdaten\"!\n\n# Wir wollen eine Likert-Scale \nminLikert &lt;- 1  # bis\nmaxLikert &lt;- 6  # erstellen.\n\n# Zum späteren Umrechnen der inversen Items:\nmaxInvItem &lt;- maxLikert + 1\n\n# Wir bauen uns eine Testumfrage mit zwei Itemserien \n# (AS1-AS6 und BS1-BS6) und N Beobachtungen.\n# Die Items AS3, AS4  und BS1 und BS5 sind dabei \n# inverse Items, welche später umgerechnet werden:\ntestdaten &lt;- tibble(\n    ID = 1:N,\n    # AS1-AS6 bilden ein Itemset:\n    AS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS6 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # BS1-BS5 bilden ein Itemset:\n    BS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # Geschlecht als sex mit (1 für Frauen und 2 für Männer)\n    sex = sample(1:2, N, replace = TRUE)\n)\n\n# Orinal testdaten einmal ausgeben:\nhead(testdaten)\n#&gt; # A tibble: 6 × 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     4     4     1     1     2     4     5     5     3     2     3     2\n#&gt; 2     2     2     1     2     2     5     6     4     5     2     2     6     1\n#&gt; 3     3     4     4     6     3     3     4     3     3     4     1     5     1\n#&gt; 4     4     2     6     1     4     5     4     6     4     5     1     3     1\n#&gt; 5     5     3     1     3     5     5     6     6     1     2     6     5     1\n#&gt; 6     6     6     4     1     3     6     6     4     6     5     3     3     1\n\nDie Spalten AS3, AS4 und BS1, BS5 waren inverse Items, die wir noch umrechnen müssen:\n\n# Inverse Item umrechnen:\ntestdaten |&gt;\n    mutate(\n        AS3 = maxInvItem - AS3,\n        AS4 = maxInvItem - AS4,\n        BS1 = maxInvItem - BS1,\n        BS5 = maxInvItem - BS5\n    ) -&gt; testdaten_korrigiert \n\n# Die Daten mit den umgerechnetern inversen Items:\nhead(testdaten_korrigiert)\n#&gt; # A tibble: 6 × 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     1     4     4     6     6     2     4     2     5     3     2     4     2\n#&gt; 2     2     2     1     5     5     5     6     3     5     2     2     1     1\n#&gt; 3     3     4     4     1     4     3     4     4     3     4     1     2     1\n#&gt; 4     4     2     6     6     3     5     4     1     4     5     1     4     1\n#&gt; 5     5     3     1     4     2     5     6     1     1     2     6     2     1\n#&gt; 6     6     6     4     6     4     6     6     3     6     5     3     4     1\n\nDie jeweiligen Itemsets werden nun zur einem Wert (Gesamtscore) zusammengefasst, in dem wir jeweils den Mittelwert von AS1-AS6 und BS1-BS5 bildenund in AS bzw. BS speichern:\n\n# Wir fassen nun die AS1-AS6 und die BS1-BS5 zusammen \n# und bilden die jeweiligen Mittelwerte:\ntestdaten_korrigiert |&gt;\n    group_by(ID, sex) |&gt;  # Damit wird für jede Zeile die Zusammenfassung gemacht!\n    summarise(\n        AS = mean(c(AS1, AS2, AS3, AS4, AS5, AS6)),\n        BS = mean(c(BS1, BS2, BS3, BS4, BS5))\n    ) -&gt; testdaten_sum\n\n# Ausgabe der Mittelwerte der AS und BS\nhead(testdaten_sum)\n#&gt; # A tibble: 6 × 4\n#&gt; # Groups:   ID [6]\n#&gt;      ID   sex    AS    BS\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2  4.33   3.2\n#&gt; 2     2     1  4      2.6\n#&gt; 3     3     1  3.33   2.8\n#&gt; 4     4     1  4.33   3  \n#&gt; 5     5     1  3.5    2.4\n#&gt; 6     6     1  5.33   4.2\n\nDie Datentabelle testdaten_sum enthält nun die Spalten AS und BS mit den entsprechenden Mittelwerten der einzelnen Items AS1-AS6 sowieso BS1- BS5.\nWir wollen nun die Ergebnisse als Boxplots anzeigen lassen. Dafür benennen wir die Geschlechter von 1,2 auf “Frau”, “Mann” um:\n\ntestdaten_sum |&gt;\n    mutate(sex = factor(sex, levels = c(1, 2),\n                             labels = c(\"Frau\", \"Mann\"))\n    ) -&gt; testdaten_sex \n\nNun können wir die Boxplots erstellen:\n\n# Darstellung der Ergebnisse als Boxplot AS ~ sex:\ngf_boxplot(AS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von AS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item AS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(2.5, 4.5)  # Gibt den Bereich von 2.5 bis 4.5 aus!\n    )  \n  )\n\n# Darstellung der Ergebnisse als Boxplot BS ~ sex:\ngf_boxplot(BS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von BS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item BS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(1, 6)  # Gibt den ganzen Bereich von 1 bis 6 aus!\n    )  \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Kennzahlen dazu erhalten wir mit favstats. Dabei wählen wir die ersten sechs Einträge (Variabelbezeichnung und Q0 bis Q4) aus:\n\nfavstats(AS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex      min       Q1   median       Q3      max\n#&gt; 1 Frau 2.166667 3.333333 3.666667 4.166667 5.333333\n#&gt; 2 Mann 3.666667 4.000000 4.333333 4.333333 4.500000\nfavstats(BS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex min  Q1 median  Q3 max\n#&gt; 1 Frau 2.4 2.8    3.4 4.2 4.6\n#&gt; 2 Mann 2.6 3.2    3.3 3.8 4.2\n\nUnter der Verwendung des Pakets likert (https://github.com/jbryer/likert) können wir die Ausgaben auch noch etwas schöner gestalten:\n\nlibrary(likert)\n\n# Wir wählen nur den Itemset BS aus und speichern in in items2:\ntestdaten_korrigiert |&gt;\n  select(\n    starts_with(\"BS\")\n  ) -&gt; items2\n\n# Leider mag likert tibbels nicht so gerne, daher:\nitems2 &lt;- as.data.frame(items2)\n\n# Wir geben den Items noch ein paar Buzzwords:\nnames(items2) &lt;- c(\"Gesundheit\", \"Familie\", \"Geld\", \"Freunde\", \"Langes Leben\")\n\n# Vorbereitung:\nl2 &lt;- likert(items2, nlevels = 5)\n\n# Zusammenfassung\nsummary(l2)\n#&gt;           Item      low   neutral     high     mean       sd\n#&gt; 3         Geld 28.57143 23.809524 47.61905 3.380952 1.359272\n#&gt; 5 Langes Leben 57.14286  4.761905 38.09524 2.619048 1.532194\n#&gt; 1   Gesundheit 38.09524 28.571429 33.33333 2.857143 1.492840\n#&gt; 2      Familie 38.09524 28.571429 33.33333 2.952381 1.499206\n#&gt; 4      Freunde 42.85714 23.809524 33.33333 2.857143 1.236354\n\n# Graphische Ausgaben:\nplot(l2)\n\nplot(l2,\"bar\")\n\nplot(l2,\"heat\")\n\nplot(l2,\"density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoilà!"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "",
    "text": "Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine möglichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt. Dabei tolerieren wir eine (kleine) Abweichung \\(e_i\\).\nBei einer einfachen linearen Regression gehen wir davon aus, dass die Datenpunkte (im wesentlichen) auf einer Geraden liegen. Mit \\(g(x)=\\beta_0 + \\beta1 \\cdot x\\) ergibt sich dann für die Datenpunkte die Gleichung:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i\\]\nUnsere Aufgabe besteht nun darin die Parameter \\(\\beta_0\\) (y-Achsenabschnitt) und \\(\\beta_1\\) (Steigung) an Hand der \\(n\\) Datenpunkte zu schätzen. Alle unsere Schätzungen kennzeichnen wir mit einem Dach (\\(\\hat{.}\\)), um sie von den (in der Regel unbekannten) Parametern besser zu unterscheiden.\nWir suchen somit nach \\(\\hat\\beta= \\left(\\hat\\beta_0,\\, \\hat\\beta_1\\right)\\), so dass die Gerade \\(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x\\) zu gegebenem \\(x_i\\) eine möglichst gute Schätzung von \\(y_i\\) (genannt \\(\\hat{y}_i\\)) hat:\n\\[\n\\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i\n\\]\nDie Abweichung \\(\\hat{e_i}\\) unserer Schätzung \\(\\hat{y}_i\\) von dem gegebenen Wert \\(y_i\\) lässt sich schreiben als:\n\\[\n\\hat{e_i} =  \\hat{y_i} - y_i =  \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i\n\\]\nWenn wir diese Abweichung über alle \\(i\\) minimieren, finden wir unser \\(\\hat\\beta\\).\nDoch das wirft eine Frage auf: Wie genau messen wir die möglichst kleinste Abweichung der \\(\\hat{e_i}\\) konkret?\nWir betrachten zunächst drei einfache Ideen:\nGewöhnlich nutzen wir die quadratischen Abweichungen, weshalb wir die drei Ideen ebenso in umgekehrter Reihenfolge betrachten wollen:"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "3. Idee: Summe der quadratischen Abweichungen",
    "text": "3. Idee: Summe der quadratischen Abweichungen\nWir bezeichnen mit\n\\[\\begin{aligned}\nQS &= QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n  &= \\sum\\limits_{i=1}^n \\hat{e_i}^2 = \\sum\\limits_{i=1}^n \\left(\\hat{y_i} - y_i \\right)^2 \\\\\n  &= \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\n\\end{aligned}\\]\ndie Quadrat-Summe der Abweichungen.\nGesucht wird \\(\\hat\\beta=\\left(\\hat\\beta_0,\\,\\hat\\beta_1\\right)\\), so das \\(QS\\) minimiert wird.\nDies ist ein Minimierungsproblem, bei dem wir zu mindestens eine (exakte) mathematisch-algebraisch Lösung in Form eines stationären Punktes finden können. Dazu berechnen wir die Nullstelle der ersten partiellen Ableitung von \\(QS\\) nach \\(\\hat\\beta_0\\) bzw. \\(\\hat\\beta_1\\).\n\nVorbemerkungen\nWegen \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i\\) ist \\(n \\cdot \\bar{x} =\\sum\\limits_{i=1}^n x_i\\) und analog \\(n \\cdot \\bar{y} =\\sum\\limits_{i=1}^n y_i\\)\n\n\nSchätzen des y-Achenabschnitts \\(\\hat\\beta_0\\)\nEs ist:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot 1 \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 + \\sum\\limits_{i=1}^n\\hat\\beta_1 \\cdot x_i - \\sum\\limits_{i=1}^n y_i\\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot\\sum\\limits_{i=1}^n x_i - \\sum\\limits_{i=1}^n y_i \\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot n \\cdot \\bar{x} - n \\cdot\\bar{y} \\right) \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right)\n\\end{aligned}\\]\nUm stationäre Punkte zu ermitteln, müssen wir den Ausdruck nun gleich Null setzen und erhalten:\n\\[\\begin{aligned}\n  0 &= \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\qquad | : (2 \\cdot n) \\\\\n  &= \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y}\n\\end{aligned}\\]\nStellen wir nach \\(\\hat\\beta_0\\) um, erhalten wir:\n\\[\\begin{aligned}\n  \\hat\\beta_0 &= - \\hat\\beta_1\\cdot\\bar{x} + \\bar{y} \\\\\n  \\hat\\beta_0 &= \\bar{y} - \\hat\\beta_1\\cdot\\bar{x}\n\\end{aligned}\\]\nUm \\(\\hat\\beta_0\\) zu bestimmen, benötigen wir \\(\\hat\\beta_1\\).\n\n\nSchätzen der Steigung \\(\\hat\\beta_1\\)\nEs ist:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot x_i \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 \\cdot x_i + \\sum\\limits_{i=1}^n \\hat\\beta_1 \\cdot x_i\\cdot x_i- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot \\sum\\limits_{i=1}^n  x_i + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right)\n\\end{aligned}\\]\nWir ersetzen nun \\(\\hat\\beta_0\\) durch \\(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\) und erhalten:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  &=\n  2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(\\left(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\right) \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - n \\cdot \\hat\\beta_1 \\cdot  \\bar{x}^2  + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n\\end{aligned}\\]\nMit Hilfe des Verschiebesatzes von Steiner (zweimal angewendet) erhalten wir:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  \n    &=2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)+ \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(\\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)\n\\end{aligned}\\]\nWir setzen nun wieder den Ausdruck gleich Null:\n\\[\\begin{aligned}\n0 &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)  \\qquad | : 2\\\\\n   &= \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{aligned}\\]\nUnd stellen dann nach \\(\\hat\\beta_1\\) um:\n\\[\\begin{aligned}\n  \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\n    &= \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\\\\n  \\hat\\beta_1\n    &= \\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\n\\end{aligned}\\]\nWir können nun Zähler und Nenner der rechten Seite mit \\(\\frac{1}{n}\\) erweitern und erhalten so:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\\\\n\\end{aligned}\\]\nOder aber wir erweitern mit \\(\\frac{1}{n-1}\\) und erhalten:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{s_{x,y}}{s^2_{x}}\n\\end{aligned}\\]\nDamit können wir zur Berechnung sowohl die Kovarianz der Grundgesamtheit \\(\\sigma_{x,y}\\) und die Varianz \\(\\sigma^2_x\\) von \\(x\\), als auch deren Schätzer \\(s_{x,y}\\) und \\(s^2_x\\) verwendet werden!\nDiese Methode nennt sich Methode der kleinsten Quadrate (engl. ordenary least square method) und wir sprechen dann auch von den Kleinste-Quadrate-Schätzern (oder kurz KQ-Schätzer bzw. OLS-Schätzer) \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\).\nErweitern wir den Ausdruck mit Standardabweichung \\(\\sigma_y\\) bzw. \\(s_y\\), so erhalten wir:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n&= \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\end{aligned}\\]\nund analog für die Schätzer:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{s_{x,y}}{s^2_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_y} \\cdot \\frac{s_y}{s_x} \\\\\n&= r_{x,y} \\cdot \\frac{s_y}{s_x} \\\\\n\\end{aligned}\\]\nDie Steigung \\(\\hat\\beta_1\\) hat somit eine direkte Beziehung mit dem Korrelationskoeffizenten \\(\\rho\\) (der Grundgesamtheit) bzw. \\(r\\) (der Stichprobe).\nFür eine Berechnung in R heißt dies: wir können die Regressionskoeffizienten \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\) direkt algebraisch ausrechnen, wenn wir\n\ndie Standardabweichungen von \\(x\\) und \\(y\\) und den Korrelationskoeffizienten oder\ndie Varianz von \\(x\\) und Kovarianz von \\(x\\) und \\(y\\)\n\nhaben.\n\n\nEin Beispiel in R:\nAuf Grundlage der Datentabelle mtcars wollen wir Prüfen wie ein linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdestärke hp) modelliert werden kann.1\n\nlibrary(mosaic)\n\n# Wir nehmen die Datentabelle 'mtcars':\nmtcars %&gt;%\n  select(hp, mpg) -&gt; dt\n\n# Ein kurzer Blick auf die Daten:\nfavstats(~ hp, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  146.6875 68.56287\nfavstats(~ mpg, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  20.09062 6.026948\n\n# Wir vergleichen den Verbrauch (mpg, miles per gallon) \n# mit den Pferdestärken (hp) mit Hilfe eines Streudiagramms:\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir zunächst die Mittelwerte von \\(x\\) (also ‘hp’) und \\(y\\) (also ‘mpg’)\n\n(mean_hp &lt;- mean(~ hp, data = dt))\n#&gt; [1] 146.6875\n(mean_mpg &lt;- mean(~ mpg, data = dt))\n#&gt; [1] 20.09062\n\nund zeichnen die Punkt \\((\\bar{x}, \\bar{y}) = (146.69, 20.09)\\) in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir nun die Schätzwerte für die Regressionsgerade\n\n(beta_1 &lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt))\n#&gt; [1] -0.06822828\n(beta_0 &lt;- mean_mpg - beta_1 * mean_hp)\n#&gt; [1] 30.09886\n\nund zeichnen diese in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \"dodgerblue\") %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988605 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nStudentisieren – einmal hin und einmal zurück\nWas passiert eigentlich, wenn wir unsere \\(x\\) und \\(y\\) Werte studentisieren (aka standardisieren oder z-transformieren)?\nZur Erinnerung, studentisieren geht so: \\[\nx^{stud} = \\frac{x - \\bar{x}}{s_x}\n\\]\nIn R können wir das mit der Funktion ‘zscore’ wie folgt machen:\n\ndt %&gt;%\n  mutate(\n    hp_stud = zscore(hp),\n    mpg_stud = zscore(mpg)\n  ) -&gt; dt\n\nNatürlich sind die Mittelwerte nun Null und die Standardabweichungen Eins:\n\nfavstats(~ hp_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;           mean sd\n#&gt;  -4.857226e-17  1\nfavstats(~ mpg_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;          mean sd\n#&gt;  4.336809e-17  1\n\nDer Grund für die kleinen Abweichungen von der Null bei den Mittelwerten sind unumgängliche Rundungsfehler, die der Computer macht!\nSchauen wir uns nun das Streudiagramm an, zusammen mit dem Mittelpunkt \\((0,0)\\)\n\ngf_point(mpg_stud ~ hp_stud, data = dt) %&gt;%\n  gf_point(0 ~ 0, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(-2, 2))\n\n\n\n\n\n\n\n\nAuch wenn die Skalierungen sich geändert haben, die Diagramme sind sehr ähnlich.\nBestimmen wir die Koeffizienten der Regressionsgerade\n\n(beta_stud_1 &lt;- cov(mpg_stud ~ hp_stud, data = dt))\n#&gt; [1] -0.7761684\n(beta_stud_0 &lt;- 0 - beta_stud_1 * 0)\n#&gt; [1] 0\n\nund setzen sie in das Streudiagramm ein:\n\n\n\n\n\n\n\n\n\nWir können das studentisierte Problem auch wieder auf unser ursprüngliches zurück rechnen.\nDie Regressionsgerade im studentisierten Problem lautet:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761684 \\cdot x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot x^{stud}\n\\end{aligned}\n\\]\nRechnen wir nun mittels der Formel \\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\]\ndie Steigung um, so erhalten wir:\n\n(b1 &lt;- beta_stud_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822828\n\nUnd setzen wir das in unsere Gleichung zur Bestimmung von \\(\\hat\\beta_0\\) ein:\n\n(b0 &lt;- mean(dt$mpg) - b1 * mean(dt$hp))\n#&gt; [1] 30.09886\n\nso erhalten wir die Schätzwerte des ursprünglichen Problem.\n\n\nEin anderer Weg um die Regressionskoeffizenten zu bestimmen…\nGehen wir das Problem noch einmal neu an. Wir suchen \\(\\hat\\beta=(\\hat\\beta_0, \\hat\\beta_1)\\) welches \\(QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\\) minimiert.\nStatt es direkt, wie oben durch Null setzen der partiellen Ableitungen, zu bestimmen, wählen wir nun einen mathematisch-numerischen Ansatz und wollen \\(\\hat\\beta \\in \\mathbf{R}^2\\) als Optimierungsproblem mit Hilfe des Gradientenverfahrens lösen.\nBeim Gradientenverfahren wird versucht, ausgehend von einem Startwert \\(\\hat\\beta^0 \\in \\mathbf{R}^2\\), gemäß der Iterationsvorschrift\n\\[\n\\hat\\beta^{k+1} = \\hat\\beta^{k} + \\alpha^k \\cdot d^k\n\\]\nfür alle \\(k=0,1, ...\\) eine Näherungslösung für \\(\\hat\\beta\\) zu finden. Dabei ist \\(\\alpha^k &gt; 0\\) eine positive Schrittweite und \\(d^k\\in\\mathbf{R}^n\\) eine Abstiegsrichtung, welche wir in jedem Iterationsschritt \\(k\\) so bestimmen, dass die Folge \\(\\hat\\beta^k\\) zu einem stationären Punkt, unserer Näherungslösung, konvergiert.\nIm einfachsten Fall, dem Verfahren des steilsten Abstieges, wird der Abstiegsvektor \\(d^k\\) aus dem Gradienten \\(\\nabla QS\\) wie folgt bestimmt:\n\\[\nd^k = -\\nabla QS\\left(\\hat\\beta^k\\right)\n\\]\nWegen \\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot n \\cdot \\left(  \\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y} \\right)\n\\]\nund\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS = 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\right)\n\\]\ngilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot(\\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y})  \\\\\n\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{pmatrix}\n\\end{aligned}\n\\]\nWir wollen hier von Anfang an mit den studentisierten Werten arbeiten, weil diese numerisch viele Vorteile haben. Darum vereinfachen sich die beiden partiellen Ableitungen noch einmal zu:\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot v\n\\]\nund\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\\\\n&= 2 \\cdot (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{aligned}\n\\]\nSomit gilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot \\hat\\beta_0 \\\\\n(n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{pmatrix}\n\\end{aligned}\n\\]\nUm die Varianz und die Kovarianz nicht jedesmal neu zu berechnen, speichern wir die Ergebnisse vorab. Ebenso, damit der Quellcode kürzer wird, speichern wir in \\(x\\) und \\(y\\) die studentisierten Werte von \\(hp\\) und \\(mpg\\):\n\n# Vorbereitungen \nvar_x &lt;- var(~ hp_stud, data = dt)\ncov_xy &lt;- cov(mpg_stud ~ hp_stud, data = dt)\n\nn &lt;- length(dt$hp_stud)\n\nx &lt;- dt$hp_stud\ny &lt;- dt$mpg_stud\n\nNun erstellen wir die \\(QS\\) und \\(\\nabla QS\\) Funktionen: Wir definieren diese Funktion wie folgt in R:\n\nqs &lt;- function(b_0, b_1) {\n  sum((b_1 * x - y)**2)\n}\n\nnabla_qs &lt;- function(b_0, b_1) {\n  c(2 * n * b_0,\n    2 * (n - 1) * (b_1 * var_x - cov_xy)\n  )\n}\n\nDie Schrittweite \\(alpha\\) bestimmen wir mit Hilfe der Armijo-Bedingung und der Backtracking Liniensuche: Diese formalisiert das Konzept “genügend” in der geforderten Verringerung des Funktionswertes. Die Bedingung \\(f(x^k + \\alpha d^k) &lt; f(x^k)\\) wird modifiziert zu \\[f(x^k + \\alpha d^k) \\leq f(x^k) + \\sigma \\alpha \\left(\\nabla f(x^k)\\right)^T d^k,\\] mit \\(\\sigma\\in (0,1)\\). Die Armijo-Bedingung umgeht Konvergenzprobleme der einfachen Bedingung, indem sie fordert, dass die Verringerung zumindest proportional zur Schrittweite und zur Richtungsableitung \\(\\left(\\nabla f(x^k)\\right)^T d^k\\) ist, mit Hilfe der Proportionalitätskonstante \\(\\sigma\\). In der Praxis werden oft sehr kleine Werte verwendet, z.B. \\(\\sigma=0.0001\\).\nDie Backtracking-Liniensuche verringert die Schrittweite wiederholt um den Faktor \\(\\rho\\) (rho) , bis die Armijo-Bedingung erfüllt ist. Sie terminiert garantiert nach einer endlichen Anzahl von Schritten. Weshalb wir sie hier einsetzen:\n\nalpha_k &lt;- function(b_0, b_1, d_k, alpha = 1, sigma = 0.0001, rho = 0.5) {\n  d_0 &lt;- d_k[1]\n  d_1 &lt;- d_k[2]\n  nabla &lt;- nabla_qs(b_0, b_1)\n  n_0 &lt;- nabla[1]\n  n_1 &lt;- nabla[2]\n\n  lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n  rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n\n  while (lhs &gt; rhs) {\n    alpha &lt;- rho * alpha\n    lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n    rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n  }\n  return(alpha)\n}\n\nEin paar Einstellungen vorab:\n\n# maximale Anzahl an Iterationen\nmax_iter &lt;- 1000\niter &lt;- 0\n\n# Genauigkeit\neps &lt;- 10**-6\n\n# Startwerte\nb_0 &lt;- 0 \nb_1 &lt;- -1 \n\nFür eine vorgegebene Genauigkeit \\(eps=10^{-6}\\), den Startwerten \\(\\hat\\beta_0^0 = 0\\) und \\(\\hat\\beta_1^0 = -1\\) können wir somit das Verfahren starten:\n\nwhile (TRUE) {\n  iter &lt;- iter + 1\n\n  d_k &lt;- -nabla_qs(b_0, b_1)\n\n  ad_ &lt;- alpha_k(b_0, b_1, d_k) * d_k\n\n  x0 &lt;- b_0 + ad_[1]\n  x1 &lt;- b_1 + ad_[2]\n\n  if ((abs(b_0 - x0) &lt; eps) & (abs(b_1 - x1) &lt; eps) | (iter &gt; max_iter)) {\n    break\n  }\n  b_0 &lt;- x0\n  b_1 &lt;- x1\n}\n\nWir haben in \\(203\\) Iterationsschritten das folgende Ergebnis für die Regressionskoeffizienten:\n\\[\n\\hat\\beta_0^{stud} = 0 \\qquad \\hat\\beta_1^{stud} = -0.7761689\n\\]\nBetrachten wir die daraus erstellte Regressionsgerade:\n\n\n\n\n\n\n\n\n\nUm die Regressionskoeffizienten für unser ursprüngliches Problem zu erhalten müssen wir wie folgt zurück rechnen:\n\n(b1 &lt;- b_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822832\n(b0 &lt;- mean(dt$mpg) -  b1 * mean(dt$hp))\n#&gt; [1] 30.09887\n\nDie Geradengleichung für das ursprüngliches Problem lautet somit:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988668 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nDie R Funktion optim\nIn R gibt es bessere Optimierungsmethoden, als die hier verwendete. Zum Beispiel können wir die Funktion optim verwenden. Die Funktion optim benötigt die zu optimierende \\(f(x)\\) und ggf. die Gradientenfunktion \\(gf(x)\\) sowie einen Startpunkt \\(x^0\\):\n\nf &lt;- function(beta) {\n  qs(beta[1], beta[2])\n}\n\ngrf &lt;- function(beta) {\n  nabla_qs(beta[1], beta[2])\n}\n\n# Der eigentliche Aufruf von optim:\nergb &lt;- optim(c(0,-0.5),f ,grf, method = \"CG\")\n\n# Auslesen der Schätzer aus dem Ergebnis:\n(optim_beta_0 &lt;- ergb$par[1])\n#&gt; [1] 0\n(optim_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.7761683\n\nWir erhalten somit für das studentisierte Problem die Gerade:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta_0^{stud} + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761683 \\cdot  x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot  x^{stud}\n\\end{aligned}\n\\]\nFür das ursprüngliche Problem rechnen wir mittels\n\noptim_b1 &lt;- optim_beta_1 * sd(dt$mpg) / sd(dt$hp)\noptim_b0 &lt;- mean(dt$mpg) -  optim_b1 * mean(dt$hp)\n\num und erhalten:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988601 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "2. Idee: Summe der absoluten Abweichungen",
    "text": "2. Idee: Summe der absoluten Abweichungen\nWir ändern nun die Abweichungsmessfunktion von der Quadrat-Summe hin zu den Absolut-Summen:\n\\[\nAS = AS(\\hat\\beta) = AS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n |\\hat{y}_i - y_i|\n\\]\nAuch hier wollen wir mit den studentisierten Daten arbeiten und stellen die Funktion der Absolut-Summen auf:\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  return(sum(abs(b_0 + b_1 * x - y)))\n}\n\nDanach konstruieren wir die zu optimierende Funktion \\(f\\):\n\n# Zu optimierende Funktion\nf &lt;- function(beta) {\n  as(beta[1], beta[2])\n}\n\nDiesmal nutzen wir optim ohne eine Gradientenfunktion:\n\nergb &lt;- optim(c(0,-1), f)\n\n# Schätzer auslesen\n(opti_as_beta_0 &lt;- ergb$par[1])\n#&gt; [1] -0.1304518\n(opti_as_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.6844911\n\nSchauen wir uns nun die so erhaltene Gerade im Vergleich mit der ‘normalen’ Regressionsgerade an:\n\n\n\n\n\n\n\n\n\nIn grün und gestrichelt sehen wir die Gerade aus der Idee der quadratischen Abweichungssummen, in blau die aus der Idee der absoluten Abweichungssummen.\nFür unser ursprüngliches Problem rechnen wir um:\n\n# Umrechnen in die ursprüngliche Fragestellung\n(as_b1 &lt;- opti_as_beta_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06016948\n(as_b0 &lt;- (mean(dt$mpg) - as_b1 * mean(dt$hp)) + opti_as_beta_0 * sd(dt$mpg))\n#&gt; [1] 28.13051\n\nUnd die dazu gehörige Darstellung:\n\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift für die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 28.1305094 -0.0601695 \\cdot x \\\\\n          &\\approx 28.131 -0.06 \\cdot x\n\\end{aligned}\n\\]\nDiese Methode nennt sich Median-Regression und ein ein Spezialfall der Quantilsregression, die sich u.a. mit dem R-Paket quantreg unmittelbar umsetzen lässt:\n\nlibrary(quantreg)\nergmedianreg &lt;- rq(mpg ~ hp, data = dt)\ncoef(ergmedianreg)\n#&gt; (Intercept)          hp \n#&gt; 28.13050847 -0.06016949"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "1. Idee: Betrag der Summe der Abweichungen",
    "text": "1. Idee: Betrag der Summe der Abweichungen\nWenn wir die Summe der Abweichungen \\(\\sum\\limits_{i=1}^n \\hat{e}_i\\) minimieren wollen, dann ist es sinnvoll den Betrag davon zu minimieren. Wir suchen also die Schätzer \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\), so dass der Ausdruck\n\\[\n\\left| \\sum_{i=1}^n \\hat{e}_i \\right| = \\left| \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \\right|\n\\]\nminimal ist.\nWegen:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i)\n&= \\sum_{i=1}^n \\hat\\beta_0 + \\sum_{i=1}^n \\hat\\beta_1 \\cdot x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot n \\cdot \\bar{x} - n \\cdot \\bar{y} \\\\\n&= n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\bar{x} - \\bar{y} \\right) \\\\\n&= n \\cdot \\left( \\hat\\beta_0 - \\bar{y} + \\hat\\beta_1 \\cdot \\bar{x}  \\right)\n\\end{aligned}\n\\]\nkönnen wir das absolute Minimum bei \\(\\hat\\beta_0 - \\bar{y} =0\\) und \\(\\hat\\beta_1 \\cdot \\bar{x}=0\\) erreichen, was zur Lösung \\(\\hat\\beta_0 =\\bar{y}\\) und \\(\\hat\\beta_1 = 0\\) führt. Dies ist unser Nullmodel in dem die \\(x_i\\) keinen Einfluss auf die \\(y_i\\) haben und wir daher pauschal die \\(y_i\\) mit \\(\\hat{y}_i=\\bar{y}\\), also dem Mittelwert der \\(y_i\\) abschätzen."
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nAls Vergleich können wir uns die Quadratsumme \\(QS\\) und Absolutsumme \\(AS\\) der drei Modelle einmal ansehen:\n\n# Quadratische Abweichungssummen\nqs &lt;- function(b_0, b_1) {\n  sum(((b_0 + b_1 * dt$hp) - dt$mpg )**2)\n}\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  sum(abs((b_0 + b_1 * dt$hp) - dt$mpg))\n}\n\n\n# Quadratsummen:\nquad_sum &lt;- c(qs(b0, b1), qs(as_b0, as_b1), qs(mean_mpg, 0))\n\n# Absolutsummen:\nabs_sum &lt;- c(as(b0, b1), as(as_b0, as_b1), as(mean_mpg, 0))\n\ntab &lt;- tibble(\n  sums = c(quad_sum, abs_sum),\n  sum_type = rep(c(\"quad\", \"abs\"), each = 3),\n  methode = rep(c(\"Idee 3\", \"Idee 2\", \"Idee 1\"), 2)\n)\n\npivot_wider(tab, names_from=sum_type, values_from=sums, names_sort=T)\n#&gt; # A tibble: 3 × 3\n#&gt;   methode   abs  quad\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Idee 3   93.0  448.\n#&gt; 2 Idee 2   87.3  477.\n#&gt; 3 Idee 1  151.  1126."
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 quantreg_6.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "title": "Über die Koeffizienten einer linearen Regression",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\nDas “Cookbook” zur Datentabelle können Sie mit Hilfe von help(\"mtcars\") aufrufen!↩︎"
  },
  {
    "objectID": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html",
    "href": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html",
    "title": "Behäbige Funktionen aka slowly varying function",
    "section": "",
    "text": "Reelle Funktionen, die ihren Funktionswert kaum ändern, kann man mit Fug und Recht durchaus behäbig nennen, korrekter wäre aber von langsam variierenden Funktionen zu sprechen\nIm Kontext von potenzgesetzlichen Verteilungen kommt der Begriff slowly varying function vor, der Funktionen beschreibt die nur sehr gering auf Änderungen ihres Parameters reagieren.\nDie Definition dieser behäbigen besser langsam variierenden Funktionen stammt von Jovan Karamata:\nEine positive stetige Funktion \\(L\\)1 auf den positiven reelen Zahlen ist langsam variierend (im unendlichen), falls für alle reellen \\(t&gt;0\\)\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = 1\\]\ngilt.\nBeispiele:\nBeweisskizze: Mit \\(L(x) = c\\) ist \\(L(x) = L(t x) = c\\) und damit \\(\\frac{L(t x)}{L(x)}= 1\\).\nBeweisskizze: Da \\(\\lim\\limits_{x \\to +\\infty} L(x) = b = \\lim\\limits_{x \\to +\\infty} L(t\\cdot x)\\) ist \\(\\lim\\limits_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = \\frac{\\lim\\limits_{x \\to +\\infty} L(t\\cdot x)}{\\lim\\limits_{x \\to +\\infty} L(x)} = \\frac{b}{b} = 1\\)\nBeweisskizze: Es gilt: - Für jede reelle Zahl \\(x&gt;0\\) ist \\(\\log_x(x) = 1\\). - Für reelle Zahlen \\(a, b\\) gilt: \\(\\frac{log(a)}{\\log(b)} = \\log_b(a)\\) - Für reelle Zahlen \\(a, b\\) gilt. \\(\\log(a \\cdot b) = log(a) + log(b)\\) - Für jede Konstante \\(k\\) gilt \\(\\lim_{x \\to +\\infty} \\log_x (k) = 0\\) Somit gilt \\(\\frac{\\log_\\beta(k \\cdot x)}{\\log_\\beta(x)} = \\log_x(k\\cdot x) = \\log_x(k) + \\log_x(x) = \\log_x(k) +1 \\to 1\\) wenn \\(x \\to +\\infty\\)\nEine regulär variierende Funktion \\(L:(0,+\\infty) \\to (0,+\\infty)\\) ist eine Funktion für die der Term\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = g(t)\\]\nmit \\(g(t)\\) für alle \\(t&gt;0\\) einen endlichen aber nicht verschwindenen Wert (m.a.W.: \\(g(t) \\neq 0\\)) hat .\nKaramata hat die regulär variierenden Funktionen nun wie folgt charaterisiert:"
  },
  {
    "objectID": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "href": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "title": "Behäbige Funktionen aka slowly varying function",
    "section": "Fußnoten",
    "text": "Fußnoten\n\n\n\\(L\\) oder auch \\(l\\) wird (angeblich) hier für den Begriff lente (serb. für faul) verwendet. Behäbig ist also doch nicht so falsch. ;-)↩︎"
  }
]