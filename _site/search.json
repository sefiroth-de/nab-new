[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "B-B√§ume und mehr: Ein Vergleich von Python-Implementierungen\n\n\n\n\n\n\n\n\nJan 4, 2026\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nDas Lotto-Abbildungsproblem\n\n\n\n\n\n\n\n\nDec 22, 2025\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nQuarto-Dokumente mit WebP anstatt PNG Bilddateien erstellen\n\n\n\n\n\n\n\n\nDec 3, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nBrotli statt Gzip auf dem Server konfigurieren\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nWie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nBeweisverfahren in der Mathematik\n\n\n\n\n\n\n\n\nJul 27, 2025\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nWillkommen in meinem Blog\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nGrafiken nebeneinander setzen mit ggplot2 oder ggformula\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nCSV Dateien bearbeiten mit Miller\n\n\n\n\n\n\n\n\nAug 27, 2021\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nDatenjudo f√ºr Frageb√∂gen\n\n\n\n\n\n\n\n\nJun 27, 2021\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nDinge die man in zwei Dimensionen machen kann - Multiple lineare Regression\n\n\n\n\n\n\n\n\nJun 24, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nRegression mit studentisierten Daten\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nInteraktionseffekte leichter interpretieren durch Transformationen\n\n\n\n\n\n\n\n\nJun 23, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nWege zur Normalverteilung\n\n\n\n\n\n\n\n\nJun 11, 2021\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n√úber die Koeffizienten einer linearen Regression\n\n\n\n\n\n\n\n\nJun 9, 2021\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nGraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest\n\n\n\n\n\n\n\n\nApr 8, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden\n\n\n\n\n\n\n\n\nFeb 17, 2021\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nBeh√§bige Funktionen aka slowly varying function\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nEin paar Gedanken √ºber potenzgesetzliche Verteilungen (power law distributions)\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nGedankenst√ºtze zu wichtigen Funktionsbegriffen in der Statistik\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nCook Abstand\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nDer Zentrale Grenzwertsatz\n\n\n\n\n\n\n\n\nApr 5, 2017\n\n4 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projekte.html",
    "href": "projekte.html",
    "title": "Projekte",
    "section": "",
    "text": "Workshop zum Thema quarto\n\n\n\n\n\n\nworkshop\n\nquarto\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nDozenten-Workshop: Methoden der quantitativen Forschung\n\n\nEinstieg-in-die-Datenanalyse-mit-R\n\n\n\nworkshop\n\nR\n\nStatistik\n\n\n\n\n\n\n\n\n\nSep 27, 2021\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nFUMS - Fresh Up your Maths Skills\n\n\n\n\n\n\nVorlesungsskript\n\nMathematik\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nMathematische Grundlagen der Informatik\n\n\n\n\n\n\nVorlesungsskript\n\nMathematik\n\nInformatik\n\n\n\n\n\n\n\n\n\nOct 18, 2019\n\n\nNorman Markgraf\n\n\n\n\n\n\n\n\n\n\n\n\nNPBT aka Norman‚Äôs Pandoc Beamer Themes\n\n\n\n\n\n\nVorlesungsskript\n\nBeamer\n\nTheme\n\npandoc\n\n\n\n\n\n\n\n\n\nFeb 6, 2017\n\n\nNorman Markgraf\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Norman Markgraf",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     X\n  \n  \n    \n     Buy me a coffee\n  \n  \n    \n     ko-fi\n  \n\n  \n  \n\n\nNorman Markgraf ist freiberuflicher Dozent f√ºr Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer.\nAls Lehrbeauftragter hat er an der Hochschule Bochum (am Campus Bochum), der Hochschule Rhein-Waal (am Campus Kleve), der FOM Hochschule f√ºr Oekonomie und Management an den Studienorten Aachen, Bonn, Dortmund, Duisburg, D√ºsseldorf, G√ºtersloh (Bertelsmann), K√∂ln, M√ºnster, Neuss, Wuppertal und M√ºnchen, der Frankfurt University of Applied Sciences (im Online-Studium) und der IU Internationale Hochschule an den Studienorten D√ºsseldorf und K√∂ln verschiedene Lehrveranstaltungen, Mentorate und Tutorien in den Bereichen Wirtschafts-/ Finanz- und Ingenieurmathematik, Progammieren in Python, Grundlagen der Informatik, Formale Beschreibungsverfahren, Big Data, Datenmodellierung / Datenbanken (inkl. NO SQL), Statistik, Quantiative (Forschungs-)Methoden und Qualitative Methoden abgehalten."
  },
  {
    "objectID": "index.html#biografie",
    "href": "index.html#biografie",
    "title": "Norman Markgraf",
    "section": "",
    "text": "Norman Markgraf ist freiberuflicher Dozent f√ºr Mathematik, Statistik, Data Science und Informatik sowie freiberuflicher Programmierer.\nAls Lehrbeauftragter hat er an der Hochschule Bochum (am Campus Bochum), der Hochschule Rhein-Waal (am Campus Kleve), der FOM Hochschule f√ºr Oekonomie und Management an den Studienorten Aachen, Bonn, Dortmund, Duisburg, D√ºsseldorf, G√ºtersloh (Bertelsmann), K√∂ln, M√ºnster, Neuss, Wuppertal und M√ºnchen, der Frankfurt University of Applied Sciences (im Online-Studium) und der IU Internationale Hochschule an den Studienorten D√ºsseldorf und K√∂ln verschiedene Lehrveranstaltungen, Mentorate und Tutorien in den Bereichen Wirtschafts-/ Finanz- und Ingenieurmathematik, Progammieren in Python, Grundlagen der Informatik, Formale Beschreibungsverfahren, Big Data, Datenmodellierung / Datenbanken (inkl. NO SQL), Statistik, Quantiative (Forschungs-)Methoden und Qualitative Methoden abgehalten."
  },
  {
    "objectID": "index.html#interessen",
    "href": "index.html#interessen",
    "title": "Norman Markgraf",
    "section": "Interessen",
    "text": "Interessen\n\nMathematik\n\nFinanzmathematik\nWirtschaftsmathematik\nIngenieurmathematik\n\nStatistik\n\nDatenanalyse\nData Literacy\nDatenkompetenz\nData Science\nR\n\nInformatik\n\nDatenbanken (SQL und NoSQL)\nBig Data\nPython\nMicroPython"
  },
  {
    "objectID": "index.html#ausbildung",
    "href": "index.html#ausbildung",
    "title": "Norman Markgraf",
    "section": "Ausbildung",
    "text": "Ausbildung\n\nWissenschaftlicher Mitarbeiter am Lehrstuhl f√ºr Prozessinformatik der Fakult√§t f√ºr Elektro- und Informationstechnik, 2006 Ruhr-Universit√§t Bochum\nDiplom-Mathematiker mit dem Schwerpunkt Informatik und dem Nebenfach Wirtschaftsinformatik, 1992 Ruhr-Universit√§t Bochum"
  },
  {
    "objectID": "index.html#impressum",
    "href": "index.html#impressum",
    "title": "Norman Markgraf",
    "section": "Impressum",
    "text": "Impressum\n\nAngabe gem√§√ü ¬ß5 TMG:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\n\n\nHaftungsausschluss - Disclaimer\n\nHaftung f√ºr Inhalte\nAlle Inhalte unseres Internetauftritts wurden mit gr√∂√üter Sorgfalt und nach bestem Gewissen erstellt. F√ºr die Richtigkeit, Vollst√§ndigkeit und Aktualit√§t der Inhalte k√∂nnen wir jedoch keine Gew√§hr √ºbernehmen. Als Diensteanbieter sind wir gem√§√ü ¬ß 7 Abs.1 TMG f√ºr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach ¬ß¬ß 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, √ºbermittelte oder gespeicherte fremde Informationen zu √ºberwachen oder nach Umst√§nden zu forschen, die auf eine rechtswidrige T√§tigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber√ºhrt.\nEine diesbez√ºgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung m√∂glich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverz√ºglich entfernen.\n\n\nHaftungsbeschr√§nkung f√ºr externe Links\nUnsere Webseite enth√§lt Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher k√∂nnen wir f√ºr die ‚Äûexternen Links‚Äú auch keine Gew√§hr auf Richtigkeit der Inhalte √ºbernehmen. F√ºr die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverst√∂√üe √ºberpr√ºft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine st√§ndige inhaltliche √úberpr√ºfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht m√∂glich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die au√üerhalb unseres Verantwortungsbereichs liegen, w√ºrde eine Haftungsverpflichtung ausschlie√ülich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch m√∂glich und zumutbar w√§re, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\nDiese Haftungsausschlusserkl√§rung gilt auch innerhalb des eigenen Internetauftrittes ‚ÄûNorman‚Äôs Academic Blog‚Äú gesetzten Links und Verweise von Fragestellern, Blogeintr√§gern, G√§sten des Diskussionsforums. F√ºr illegale, fehlerhafte oder unvollst√§ndige Inhalte und insbesondere f√ºr Sch√§den, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der √ºber Links auf die jeweilige Ver√∂ffentlichung lediglich verweist.\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverz√ºglich entfernt.\n\n\nUrheberrecht\nDie auf unserer Webseite ver√∂ffentlichen Inhalte und Werke unterliegen dem deutschen Urheberrecht (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external)) . Die Vervielf√§ltigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers au√üerhalb der Grenzen des Urheberrechtes bed√ºrfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes (http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf (link is external) ). Downloads und Kopien dieser Seite sind nur f√ºr den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverz√ºglich entfernen.\nDieses Impressum wurde freundlicherweise von (www.jurarat.de) zur Verf√ºgung gestellt."
  },
  {
    "objectID": "projekte/NPBT/index.html",
    "href": "projekte/NPBT/index.html",
    "title": "NPBT aka Norman‚Äôs Pandoc Beamer Themes",
    "section": "",
    "text": "Norman‚Äôs Pandoc Beamer Themes ist eine Sammlung von Themes um schnell das Design f√ºr verschiedene Anl√§sse relativ schnell zu √§ndern."
  },
  {
    "objectID": "projekte/NPBT/index.html#npbt-aka-normans-pandoc-beamer-themes",
    "href": "projekte/NPBT/index.html#npbt-aka-normans-pandoc-beamer-themes",
    "title": "NPBT aka Norman‚Äôs Pandoc Beamer Themes",
    "section": "",
    "text": "Norman‚Äôs Pandoc Beamer Themes ist eine Sammlung von Themes um schnell das Design f√ºr verschiedene Anl√§sse relativ schnell zu √§ndern."
  },
  {
    "objectID": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html",
    "href": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html",
    "title": "Dozenten-Workshop: Methoden der quantitativen Forschung",
    "section": "",
    "text": "Einstieg in die Datenanalyse mit R ist ein Workshop den ich regelm√§ssig f√ºr die FOM und ihre Dozent:innen gehalten habe.\nEr stellt einen Einstieg in R und RStudio-Desktop dar und fokusiert dann auf Simulationsbasiserte Inferenz (SBI)."
  },
  {
    "objectID": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html#dozenten-workshop-methiden-der-quantitativen-forschung",
    "href": "projekte/DozWS-MqF-Einstieg-in-die-Datenanalyse-mit-R/index.html#dozenten-workshop-methiden-der-quantitativen-forschung",
    "title": "Dozenten-Workshop: Methoden der quantitativen Forschung",
    "section": "",
    "text": "Einstieg in die Datenanalyse mit R ist ein Workshop den ich regelm√§ssig f√ºr die FOM und ihre Dozent:innen gehalten habe.\nEr stellt einen Einstieg in R und RStudio-Desktop dar und fokusiert dann auf Simulationsbasiserte Inferenz (SBI)."
  },
  {
    "objectID": "projekte/MathematischeGrundelagenDerInformatik/index.html",
    "href": "projekte/MathematischeGrundelagenDerInformatik/index.html",
    "title": "Mathematische Grundlagen der Informatik",
    "section": "",
    "text": "Die ist das Skript zu den von mir an der FOM gehaltenen Vorlesungen im Fach ‚ÄúMathematische Grundlagen der Informatik‚Äù.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdr√ºckliches und schriftliches Einverst√§ndnis benutzt werden.\nF√ºr alle anderen gilt:\n¬© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "projekte/MathematischeGrundelagenDerInformatik/index.html#vorlesungsskript-mathematische-grundlagen-der-informatik",
    "href": "projekte/MathematischeGrundelagenDerInformatik/index.html#vorlesungsskript-mathematische-grundlagen-der-informatik",
    "title": "Mathematische Grundlagen der Informatik",
    "section": "",
    "text": "Die ist das Skript zu den von mir an der FOM gehaltenen Vorlesungen im Fach ‚ÄúMathematische Grundlagen der Informatik‚Äù.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdr√ºckliches und schriftliches Einverst√§ndnis benutzt werden.\nF√ºr alle anderen gilt:\n¬© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "publications/STATISTIK21/index.html",
    "href": "publications/STATISTIK21/index.html",
    "title": "Statistik21",
    "section": "",
    "text": "Statistik21 ist ein Projekt der FOM mit dem Ziel, die Passgenauigkeit der Statistik-Lehre an der FOM in Bezug auf die notwendigen Methodenkennnisse von Studierenden didaktisch und konzeptionell kontinuierlich weiter zu verbessern."
  },
  {
    "objectID": "publications/STATISTIK21/index.html#zusammenfassung",
    "href": "publications/STATISTIK21/index.html#zusammenfassung",
    "title": "Statistik21",
    "section": "",
    "text": "Statistik21 ist ein Projekt der FOM mit dem Ziel, die Passgenauigkeit der Statistik-Lehre an der FOM in Bezug auf die notwendigen Methodenkennnisse von Studierenden didaktisch und konzeptionell kontinuierlich weiter zu verbessern."
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "",
    "text": "Wir wollen den Fall unterschen bei dem wir mit zwei statistischen Variabeln (\\(X\\) und \\(Y\\)) eine dritte Variable (\\(Z\\)) mittels einer multiplen linearen Regression modellieren.\nEs seien die Datenpunkte \\((x_1, y_1, z_1), \\dots, (x_n, y_n, z_n)\\) gegeben und wir wollen eine lineare Funktion \\(g(x,y)\\) finden, so dass\n\\[\nz_i = g(x_i,y_i)+ \\epsilon_i =\\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot y_i + \\epsilon_i\n\\]\ngilt und der Abweichungsterm \\(\\epsilon_i\\) m√∂glichst klein ist.\nAuf Grundlage unserer Datenpunkt wollen wir die Koeffizienten so sch√§tzen, dass die Summe der quadratische Abweichungen minimal ist. \\[\n  QS = QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\n  = \\sum\\limits_{i=1}^n (z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i )^2\n\\]\nDas f√ºhrt zu der folgenden, notwendigen Bedingen (f√ºr station√§re Punkte):\n\\[\n\\nabla QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\end{pmatrix}\n\\]\nIm einzelnen hei√üt das:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot\\sum\\limits_{i=1}^n \\left(z_i - \\hat\\beta_0 - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i \\right) \\\\\n&= -2 \\cdot n \\cdot \\left(\\bar{z} - \\hat\\beta_0 - \\hat\\beta_1 \\cdot\\bar{x} - \\hat\\beta_2 \\cdot\\bar{y} \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot x_i - \\hat\\beta_0 \\cdot x_i - \\hat\\beta_1 \\cdot x_i\\cdot x_i - \\hat\\beta_2 \\cdot y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) &= -2 \\cdot \\sum\\limits_{i=1}^n \\left( z_i\\cdot y_i - \\hat\\beta_0 \\cdot y_i - \\hat\\beta_1 \\cdot x_i\\cdot y_i - \\hat\\beta_2 \\cdot y_i\\cdot y_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i -  \\hat\\beta_0 \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die 1. Gleichung gleich Null und stellen nach \\(\\hat\\beta_0\\) um:\n\\[\n  \\hat\\beta_0 = \\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}\n\\] Nun ersetzen wir \\(\\hat\\beta_0\\) in den verbleibenden Gleichungen durch \\(z_i - \\hat\\beta_1 \\cdot x_i - \\hat\\beta_2 \\cdot y_i\\) und nutzen den Verschiebesatz:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot x_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{x}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i\\cdot x_i \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (x_i -\\bar{x})^2 - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\\\\n\\frac{\\partial}{\\partial \\hat\\beta_2} QS\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n  z_i\\cdot y_i - (\\bar{z}  - \\hat\\beta_1 \\cdot \\bar{x} - \\hat\\beta_2 \\cdot \\bar{y}) \\cdot n \\cdot \\bar{y}- \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n x_i\\cdot y_i - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n y_i^2 \\right) \\\\\n&= -2 \\cdot \\left(\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i -\\bar{y})^2 - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x}) \\right) \\\\\n\\end{aligned}\n\\]\nWir setzen die beiden Gleichungen nun gleich Null und formen nach \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\) um:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2}\n\\end{aligned}\n\\]\nDurch Erweiterung von Z√§hler nun Nenner mit \\(\\frac{1}{n-1}\\) erhalten wir:\n\\[\n\\begin{aligned}\n  \\hat\\beta_1\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(x_i - \\bar{x}) - \\hat\\beta_2 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i- \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (x_i -\\bar{x})^2} \\\\\n  &= \\frac{s_{x,z}-\\hat\\beta_2\\cdot s_{x,y}}{s^2_{x}} = \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  \\\\\n  \\hat\\beta_2\n  &= \\frac{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (z_i-\\bar{z})(y_i - \\bar{y}) - \\hat\\beta_1 \\cdot \\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i-\\bar{y})(x_i - \\bar{x})}{\\frac{1}{n-1}\\cdot\\sum\\limits_{i=1}^n (y_i -\\bar{y})^2} \\\\\n  &= \\frac{s_{y,z}-\\hat\\beta_1\\cdot s_{x,y}}{s^2_{y}} = \\frac{s_{y,z}}{s^2_y}-\\hat\\beta_1 \\frac{s_{x,y}}{s^2_{y}} \\\\\n\\end{aligned}\n\\]\nwir setzen nun die erste in die zweite Gleichung ein und erhalten:\n\\[\n\\begin{aligned}\n\\hat\\beta_2\n  &= \\frac{s_{y,z}}{s^2_y} - \\left(\\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\right) \\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}} + \\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}} \\\\\n  &= \\frac{\\frac{s_{y,z}}{s^2_y} - \\frac{s_{x,z}}{s^2_x}\\frac{s_{x,y}}{s^2_{y}}}{1-\\frac{s_{x,y}}{s^2_{x}}\\frac{s_{x,y}}{s^2_{y}}} \\\\\n  &= \\frac{\\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x\\cdot s^2_y}}{\\frac{s^2_x s^2_y-(s_{x,y})^2}{s^2_x \\cdot s^2_y}}\n  = \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2}\n\\end{aligned}\n\\]\nUnd damit weiter:\n\\[\n\\begin{aligned}\n\\hat\\beta_1  \n  &= \\frac{s_{x,z}}{s^2_x}-\\hat\\beta_2 \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z}}{s^2_x} - \\frac{s_{y,z}\\cdot s^2_x - s_{x,z} s_{x,y}}{s^2_x s^2_y-(s_{x,y})^2} \\frac{s_{x,y}}{s^2_{x}} \\\\\n  &= \\frac{s_{x,z} (s^2_x s^2_y - (s_{x,y})^2) - s_{y,z}s_{x,y}s^2_x + s_{x,z}s_{x,y}s_{x,y}}{s^2_x (s^2_x s^2_y - (s_{x,y})^2)} \\\\\n    &= \\frac{s_{x,z}s^2_x s^2_y - s_{x,z}(s_{x,y})^2 - s_{y,z}s_{x,y}s^2_x + s_{x,z}(s_{x,y})^2}{s^2_x s^2_x s^2_y- s^2_x(s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nlibrary(mosaic)\n\nmtcars %&gt;%\n  select(mpg, hp, wt) -&gt; dt\n\n# Von R berechnete Koeffizienten:\ncoef(lm(mpg ~ hp + wt, data = dt))\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074\n\nmean_x = mean( ~ hp, data = dt)\nmean_y = mean( ~ wt, data = dt)\nmean_z = mean( ~ mpg, data = dt)\n\ns_xy &lt;- cov(hp ~ wt, data = dt)\ns_xz &lt;- cov(hp ~ mpg, data = dt)\ns_yz &lt;- cov(wt ~ mpg, data = dt)\n\nvar_x &lt;- var(~ hp, data = dt)\nvar_y &lt;- var(~ wt, data = dt)\nb1z &lt;- s_xz*var_x*var_y - s_xz*(s_xy)**2 - s_yz*s_xy*var_x + s_xz*s_xy**2\nb1n &lt;- var_x*var_x*var_y - var_x*s_xy**2\nb1 &lt;- b1z / b1n\nb2 &lt;- (s_yz*var_x - s_xz*s_xy) / (var_x * var_y - s_xy*s_xy)\nb0 &lt;- mean_z - b1 * mean_x - b2 * mean_y\n\n# Koeffizienten zur Ausgabe aufbereiten:\nmy_coef &lt;- c(b0, b1, b2)\nnames(my_coef) &lt;- c(\"(Intercept)\", \"hp\", \"wt\")\n\n# Von Hand berechnete Koeffizienten:\nmy_coef\n#&gt; (Intercept)          hp          wt \n#&gt; 37.22727012 -0.03177295 -3.87783074"
  },
  {
    "objectID": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "href": "posts/2021-06-24-dinge-die-man-in-zwei-dimensionen-machen-kann-multiple-lineare-regression/index.html#was-passiert-wenn-wir-alle-datenpunkte-studentisieren",
    "title": "Dinge die man in zwei Dimensionen machen kann - Multiple lineare Regression",
    "section": "Was passiert, wenn wir alle Datenpunkte studentisieren?",
    "text": "Was passiert, wenn wir alle Datenpunkte studentisieren?\nWir rechnen um in:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x}; \\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}; \\quad z_i^{\\text{stud}} = \\frac{z_i-\\bar{z}}{s_z}\n\\]\nDamit ist\n\\[\n\\bar{x_i}^\\text{stud} = 0; \\quad \\bar{y_i}^\\text{stud} = 0;\\quad \\bar{z_i}^\\text{stud} = 0\n\\] und\n\\[\ns_{{x_i}^\\text{stud}} = 1; \\quad s_{{y_i}^\\text{stud}} = 1;\\quad s_{{z_i}^\\text{stud}} = 1\n\\]\nZur Vereinfachung lassen wir die Kennzeichnung ‚Äústud‚Äù weg. Damit ist dann:\n\\[\n\\begin{aligned}\n\\hat\\beta_0\n  &= 0\\\\\n\\\\\n\\hat\\beta_1\n  &= \\frac{s_{x,z} \\cdot s^2_x \\cdot s^2_y - s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y}s^2_x + s_{x,z} \\cdot (s_{x,y})^2}{s^2_x \\cdot s^2_x s^2_y- s^2_x \\cdot (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} \\cdot 1 \\cdot 1 -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} \\cdot 1 + s_{x,z} \\cdot (s_{x,y})^2}{1 \\cdot 1 \\cdot 1 - 1 \\cdot (s_{x,y})^2}\\\\\n  &= \\frac{s_{x,z} -  s_{x,z} \\cdot (s_{x,y})^2 - s_{y,z} \\cdot s_{x,y} + s_{x,z} \\cdot (s_{x,y})^2}{1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{x,z} - s_{y,z} \\cdot s_{x,y} }{1 - (s_{x,y})^2} \\\\\n\\\\\n\\hat\\beta_2\n  &= \\frac{s_{y,z} \\cdot s^2_x - s_{x,z} \\cdot s_{x,y}}{s^2_x \\cdot s^2_y - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} \\cdot 1 - s_{x,z} \\cdot s_{x,y}}{1 \\cdot 1 - (s_{x,y})^2} \\\\\n  &= \\frac{s_{y,z} - s_{x,z} \\cdot s_{x,y}}{1 - (s_{x,y})^2} \\\\\n\\end{aligned}\n\\]\nWir schauen uns ein paar F√§lle genauer an:\n\nFall: \\(X\\) und \\(Y\\) sind unabh√§ngig. Dann ist \\(s_{x,y}=0\\) und wir erhalten \\(\\hat\\beta_1=s_{x,z}\\in[-1;1]\\) und \\(\\hat\\beta_2=s_{y,z}\\in[-1;1]\\).\nFall: \\(X\\) und \\(Y\\) sind abh√§ngig. Dann ist \\(|s_{x,y}|=1\\) und es gibt keine L√∂sung f√ºr \\(\\hat\\beta_1\\) und \\(\\hat\\beta_2\\).\nFall: \\(0 &lt; |s_{x,y}| &lt; 1\\). ‚Ä¶"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "",
    "text": "Bei einer multiplen linearen Regression kann man den Einfluss einer unabh√§gigen Variable auf das Verhalten einer anderen unabh√§gigen Variable in Bezug auf die abh√§gige Variable mit modellieren.\nWir wollen das einmal an dem Beispiel der folgenden Datentabelle Impact of Beauty on Instructor‚Äôs Teaching Ratings und der Fragestellung in wie weit das Alter und das Geschlecht einen Einfluss auf das Evaluationsergebnis haben.\nDazu stellen laden wir die Daten aus dem Internet:\n\nlibrary(mosaic)\nurl &lt;- paste0(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/\",\n              \"TeachingRatings.csv\")\nteacherratings &lt;- read.csv(url)\n\nund betrachten das Streudiagramm:\n\ngf_point(eval ~ age, color = ~gender, data = teacherratings)"
  },
  {
    "objectID": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "href": "posts/2021-06-23-interaktionseffekte-leichter-interpretieren-durch-transformationen/index.html#nachtrag-und-danksagung",
    "title": "Interaktionseffekte leichter interpretieren durch Transformationen",
    "section": "Nachtrag und Danksagung",
    "text": "Nachtrag und Danksagung\nDie Idee zu diesem Blog-Post verdanke ich dem Blog von Prof.¬†Dr.¬†Sebastian Sauer. Hier der Link zum Orginal-Blog: https://data-se.netlify.app/2021/06/17/beispiel-zur-interpretation-des-interaktionseffekts/\nDanke auch f√ºr die kritische Durchsicht und die hilfreichen Anmerkungen."
  },
  {
    "objectID": "posts/2025-07-27-Beweisverfahren/index.html",
    "href": "posts/2025-07-27-Beweisverfahren/index.html",
    "title": "Beweisverfahren in der Mathematik",
    "section": "",
    "text": "Trivial!\n\n\n\nDer Satz folgt sofort aus dem Fakt\n\\[\n  \\left|\n    \\oplus_{k \\in S}\n      \\left(\n       \\mathfrak{K}^{\\mathbb{F}^\\alpha(i)}  \n      \\right)_{i \\in \\mathcal{U}_k}\n  \\right|\n  \\preceq \\aleph_1\n  \\text{ wenn }\n  \\left[\n    \\mathfrak{H}_\\mathcal{W}\n  \\right]     \n  \\cap \\mathbb{F}^\\alpha(\\mathbb{N}) \\neq \\emptyset.\n\\]\n\n\n\nDer Satz ist ein einfaches Korrollat eines Ergebnis bewiesen in einer handgeschriebenen Anmerkung ausgeh√§ndigt in einer Vorlesung bei der Jugoslawischen Mathematischen Gesellschaft 1973.\n\n\n\nDer Beweis kann auf Seite 478 gefunden werden, in einem Buch, welches nur 396 Seiten hat.\n\n\n\nProposition 5.18 in [BL] ist ein einfaches Korollar zm Satz 7.18 in [C], welches wiederum auf Korollar 2.14 in [K] basiert. Dieses wiederum wird unter Bezugnahme auf Proposition 5.18 in [BL] hergeleitet.\n\n\n\nMein guter Kollege Andrew meinte, er glaube, er h√§tte daf√ºr vor ein paar Jahren einen Beweis finden k√∂nnen ‚Ä¶\n\n\n\nF√ºr Interessierte ist das Ergebnis auf der Webseite zu diesem Buch zu sehen. ‚Äì Die es leider nicht mehr gibt.\n\n\n\nKapitel 3: Der Beweis hierf√ºr wird erst in Kapitel 7 erbracht, wenn wir die Theorie weiterentwickelt haben.\nKapitel 7: Der Einfachheit halber beweisen wir dies nur f√ºr den Fall z = 0. Der allgemeine Fall wird jedoch in Anhang C behandelt.\nAnhang C: Der formale Beweis geht √ºber den Rahmen dieses Buches hinaus, aber unsere Intuition sagt uns nat√ºrlich, dass dies wahr ist."
  },
  {
    "objectID": "posts/2025-07-27-Beweisverfahren/index.html#allgemeine-beweistechniken",
    "href": "posts/2025-07-27-Beweisverfahren/index.html#allgemeine-beweistechniken",
    "title": "Beweisverfahren in der Mathematik",
    "section": "",
    "text": "Trivial!\n\n\n\nDer Satz folgt sofort aus dem Fakt\n\\[\n  \\left|\n    \\oplus_{k \\in S}\n      \\left(\n       \\mathfrak{K}^{\\mathbb{F}^\\alpha(i)}  \n      \\right)_{i \\in \\mathcal{U}_k}\n  \\right|\n  \\preceq \\aleph_1\n  \\text{ wenn }\n  \\left[\n    \\mathfrak{H}_\\mathcal{W}\n  \\right]     \n  \\cap \\mathbb{F}^\\alpha(\\mathbb{N}) \\neq \\emptyset.\n\\]\n\n\n\nDer Satz ist ein einfaches Korrollat eines Ergebnis bewiesen in einer handgeschriebenen Anmerkung ausgeh√§ndigt in einer Vorlesung bei der Jugoslawischen Mathematischen Gesellschaft 1973.\n\n\n\nDer Beweis kann auf Seite 478 gefunden werden, in einem Buch, welches nur 396 Seiten hat.\n\n\n\nProposition 5.18 in [BL] ist ein einfaches Korollar zm Satz 7.18 in [C], welches wiederum auf Korollar 2.14 in [K] basiert. Dieses wiederum wird unter Bezugnahme auf Proposition 5.18 in [BL] hergeleitet.\n\n\n\nMein guter Kollege Andrew meinte, er glaube, er h√§tte daf√ºr vor ein paar Jahren einen Beweis finden k√∂nnen ‚Ä¶\n\n\n\nF√ºr Interessierte ist das Ergebnis auf der Webseite zu diesem Buch zu sehen. ‚Äì Die es leider nicht mehr gibt.\n\n\n\nKapitel 3: Der Beweis hierf√ºr wird erst in Kapitel 7 erbracht, wenn wir die Theorie weiterentwickelt haben.\nKapitel 7: Der Einfachheit halber beweisen wir dies nur f√ºr den Fall z = 0. Der allgemeine Fall wird jedoch in Anhang C behandelt.\nAnhang C: Der formale Beweis geht √ºber den Rahmen dieses Buches hinaus, aber unsere Intuition sagt uns nat√ºrlich, dass dies wahr ist."
  },
  {
    "objectID": "posts/2021-02-12-gedankenst√ºtze-zu-wichtigen-funktionsbegriffen-in-der-statistik/index.html",
    "href": "posts/2021-02-12-gedankenst√ºtze-zu-wichtigen-funktionsbegriffen-in-der-statistik/index.html",
    "title": "Gedankenst√ºtze zu wichtigen Funktionsbegriffen in der Statistik",
    "section": "",
    "text": "Eine kleine Liste von fundermentalen Begriffen in der Statistik.\nGilt f√ºr eine reelle Funktion \\(f: \\mathbf{R} \\to \\mathbf{R}\\):\n\n\\(f(x)\\) ist nichtnegativ, d.h., \\(f(x) \\geq 0\\), f√ºr alle \\(x \\in \\mathbf{R}\\).\n\\(f(x)\\) ist integrierbar.\n\\(f(x)\\) ist normiert in dem Sinne, dass \\[\\int_{-\\infty}^\\infty f(x) \\,\\text{d}x = 1\\]\n\nDann nennen wir \\(f(x)\\) eine Wahrscheinlichkeitsdichtefunktion (engl. probability density funktion kurz pdf) oder kurz Dichte (engl. density).\nDurch \\[P([a, b]) := \\int_a^b f(x) \\text{d} x\\] definiert \\(f\\) eine Wahrscheinlichkeitsverteilung auf den reellen Zahlen.\nIst \\(X\\) eine reelwertige Zufallsvariable (kurz ZV) und existiert eine reelle Funktion \\(f_X(x)\\) der Art, dass f√ºr alle \\(a \\in \\mathbf{R}\\)\n\\[P(X \\leq a) = \\int_{-\\infty}^a f_X(x) \\text{d}x\\]\ngilt, so nennt man \\(f\\) die Wahrscheinlichkeitsdichtefunktion von \\(X\\).\nDie Funktion\n\\[F_X(a) = P(X \\leq a)\\]\nnenen wir (Wahrscheinlichkeits-)Verteilung(-sfunktion) (engl. cumulative distribution function kurz cdf aber auch nur distribution function) von \\(X\\).\nGenau dann ist eine Funktion \\(F: \\mathbf{R} \\to [0, 1]\\) eine Verteilungsfunktion, wenngilt:\n\nEs ist \\(\\lim_{t \\to -\\infty} F(t)=0\\) und \\(\\lim_{t \\to +\\infty} F(t)=1\\)\nDie Funktion \\(\\overline{F}(t)\\) ist monoton wachsend.\nDie Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig\n\nDie Funktion\n\\[\\overline{F}_X(a) = 1 - F_X(a)\\]\nnennen wir √úberlebensfunktion (engl. survival function, complementarey cumulative distribution funktion kurz ccdf, tail distribution, exceedance oder reliability function).\nEs gilt \\(\\overline{F}_X(a) + F_X(a) = 1\\).\nGenau dann ist eine Funktion \\(\\overline{F}: \\mathbf{R} \\to [0, 1]\\) eine √úberlebensfunktion, wenn gilt:\n\nEs ist \\(\\lim_\\limits{t \\to -\\infty} \\overline{F}(t)=1\\) und \\(\\lim_{t \\to + \\infty}\\overline{F}(t)=0\\)\nDie Funktion \\(\\overline{F}(t)\\) ist monoton fallend.\nDie Funktion \\(\\overline{F}(t)\\) ist rechtsseitig stetig\n\nEin paar weitere Eigenschaften von √úberlebensfunktionen:\n\nNicht-negative stetige ZV \\(X\\) mit Erwartungswert, also \\(\\int_0^\\infty x f(x) \\text{d} x = \\mu &lt; \\infty\\), erf√ºllen die Markov-Ungleichung\n\n\\[\\overline{F}_X(x) \\leq \\frac{\\operatorname{E}(X)}{x}\\]\n\nIst \\(X\\) eine ZV und \\(\\overline{F}_X\\) die zugeh√∂rige √úberlebensfunktion. Existiert \\(E(X)\\), dann gilt \\(\\lim_{t \\to +\\infty}\\overline{F}(x)=0 = o\\left(\\frac{1}{x}\\right)\\).\n\nBeweisskizze: Sei \\(f_X\\) die Dichtefunktion von \\(F_X\\) zur ZV \\(X\\). F√ºr jedes \\(c&gt;0\\) ist dann \\[\n\\begin{aligned}\n      E(X) = \\int_0^\\infty x \\cdot f_X(x) \\text{d}x &=  \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty x \\cdot f_X(x) \\text{d}x \\\\\n      &\\geq  \\int_0^cx \\cdot f_X(x) \\text{d}x +\\int_c^\\infty c \\cdot f_X(x) \\text{d}x \\\\\n      &=  \\int_0^cx \\cdot f_X(x) \\text{d}x + c \\cdot \\int_c^\\infty f_X(x) \\text{d}x \\\\\n      &=  \\int_0^cx \\cdot f_X(x) \\text{d}x +c \\cdot \\overline{F}(c)\n  \\end{aligned}\n\\]\nDamit gilt nun: \\[\n0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\n\\] Wegen \\(\\lim\\limits_{c \\to +\\infty} \\int_0^cx \\cdot f_X(x) \\text{d}x = E(X)\\) folgt:\n\\[0 \\leq c \\cdot \\overline{F}(c) \\leq E(X) - \\int_0^cx \\cdot f_X(x) \\text{d}x\\to 0 \\text{ wenn } c \\to \\infty\n\\]\nF√ºr nicht-negative ZV \\(X\\) gilt:\n\\[\nE(X) = \\int_0^\\infty \\overline{F}_X(x) \\,\\text{d} x\n\\]"
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html",
    "title": "B-B√§ume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "",
    "text": "B-B√§ume sind eine Art von selbstbalancierenden Suchb√§umen, die in Datenbanken und Dateisystemen weit verbreitet sind. In diesem Beitrag werde ich verschiedene Python-Implementierungen von B-B√§umen vergleichen und ihre Vor- und Nachteile diskutieren."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#einf√ºhrung",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#einf√ºhrung",
    "title": "B-B√§ume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "",
    "text": "B-B√§ume sind eine Art von selbstbalancierenden Suchb√§umen, die in Datenbanken und Dateisystemen weit verbreitet sind. In diesem Beitrag werde ich verschiedene Python-Implementierungen von B-B√§umen vergleichen und ihre Vor- und Nachteile diskutieren."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#implementierungen-im-vergleich",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#implementierungen-im-vergleich",
    "title": "B-B√§ume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "Implementierungen im Vergleich",
    "text": "Implementierungen im Vergleich\n\nbintrees: Diese Bibliothek bietet eine einfache Implementierung von B-B√§umen und anderen Baumstrukturen. Sie ist leicht zu verwenden und gut dokumentiert, aber m√∂glicherweise nicht die leistungsf√§higste Option f√ºr gro√üe Datenmengen.\nbplustree: Diese Bibliothek implementiert B+-B√§ume, eine Variante von B-B√§umen, die f√ºr Datenbankanwendungen optimiert ist. Sie bietet gute Leistung und Skalierbarkeit, erfordert jedoch ein tieferes Verst√§ndnis der B+-Baum-Struktur.\npybloom-live: Obwohl haupts√§chlich f√ºr Bloom-Filter gedacht, enth√§lt diese Bibliothek auch eine B-Baum-Implementierung. Sie ist n√ºtzlich f√ºr Anwendungen, die sowohl B√§ume als auch probabilistische Datenstrukturen ben√∂tigen.\nbtree: Diese Implementierung ist in der Standardbibliothek von Python enthalten und bietet grundlegende B-Baum-Funktionalit√§ten. Sie ist einfach zu verwenden, aber m√∂glicherweise nicht so leistungsf√§hig wie spezialisierte Bibliotheken."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#leistungstest",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#leistungstest",
    "title": "B-B√§ume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "Leistungstest",
    "text": "Leistungstest\nUm die Leistung der verschiedenen Implementierungen zu vergleichen, habe ich einen einfachen Test durchgef√ºhrt, bei dem ich eine gro√üe Anzahl von Einf√ºgungen und Suchvorg√§ngen durchgef√ºhrt habe. Hier sind die Ergebnisse:\nimport time\nfrom bintrees import FastRBTree\nfrom bplustree import BPlusTree\n# Testdaten\ndata = list(range(100000))\n# bintrees Test\nbintree = FastRBTree()\nstart_time = time.time()\nfor item in data:\n    bintree.insert(item, item)\nbintree_time = time.time() - start_time\n# bplustree Test\nbplustree = BPlusTree('bplustree.db', order=50)\nstart_time = time.time()\nfor item in data:\n    bplustree.insert(item, item)\nbplustree_time = time.time() - start_time\nprint(f\"bintrees Einf√ºgezeit: {bintree_time} Sekunden\")\nprint(f\"bplustree Einf√ºgezeit: {bplustree_time} Sekunden\")\nDie Ergebnisse zeigten, dass bplustree bei gro√üen Datenmengen eine bessere Leistung erzielte, insbesondere bei Suchvorg√§ngen, w√§hrend bintrees f√ºr kleinere Datens√§tze ausreichend war."
  },
  {
    "objectID": "posts/2026-01-04-B-Baeume-und-mehr/index.html#fazit",
    "href": "posts/2026-01-04-B-Baeume-und-mehr/index.html#fazit",
    "title": "B-B√§ume und mehr: Ein Vergleich von Python-Implementierungen",
    "section": "Fazit",
    "text": "Fazit\nDie Wahl der richtigen B-Baum-Implementierung h√§ngt stark von den spezifischen Anforderungen Ihres Projekkts ab. F√ºr einfache Anwendungen kann bintrees ausreichend sein, w√§hrend bplustree besser f√ºr komplexe Datenbankanwendungen geeignet ist. Es lohnt sich, die verschiedenen Optionen zu testen, um die beste Leistung f√ºr Ihre spezifischen Anwendungsf√§lle zu erzielen. Unabh√§ngig von der gew√§hlten Implementierung bieten B-B√§ume eine effiziente M√∂glichkeit, gro√üe Datenmengen zu verwalten und zu durchsuchen. Ich hoffe, dieser Vergleich hilft Ihnen bei der Auswahl der richtigen B-Baum-Implementierung f√ºr Ihre Python-Projekte! Happy Coding! üöÄ"
  },
  {
    "objectID": "posts/2025-08-09-brotli-statt-gzip-konfigurieren/index.html",
    "href": "posts/2025-08-09-brotli-statt-gzip-konfigurieren/index.html",
    "title": "Brotli statt Gzip auf dem Server konfigurieren",
    "section": "",
    "text": "Wie mensch einen Webserver davon √ºberzeugt Brotli zu nutzen\nIch habe meinen Webserver von gzip auf brotli umger√ºstet. Dachte ich jeden falls, aber alle Test mit\n&gt; curl -I -H 'Accept-Encoding: br' https://sefiroth.de\nschlugen fehl. Nach langer Suche im Internet habe ich herausgefunden, dass zwar die Datei brotli.load in den Verzeichnissen /etc/apache2/mods-enable und /etc/apache2/mods-available existieren, nicht aber die Datei brotli.conf, mit welcher die ganzen Einstellungen gemacht werden.\nF√ºgt mensch nun in diese Datei den Inhalt\n&lt;IfModule mod_brotli.c&gt;\n    AddOutputFilterByType BROTLI_COMPRESS text/html text/plain text/xml text/css text/javascript app\nlication/javascript application/x-javascript application/json application/xml\n    BrotliCompressionQuality 6\n&lt;/IfModule&gt;\nein und startet danach den apache-Server neu, so l√§uft brotli endlich auch auf dem Server!"
  },
  {
    "objectID": "posts/2021-02-12-ein-paar-gedanken-ueber-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "href": "posts/2021-02-12-ein-paar-gedanken-ueber-potenzgesetzliche-verteilungen-power-law-distributions/index.html",
    "title": "Ein paar Gedanken √ºber potenzgesetzliche Verteilungen (power law distributions)",
    "section": "",
    "text": "Eine Funktion \\(f(x)\\) hei√üt potenzgesetzlich, falls\n\\[f(x) = C \\cdot x^a\\]\ngilt, f√ºr mindestens alle reellen \\(x &gt; x_{min}\\).\nGew√∂hnlich setzt man \\(\\alpha = -a\\) und schreibt\n\\[f(x) = C \\cdot x^{-\\alpha}.\\]\nDamit ergibt sich f√ºr \\(f'(x)\\) die Form:\n\\[\nf'(x) = -C \\cdot \\alpha \\cdot x^{-\\alpha -1} = C^* \\cdot x^{-(\\alpha + 1)}\n\\] mit \\(C^* = -C \\cdot \\alpha\\).\nEine (streng) potenzgesetzliche Verteilungen (engl. (strong) power-law probability distribution) zur ZV \\(X\\) ist eine Verteilung deren √úberlebensfunktion \\(\\overline{F}_X(x)=P(X &gt; x)\\) die folgende Gestalt hat: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha}\\]\nMit der Dichte \\(f_X\\) ergibt sich: \\[\\overline{F}(x)=P(X &gt; x) = C \\cdot x^{-\\alpha} =\\int_x^\\infty f_X(t) \\text{d}t = C^* \\cdot \\int_x^\\infty  t^{-(\\alpha+1)} \\text{d}t = C^* \\cdot \\int_x^\\infty t^{-\\alpha} \\text{d}t\\]\nAnstelle der Konstanten \\(C\\) tritt oft eine langsam variierende Funktion (engl. slowly varying funktion). Wir erhalten somit die folgende, allgemeinere Definition:\nEine potenzgesetzliche Verteilungen (engl. power-law probability distribution) (zu einer Zufallsvariable \\(X\\)) ist eine Verteilung deren √úberlebensfunktion \\(\\overline{F}(x)=P(X &gt; x)\\) die folgende Gestalt hat:\n\\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha}\\]\nDabei ist \\(L(x):(x_{\\min}, +\\infty) \\to (x_{\\min}, +\\infty)\\) eine langsam variierende Funktion, also gilt f√ºr alle \\(t&gt;0\\):\n\\[\n\\lim_{x \\to +\\infty} \\frac{L(t \\cdot x)}{L(x)} = 1\n\\]\nIst nun wieder \\(f_X\\) die Dichte, so erhalten wir: \\[\\overline{F}(x)=P(X &gt; x) = L(x) \\cdot x^{-\\alpha} = \\int_x^\\infty f_X(t) dt\\]\n\\[f'(x) = [L(x)x^{-\\alpha}]'\\] \\[\\int_x^\\infty f_X(t) dt= \\int_x^\\infty [L(t)t^{-\\alpha}]' dt\\]\n\\(\\Delta x_0 = h = x_1 - x_0\\) \\(x_1 = x_0 + \\Delta x_0 = x_0 + h\\) \\(x_1 = c \\cdot x_0\\) \\(x_0 + h = c \\cdot x_0 &lt;=&gt; c = 1 + \\frac{h}{x_0}\\) \\(L(x_1) = L(x_0+h) = L(c \\cdot x_0)\\) \\(L(x_1) - L(x_0) = L(c \\cdot x_0) - L(x_0) = L(x_0 + h) - L(x_0)}\\) $\nFakten:\n\nSinnvoll nur, wenn \\(\\alpha &gt; 0\\).\nIst \\(\\alpha &lt; 3\\), dann ist die Varianz und die Schiefe (engl. skewness) (mathematisch) nicht definiert.\nF√ºr \\(k &gt; \\alpha-1\\) ist das k. Moment unendlich.\n\nLogarithmiert man \\(y=f(x)=C \\cdot x^{-\\alpha}\\), so erh√§lt mensch:\n\\[\\log(y) = \\log(C) -\\alpha \\cdot \\log(x)\\]\nIst eine Verteilung potenzgesetzlich, dann kann man \\(\\alpha\\), wie folgt absch√§tzen:\nSeien \\(x_0, x_1 &gt; x_{min}\\) zwei reelle Zahlen, \\(y_0=f(x_0)\\) bzw. \\(y_1 = f(y_1)\\).\nDann kann mensch wegen\n\\[\\begin{align*}\n  \\log(y_1) - \\log(y_0) &= \\log(C) - \\alpha \\cdot\\log(x_1) - \\log(C) + \\alpha \\cdot \\log(x_0) \\\\\n                        &= \\alpha \\cdot\\left(\\log(x_0)- \\log(x_1) \\right)\n\\end{align*}\\]\nden Wert f√ºr \\(\\alpha\\), so kann man mittels\n\\[\\alpha = \\frac{\\log(y_1) - \\log(y_0)}{\\log(x_0)- \\log(x_1)}\\]\nden Wert f√ºr \\(\\alpha\\) bestimmen.\nMit dem so ermittelten \\(\\alpha\\), k√∂nnen wir \\(C\\) wegen\n\\[\\log(C) = \\log(y)+ \\alpha\\log(x)\\]\nmit Hilfe von\n\\[C = y \\cdot x_0^\\alpha = f(x_0) \\cdot x_0^\\alpha\\]\nf√ºr ein \\(x_0 &gt; x_{min}\\) absch√§tzen.\nBeispiel:\nNehmen wir die t-Verteilung mit \\(n=2\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{2}(x)\\).\nDann dann k√∂nnen wir \\(\\alpha=3\\) und \\(C=1\\) absch√§tzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nNoch ein Beispiel:\nNehmen wir die t-Verteilung mit \\(n=1\\) Freiheitsgeraden. Die Dichtefunktion bezeichnen wir mit \\(t_{1}(x)\\).\nDann dann k√∂nnen wir \\(\\alpha=2\\) und \\(C=0.32\\) absch√§tzen.\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nEin ‚ÄôGegen-‚ÄôBeispiel:\nBetrachten wir nun die (rechte Seite ‚Äì \\(x&gt;1=x_{min}\\)) einer Gau√ü‚Äôschen Standardnormalverteilung.\nMit den St√ºtzstellen \\(x_0 = 1\\) und \\(x_1 = 5\\) k√∂nnen wir \\(\\alpha=7.46\\) und \\(C=0.24\\) absch√§tzen. Schauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"norm\",\n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(C*x**(-alpha) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( C*x**(-alpha) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWir erkennen, dass hier etwas nicht passt. Die Standardnormalverteilung ist (vielleicht) keine potenzgesetzich Verteilung?\nEin oft verwendetes Kriterium ist, dass sich die Funktion in der doppelt-logarithmischen Darstellung als Gerade offenbart.\nSchauen wir daher einmal nach:\n\nlower_bound &lt;- 1\nupper_bound &lt;- 8\n\ngf_dist(\"norm\", \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\n** ‚Äî **\nWeiter gilt f√ºr potenzgesetzliche Verteilungen wegen\n\\[\\frac{f(x)}{f(c\\cdot x)} = \\frac{C \\cdot x^{-\\alpha}}{C \\cdot (c\\cdot x)^{-\\alpha}}\n= c^\\alpha\\]\n\\(f(x)\\) und \\(f(c\\cdot x)\\) f√ºr alle (beliebig aber festen) \\(c&gt;0\\) proportional, was man gerne als \\(f(x) \\propto f(c \\cdot x)\\) schreibt.\n** ‚Äî **\nDie Wahrscheinlichkeit f√ºr ein (mindestens) \\(8-\\sigma\\) Ereignis liegt bei einer Standardnormalverteilung bei etwa \\(6.66133814775094\\times 10^{-16}\\).\nBei einer t-Verteilung mit 2 Freiheitsgeraden bei etwa 0.00763403608266899$.\nW√§hrend die Eintrittschance eines (mindestens) \\(8-\\sigma\\) Ereignisses bei der Standardnormalverteilung bei etwa \\(1 : round(1/(1-pnorm(8)),0)\\) liegt, ist diese der t-Verteilung mit einem Freiheitsgrad bei etwa $1 : 131"
  },
  {
    "objectID": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html",
    "href": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html",
    "title": "Beh√§bige Funktionen aka slowly varying function",
    "section": "",
    "text": "Reelle Funktionen, die ihren Funktionswert kaum √§ndern, kann man mit Fug und Recht durchaus beh√§big nennen, korrekter w√§re aber von langsam variierenden Funktionen zu sprechen\nIm Kontext von potenzgesetzlichen Verteilungen kommt der Begriff slowly varying function vor, der Funktionen beschreibt die nur sehr gering auf √Ñnderungen ihres Parameters reagieren.\nDie Definition dieser beh√§bigen besser langsam variierenden Funktionen stammt von Jovan Karamata:\nEine positive stetige Funktion \\(L\\)1 auf den positiven reelen Zahlen ist langsam variierend (im unendlichen), falls f√ºr alle reellen \\(t&gt;0\\)\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = 1\\]\ngilt.\nBeispiele:\nBeweisskizze: Mit \\(L(x) = c\\) ist \\(L(x) = L(t x) = c\\) und damit \\(\\frac{L(t x)}{L(x)}= 1\\).\nBeweisskizze: Da \\(\\lim\\limits_{x \\to +\\infty} L(x) = b = \\lim\\limits_{x \\to +\\infty} L(t\\cdot x)\\) ist \\(\\lim\\limits_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = \\frac{\\lim\\limits_{x \\to +\\infty} L(t\\cdot x)}{\\lim\\limits_{x \\to +\\infty} L(x)} = \\frac{b}{b} = 1\\)\nBeweisskizze: Es gilt: - F√ºr jede reelle Zahl \\(x&gt;0\\) ist \\(\\log_x(x) = 1\\). - F√ºr reelle Zahlen \\(a, b\\) gilt: \\(\\frac{log(a)}{\\log(b)} = \\log_b(a)\\) - F√ºr reelle Zahlen \\(a, b\\) gilt. \\(\\log(a \\cdot b) = log(a) + log(b)\\) - F√ºr jede Konstante \\(k\\) gilt \\(\\lim_{x \\to +\\infty} \\log_x (k) = 0\\) Somit gilt \\(\\frac{\\log_\\beta(k \\cdot x)}{\\log_\\beta(x)} = \\log_x(k\\cdot x) = \\log_x(k) + \\log_x(x) = \\log_x(k) +1 \\to 1\\) wenn \\(x \\to +\\infty\\)\nEine regul√§r variierende Funktion \\(L:(0,+\\infty) \\to (0,+\\infty)\\) ist eine Funktion f√ºr die der Term\n\\[\\lim_{x \\to +\\infty} \\frac{L(t\\cdot x)}{L(x)} = g(t)\\]\nmit \\(g(t)\\) f√ºr alle \\(t&gt;0\\) einen endlichen aber nicht verschwindenen Wert (m.a.W.: \\(g(t) \\neq 0\\)) hat .\nKaramata hat die regul√§r variierenden Funktionen nun wie folgt charaterisiert:"
  },
  {
    "objectID": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "href": "posts/2021-02-13-behaebige-funktionen-aka-slowly-varying-function/index.html#footnotes",
    "title": "Beh√§bige Funktionen aka slowly varying function",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\n\\(L\\) oder auch \\(l\\) wird (angeblich) hier f√ºr den Begriff lente (serb. f√ºr faul) verwendet. Beh√§big ist also doch nicht so falsch. ;-)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-06-27-datenjudo-fuer-frageboegen/index.html",
    "href": "posts/2021-06-27-datenjudo-fuer-frageboegen/index.html",
    "title": "Datenjudo f√ºr Frageb√∂gen",
    "section": "",
    "text": "Ab und zu bekomme ich die Frage, wie man einen Fragebogen mit Likert-Scalen-Items auswerten kann.\nDazu kann etwas gezieltes Datenjudo helfen. Wir schauen uns das folgende generierte Mini-Beispiel an:\n\nlibrary(mosaic)  # Basis Paket\nlibrary(tibble)  # Eine modernere Variante der data.frames!\nset.seed(2009)   # Reproduzierbarkeit\n\nN &lt;- 25  # Anzahl der Testzeileneintr√§ge in den \"testdaten\"!\n\n# Wir wollen eine Likert-Scale \nminLikert &lt;- 1  # bis\nmaxLikert &lt;- 6  # erstellen.\n\n# Zum sp√§teren Umrechnen der inversen Items:\nmaxInvItem &lt;- maxLikert + 1\n\n# Wir bauen uns eine Testumfrage mit zwei Itemserien \n# (AS1-AS6 und BS1-BS6) und N Beobachtungen.\n# Die Items AS3, AS4  und BS1 und BS5 sind dabei \n# inverse Items, welche sp√§ter umgerechnet werden:\ntestdaten &lt;- tibble(\n    ID = 1:N,\n    # AS1-AS6 bilden ein Itemset:\n    AS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    AS6 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # BS1-BS5 bilden ein Itemset:\n    BS1 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS2 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS3 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS4 = sample(minLikert:maxLikert, N, replace = TRUE),\n    BS5 = sample(minLikert:maxLikert, N, replace = TRUE),\n    # Geschlecht als sex mit (1 f√ºr Frauen und 2 f√ºr M√§nner)\n    sex = sample(1:2, N, replace = TRUE)\n)\n\n# Orinal testdaten einmal ausgeben:\nhead(testdaten)\n#&gt; # A tibble: 6 √ó 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     4     4     1     1     2     4     5     5     3     2     3     2\n#&gt; 2     2     2     1     2     2     5     6     4     5     2     2     6     1\n#&gt; 3     3     4     4     6     3     3     4     3     3     4     1     5     1\n#&gt; 4     4     2     6     1     4     5     4     6     4     5     1     3     1\n#&gt; 5     5     3     1     3     5     5     6     6     1     2     6     5     1\n#&gt; 6     6     6     4     1     3     6     6     4     6     5     3     3     1\n\nDie Spalten AS3, AS4 und BS1, BS5 waren inverse Items, die wir noch umrechnen m√ºssen:\n\n# Inverse Item umrechnen:\ntestdaten |&gt;\n    mutate(\n        AS3 = maxInvItem - AS3,\n        AS4 = maxInvItem - AS4,\n        BS1 = maxInvItem - BS1,\n        BS5 = maxInvItem - BS5\n    ) -&gt; testdaten_korrigiert \n\n# Die Daten mit den umgerechnetern inversen Items:\nhead(testdaten_korrigiert)\n#&gt; # A tibble: 6 √ó 13\n#&gt;      ID   AS1   AS2   AS3   AS4   AS5   AS6   BS1   BS2   BS3   BS4   BS5   sex\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     1     4     4     6     6     2     4     2     5     3     2     4     2\n#&gt; 2     2     2     1     5     5     5     6     3     5     2     2     1     1\n#&gt; 3     3     4     4     1     4     3     4     4     3     4     1     2     1\n#&gt; 4     4     2     6     6     3     5     4     1     4     5     1     4     1\n#&gt; 5     5     3     1     4     2     5     6     1     1     2     6     2     1\n#&gt; 6     6     6     4     6     4     6     6     3     6     5     3     4     1\n\nDie jeweiligen Itemsets werden nun zur einem Wert (Gesamtscore) zusammengefasst, in dem wir jeweils den Mittelwert von AS1-AS6 und BS1-BS5 bildenund in AS bzw. BS speichern:\n\n# Wir fassen nun die AS1-AS6 und die BS1-BS5 zusammen \n# und bilden die jeweiligen Mittelwerte:\ntestdaten_korrigiert |&gt;\n    group_by(ID, sex) |&gt;  # Damit wird f√ºr jede Zeile die Zusammenfassung gemacht!\n    summarise(\n        AS = mean(c(AS1, AS2, AS3, AS4, AS5, AS6)),\n        BS = mean(c(BS1, BS2, BS3, BS4, BS5))\n    ) -&gt; testdaten_sum\n\n# Ausgabe der Mittelwerte der AS und BS\nhead(testdaten_sum)\n#&gt; # A tibble: 6 √ó 4\n#&gt; # Groups:   ID [6]\n#&gt;      ID   sex    AS    BS\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2  4.33   3.2\n#&gt; 2     2     1  4      2.6\n#&gt; 3     3     1  3.33   2.8\n#&gt; 4     4     1  4.33   3  \n#&gt; 5     5     1  3.5    2.4\n#&gt; 6     6     1  5.33   4.2\n\nDie Datentabelle testdaten_sum enth√§lt nun die Spalten AS und BS mit den entsprechenden Mittelwerten der einzelnen Items AS1-AS6 sowieso BS1- BS5.\nWir wollen nun die Ergebnisse als Boxplots anzeigen lassen. Daf√ºr benennen wir die Geschlechter von 1,2 auf ‚ÄúFrau‚Äù, ‚ÄúMann‚Äù um:\n\ntestdaten_sum |&gt;\n    mutate(sex = factor(sex, levels = c(1, 2),\n                             labels = c(\"Frau\", \"Mann\"))\n    ) -&gt; testdaten_sex \n\nNun k√∂nnen wir die Boxplots erstellen:\n\n# Darstellung der Ergebnisse als Boxplot AS ~ sex:\ngf_boxplot(AS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von AS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item AS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(2.5, 4.5)  # Gibt den Bereich von 2.5 bis 4.5 aus!\n    )  \n  )\n\n# Darstellung der Ergebnisse als Boxplot BS ~ sex:\ngf_boxplot(BS ~ sex, data = testdaten_sex) %&gt;%\n    gf_labs(\n        title = \"Boxplot von BS nach Geschlechtern\",\n        x = \"Geschlechter\",\n        y = \"Item BS\"\n    ) |&gt;\n  gf_refine(\n    scale_y_continuous(\n      breaks = 1:6, \n      label = 1:6,\n      limits = c(1, 6)  # Gibt den ganzen Bereich von 1 bis 6 aus!\n    )  \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Kennzahlen dazu erhalten wir mit favstats. Dabei w√§hlen wir die ersten sechs Eintr√§ge (Variabelbezeichnung und Q0 bis Q4) aus:\n\nfavstats(AS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex      min       Q1   median       Q3      max\n#&gt; 1 Frau 2.166667 3.333333 3.666667 4.166667 5.333333\n#&gt; 2 Mann 3.666667 4.000000 4.333333 4.333333 4.500000\nfavstats(BS ~ sex, data = testdaten_sex)[1:6]\n#&gt;    sex min  Q1 median  Q3 max\n#&gt; 1 Frau 2.4 2.8    3.4 4.2 4.6\n#&gt; 2 Mann 2.6 3.2    3.3 3.8 4.2\n\nUnter der Verwendung des Pakets likert (https://github.com/jbryer/likert) k√∂nnen wir die Ausgaben auch noch etwas sch√∂ner gestalten:\n\nlibrary(likert)\n\n# Wir w√§hlen nur den Itemset BS aus und speichern in in items2:\ntestdaten_korrigiert |&gt;\n  select(\n    starts_with(\"BS\")\n  ) -&gt; items2\n\n# Leider mag likert tibbels nicht so gerne, daher:\nitems2 &lt;- as.data.frame(items2)\n\n# Wir geben den Items noch ein paar Buzzwords:\nnames(items2) &lt;- c(\"Gesundheit\", \"Familie\", \"Geld\", \"Freunde\", \"Langes Leben\")\n\n# Vorbereitung:\nl2 &lt;- likert(items2, nlevels = 5)\n\n# Zusammenfassung\nsummary(l2)\n#&gt;           Item      low   neutral     high     mean       sd\n#&gt; 3         Geld 28.57143 23.809524 47.61905 3.380952 1.359272\n#&gt; 5 Langes Leben 57.14286  4.761905 38.09524 2.619048 1.532194\n#&gt; 1   Gesundheit 38.09524 28.571429 33.33333 2.857143 1.492840\n#&gt; 2      Familie 38.09524 28.571429 33.33333 2.952381 1.499206\n#&gt; 4      Freunde 42.85714 23.809524 33.33333 2.857143 1.236354\n\n# Graphische Ausgaben:\nplot(l2)\n\nplot(l2,\"bar\")\n\nplot(l2,\"heat\")\n\nplot(l2,\"density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoil√†!"
  },
  {
    "objectID": "posts/2025-12-22-Das-Lotto-Abbildungs-Problem/index.html",
    "href": "posts/2025-12-22-Das-Lotto-Abbildungs-Problem/index.html",
    "title": "Das Lotto-Abbildungsproblem",
    "section": "",
    "text": "Die Beschreibung\nJeder, der Kombinatorik oder Wahrscheinlichkeitsrechnung studiert hat, kennt das klassische Lotto-Problem: Aus einer Menge von 49 Zahlen werden 6 Zahlen ohne Zur√ºcklegen gezogen. Wie viele verschiedene Kombinationen sind m√∂glich?\nDie Antwort ist bekanntlich: \\[\n  \\binom{49}{6} = \\frac{49!}{6!(49-6)!} = 13\\,983\\,816\n\\]\nAber viele sehen nicht, dass dies auch bedeutet, dass jedes Ziehungsergebnis ein-ein-deutig auf die Zahlen \\(1\\) bis \\(13\\,983\\,816\\) abbildet. Genauer, es gibt (mindestens) eine Abbilung aus \\(\\mathcal{L} = \\{1, 2, \\ldots, 49\\}\\) in \\(\\mathcal{M} = \\{1, 2, \\ldots, 13\\,983\\,816\\}\\), so dass jede 6-elementige Teilmenge von \\(\\mathcal{L}\\) auf ein eindeutiges Element von \\(\\mathcal{M}\\) abgebildet wird.\n\n\nDie Frage\nWie sieht so eine Abbildung aus und vorallem wie lautet die Umkehrabbildung?\n\n\nEine m√∂gliche L√∂sung\nWir k√∂nnen die Abbildung konstruieren, indem wir die 6 gezogenen Zahlen sortieren und dann eine lexikographische Ordnung verwenden, um jede Kombination zu nummerieren. Angenommen, die gezogenen Zahlen sind \\(a_1 &lt; a_2 &lt; a_3 &lt; a_4 &lt; a_5 &lt; a_6\\). Dann k√∂nnen wir die Abbildung \\(f: \\mathcal{L}^6 \\to \\mathcal{M}\\) wie folgt definieren:\n\\[\nf(a_1, a_2, a_3, a_4, a_5, a_6) = \\sum_{i=1}^{6} \\binom{a_i - 1}{i}\n\\]\nHierbei ist \\(\\binom{n}{k}\\) der Binomialkoeffizient, der die Anzahl der M√∂glichkeiten angibt, \\(k\\) Elemente aus einer Menge von \\(n\\) Elementen auszuw√§hlen.\nDiese Abbildung ist eindeutig, da jede Kombination von 6 Zahlen eine eindeutige Summe von Binomialkoeffizienten ergibt.\nDie Umkehrabbildung \\(f^{-1}: \\mathcal{M} \\to \\mathcal{L}^6\\) kann durch eine iterative Methode konstruiert werden, bei der wir die gr√∂√üte Zahl \\(a_6\\) bestimmen, die noch in die Summe passt, dann \\(a_5\\), und so weiter, bis wir alle 6 Zahlen gefunden haben.\nHier ist ein Beispiel f√ºr die Umkehrabbildung:\n\nSetze \\(n = f(a_1, a_2, a_3, a_4, a_5, a_6)\\).\nF√ºr \\(i\\) von 6 bis 1:\n\nFinde die gr√∂√üte Zahl \\(a_i\\) such that \\(\\binom{a_i - 1}{i} \\leq n\\).\nSetze \\(n = n - \\binom{a_i - 1}{i}\\).\n\nGib die Zahlen \\(a_1, a_2, a_3, a_4, a_5, a_6\\) zur√ºck.\n\nDiese Methode garantiert, dass wir die urspr√ºnglichen gezogenen Zahlen zur√ºckerhalten, da wir die gleiche Logik wie in der Vorw√§rtsabbildung verwenden, aber in umgekehrter Reihenfolge.\n\n\nFazit\nDie Abbildung des Lotto-Problems zeigt, wie kombinatorische Konzepte verwendet werden k√∂nnen, um eindeutige Zuordnungen zwischen Mengen zu erstellen. Diese Art von Abbildungen ist nicht nur in der Mathematik interessant, sondern findet auch Anwendungen in Bereichen wie Kryptographie und Datenkompression.\n\n\nImplementierung in R und Python\n\nRPython\n\n\n\n# R-Code zur Berechnung der Abbildung und Umkehrabbildung\nlibrary(gmp)\n\n# Funktion zur Berechnung der Abbildung\nlotto_abbildung &lt;- function(a) {\n  l &lt;- sapply(1:6, function(i) chooseZ(a[i] - 1, i), simplify = \"array\", USE.NAMES = FALSE)\n  return(l[[1]] + l[[2]] + l[[3]] + l[[4]] + l[[5]] + l[[6]])\n}\n\n# Funktion zur Berechnung der Umkehrabbildung\nlotto_umkehrabbildung &lt;- function(n) {\n  a &lt;- integer(6)\n  for (i in 6:1) {\n    a[i] &lt;- max(which(chooseZ(0:49, i) &lt;= n))\n    n &lt;- n - chooseZ(a[i]-1, i)\n  }\n  return(a)\n}\n\n\n\n\n# Python-Code zur Berechnung der Abbildung und Umkehrabbildung\nfrom math import comb\n\n# Funktion zur Berechnung der Abbildung\ndef lotto_abbildung(a):\n    l = [comb(a[i] - 1, i + 1) for i in range(6)]\n    return sum(l)\n\n# Funktion zur Berechnung der Umkehrabbildung\ndef lotto_umkehrabbildung(n):\n    a = [0] * 6\n    for i in range(5, -1, -1):\n        tmp = max(k for k in range(50) if comb(k, i + 1) &lt;= n)\n        a[i] = 1 + tmp\n        n -= comb(tmp, i + 1)\n    return a\n\n\n\n\n\n\nBeispiel\n\nRPython\n\n\n\ngezogene_zahlen &lt;- c(5, 12, 23, 34, 45, 49)\nabbildung &lt;- lotto_abbildung(gezogene_zahlen)\numkehrabbildung &lt;- lotto_umkehrabbildung(abbildung)\n\n\n\n\ngezogene_zahlen = [5, 12, 23, 34, 45, 49]\nabbildung = lotto_abbildung(gezogene_zahlen)\numkehrabbildung = lotto_umkehrabbildung(abbildung)\n\n\n\n\nDie Ausgabe zeigt die eindeutige Nummer der gezogenen Zahlen und die zur√ºckgewonnenen Zahlen\n\nRPython\n\n\n\n# Ausgabe\nabbildung  # Eindeutige Nummer\n\nBig Integer ('bigz') :\n[1] 13400039\n\numkehrabbildung  # Zur√ºckgewonnene Zahlen\n\n[1]  5 12 23 34 45 49\n\n\n\n\n\n# Ausgabe\nprint(abbildung)  # Eindeutige Nummer\n\n13400039\n\nprint(umkehrabbildung)  # Zur√ºckgewonnene Zahlen\n\n[5, 12, 23, 34, 45, 49]\n\n\n\n\n\nDie Ausgabe zeigt die eindeutige Nummer der gezogenen Zahlen und die zur√ºckgewonnenen Zahlen.\nDie Frage bleibt, geht es auch ohne (extra) Scheifen bei der Suche nach dem n√§chsten \\(a_i\\) in der Umkehrabbildung?\n\n\nReferenzen\n\nWikipedia: Lotto (Spiel)\nWikipedia: Binomialkoeffizient\nKombinatorik und Wahrscheinlichkeitsrechnung"
  },
  {
    "objectID": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "href": "posts/csv-dateien-bearbeiten-mit-miller/index.html",
    "title": "CSV Dateien bearbeiten mit Miller",
    "section": "",
    "text": "Miller beschreibt sich selbst folgenderma√üen:\n\nMiller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. You get to work with your data using named fields, without needing to count positional column indices.\n\nMiller kombiniert die Funktionalit√§t von awk, sed und cut und eignet sich besonders f√ºr feldbasierte Datenmanipulation.\n\nBeispiel 1: Erstellung einer Markdown-Tabelle\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --omd cat\nMit diesem Befehl wird eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, anschlie√üend werden alle Kommata durch Punkte ersetzt und schlie√ülich wird daraus eine Markdown-Tabelle erzeugt.\n\n\nBeispiel 2: Gerahmte Darstellung\n&gt; mlr --icsv --fs semicolon --otsv cat tips.csv | \\\n  sed 's/,/./g' | mlr --itsv --opprint --barred cat\nMit diesem Befehl wird ebenfalls eine durch Semikolon getrennte CSV-Datei in eine durch Tabulator getrennte TSV-Datei umgewandelt, alle Kommata werden durch Punkte ersetzt und am Ende wird eine eingerahmte Tabelle erzeugt."
  },
  {
    "objectID": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html",
    "href": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html",
    "title": "Quarto-Dokumente mit WebP anstatt PNG Bilddateien erstellen",
    "section": "",
    "text": "Ich bin √ºber die Quarto-Extension von KLaus Brunner mit dem Namen Convert PNG to WebP (Quarto Extension) gestolpert und nutze diesen nun regelm√§ssig.\nDaf√ºr habe ich auf meinem Mac via HomeBrew die Application ‚Äúwebp‚Äù installiert.\n&gt; brew install webp\nDamit habe ich das Programm cwebp installiert. Zum Testen nutze ich:\n&gt; cwebp -version\nAktuell bekomme ich da die Ausgabe:\n1.6.0\nlibsharpyuv: 0.4.2\nDas Programm ersetzt die erzeugten Bild-Ausgaben, welche normalerweise im PNG-Format erzeugt werden, durch welche im WebP-Format.\nDie KI sagt:\n\nWebP ist ein modernes Bildformat von Google, das eine effizientere Komprimierung als herk√∂mmliche Formate wie JPEG und PNG erm√∂glicht, um Bilder im Web kleiner zu machen und Ladezeiten zu beschleunigen. Es unterst√ºtzt sowohl verlustfreie als auch verlustbehaftete Komprimierung sowie Transparenz und Animationen, √§hnlich wie PNG und GIF. WebP wird von den meisten modernen Browsern unterst√ºtzt, aber f√ºr √§ltere Browser ist es ratsam, eine alternative Bildversion bereitzustellen\n\n\n\nWir installieren im Quarto-Projekt einfach mittels:\n&gt; quarto add klausbrunner/convert-to-webp\nDanach nutzen wir die Datei _quarto.yml und f√ºgen die folgenden Zeilen hinzu:\nfilters:\n  - convert-to-webp\n\nwebp-delete-originals: false\nwebp-disable: false\nJetzt wird nach jeden ‚Äòrendern‚Äô eines Dokuments die erzeugten PNG-Dateien in WebP-Dateien √ºbersetzt und im HTML-Code ersetzt."
  },
  {
    "objectID": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html#installation-im-quarto-projekt",
    "href": "posts/2025-12-03-Quarto-Dokumente-mit-webp-anstatt-png/index.html#installation-im-quarto-projekt",
    "title": "Quarto-Dokumente mit WebP anstatt PNG Bilddateien erstellen",
    "section": "",
    "text": "Wir installieren im Quarto-Projekt einfach mittels:\n&gt; quarto add klausbrunner/convert-to-webp\nDanach nutzen wir die Datei _quarto.yml und f√ºgen die folgenden Zeilen hinzu:\nfilters:\n  - convert-to-webp\n\nwebp-delete-originals: false\nwebp-disable: false\nJetzt wird nach jeden ‚Äòrendern‚Äô eines Dokuments die erzeugten PNG-Dateien in WebP-Dateien √ºbersetzt und im HTML-Code ersetzt."
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html",
    "href": "posts/2020-06-29-cook-abstand/index.html",
    "title": "Cook Abstand",
    "section": "",
    "text": "Frage: Was macht einen Wert zum Ausrei√üer?\nEine m√∂gliche Antwort lautet: Ein Wert gilt als Ausrei√üer, wenn er deutlich von den √ºbrigen Werten abweicht und einen (starken) Einfluss auf das Modell aus√ºbt.\nEin Verfahren zur Identifikation solcher Ausrei√üer ist der Cook-Abstand (engl.: Cook‚Äôs distance). Die zugrunde liegende Idee besteht darin zu messen, wie stark ein einzelner Wert das Modell beeinflusst. Dazu vergleicht man das Modell einmal mit und einmal ohne diesen Wert.\nSehen wir uns den Cook-Abstand anhand eines einfachen linearen Regressionsmodells n√§her an:"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#unser-modell",
    "href": "posts/2020-06-29-cook-abstand/index.html#unser-modell",
    "title": "Cook Abstand",
    "section": "Unser Modell:",
    "text": "Unser Modell:\nZuerst ein Streudiagramm zur Visualisierung der Daten:\n\ngf_point(tip ~ total_bill, data = tips)\n\n\n\n\n\n\n\n\nDann erstellen wir ein lineares Regressionsmodell:\n\nerg_lm &lt;- lm(tip ~ total_bill, data = tips)\nsummary(erg_lm)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1982 -0.5652 -0.0974  0.4863  3.7434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.920270   0.159735   5.761 2.53e-08 ***\ntotal_bill  0.105025   0.007365  14.260  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.022 on 242 degrees of freedom\nMultiple R-squared:  0.4566,    Adjusted R-squared:  0.4544 \nF-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16\n\n\nVisualisierung der Regressionsgeraden:\n\ngf_point(tip ~ total_bill, data = tips) %&gt;%\n  gf_coefline(\n    model = erg_lm,\n    color = ~ \"Regressionsgerade\",\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nEinflussreiche Ausrei√üer k√∂nnen bei linearen Modellen problematisch sein. Was passiert, wenn wir einen potenziellen Ausrei√üer entfernen?\nBeispiel: Wir eliminieren die Beobachtung mit dem Index 173\n\ntips %&gt;% slice(173) -&gt; tips_removed\ntips_removed\n\n  total_bill  tip\n1       7.25 5.15\n\n\n\ntips %&gt;% slice(-173) -&gt; tips_red\nerg_lm_red &lt;- lm(tip ~ total_bill, data = tips_red)\nsummary(erg_lm_red)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips_red)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2136 -0.5351 -0.0818  0.4951  3.6869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.86065    0.15709   5.479 1.08e-07 ***\ntotal_bill   0.10731    0.00723  14.843  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9992 on 241 degrees of freedom\nMultiple R-squared:  0.4776,    Adjusted R-squared:  0.4754 \nF-statistic: 220.3 on 1 and 241 DF,  p-value: &lt; 2.2e-16\n\n\nGrafischer Vergleich:\n\ngf_point(tip ~ total_bill, data = tips_red) %&gt;%\n  gf_coefline(\n    model = erg_lm,\n    color = ~ \"Regressionsgerade\"\n    ) %&gt;%\n  gf_point(\n    tip ~ total_bill, \n    colour = ~ \"Entfernter Punkt\",\n    data = tips_removed)"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#berechnung-des-cook-abstands",
    "href": "posts/2020-06-29-cook-abstand/index.html#berechnung-des-cook-abstands",
    "title": "Cook Abstand",
    "section": "Berechnung des Cook-Abstands",
    "text": "Berechnung des Cook-Abstands\nWir vergleichen die Prognosen beider Modelle:\n\nnew_data &lt;- tibble(total_bill = tips$total_bill)\nprognose_lm &lt;- predict(erg_lm, newdata = new_data)\nprognose_lm_red &lt;- predict(erg_lm_red, newdata = new_data)\n\nBerechnung:\n\\[\nd_j = \\sum_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2\n\\] Dabei ist \\(\\hat{y}_i\\) die Prognose des Wertes \\(y_i\\) auf Basis von \\(x_i\\) mit dem Originalmodell und \\(\\hat{y}_{i(j)}\\) die Prognose wenn man die \\(j\\)-te Beobachtung aus dem Modell gestrichen hat.\n\nd_j &lt;- sum((prognose_lm - prognose_lm_red)^2)\nd_j\n\n[1] 0.1511406\n\n\nDer Cook Abstand \\(D_j\\) wird nun noch normiert durch \\[{\\text{var}_{\\text{cook}}} = p \\cdot s_{\\epsilon_i^2}^2\\] Dabei ist \\(s_{\\epsilon_i^2}^2\\) der erwartungstreue Sch√§tzer der Varianz der Residuen und \\(p\\) die Anzahl aller erkl√§renden Variablen plus Eins, also: $ 1 + 1 = 2$.\nNormierung des Cook-Abstands:\n\\[\n  D_j = \\frac{d_j}{\\text{var}_{\\text{cook}}} = \\frac{\\sum\\limits_{i=1}^n \\left(\\hat{y}_i - \\hat{y}_{i(j)}\\right)^2}{p \\cdot s_{\\epsilon_i^2}^2}\n\\]\n\n# Summary des Modells\nselm &lt;- summary(erg_lm)\n\n# Wir finden p als rank im Modell\np &lt;- erg_lm$rank \n\n# Wir finden den erwatungtreuen Sch√§tzer im Summary des Modells\ns_quad_eps_quad &lt;- (selm$sigma)^2 \n\nvar_cook = p * s_quad_eps_quad\n\nD_j = d_j / var_cook\nD_j\n\n[1] 0.07234504\n\n\nAlternativ kann der Wert direkt mit cooks.distance() berechnet werden:\n\ncooks.distance(erg_lm)[173]\n\n       173 \n0.07234504 \n\n\nWann aber ist nun ein Wert ein einflussreicher Ausrei√üer?"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#entscheidungskriterien",
    "href": "posts/2020-06-29-cook-abstand/index.html#entscheidungskriterien",
    "title": "Cook Abstand",
    "section": "Entscheidungskriterien",
    "text": "Entscheidungskriterien\nCook selber gibt daf√ºr die Bedingung \\(D_j &gt; 1\\) an. Andere Autor*innen schreiben \\(D_j &gt; 4/n\\), wobei \\(n\\) die Anzahl der Beobachtung ist.\nIn unserem Beispiel liefert die Variante \\(D_j &gt; 1\\)\n\ncooks &lt;- cooks.distance(erg_lm)\nnames(cooks) &lt;- NULL\nn &lt;- nrow(tips)\n\nany(cooks &gt; 1)\n\n[1] FALSE\n\n\nkeinen Ausrei√üer.\nWenn wir jedoch mit \\(D_j &gt; 4/n\\) suchen .\n\nany(cooks &gt; 4/n)\n\n[1] TRUE\n\n\ndann gibt es Ausrei√üer.\nDie Indices dieser finden wir mit:\n\nwhich(cooks &gt; 4/n)\n\n [1]  24  48  57 103 142 157 171 173 179 183 184 185 188 208 211 213 215 238\n\n\nDaten bereinigen:\n\nremove &lt;- which(cooks &gt; 4/n)\ntips %&gt;% slice(-remove) -&gt; tips_no_outliers\ntips %&gt;% slice(remove) -&gt; tips_removed\nerg_lm_no_outliers &lt;- lm(tip ~ total_bill, data = tips_no_outliers)\n\nErgebnis des Modells:\n\nsummary(erg_lm_no_outliers)\n\n\nCall:\nlm(formula = tip ~ total_bill, data = tips_no_outliers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.22592 -0.48166 -0.06794  0.46992  2.31414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.773324   0.139435   5.546  8.2e-08 ***\ntotal_bill  0.111799   0.006958  16.069  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7778 on 224 degrees of freedom\nMultiple R-squared:  0.5355,    Adjusted R-squared:  0.5334 \nF-statistic: 258.2 on 1 and 224 DF,  p-value: &lt; 2.2e-16\n\n\nDaten bereinigen:\n\ngf_point(tip ~ total_bill, data = erg_lm_no_outliers) %&gt;%\n  gf_coefline(\n    model = erg_lm_no_outliers, \n    color = ~\"Regressionsgerade\"\n  )\n\n\n\n\n\n\n\n\nGrafischer Vergleich beider Modelle:\n\ngf_point(tip ~ total_bill, data = erg_lm) %&gt;%\n  gf_coefline(\n    model =  erg_lm,\n    color = ~ \"Regressionsgerade (Orginal)\"\n  ) %&gt;%\n  gf_coefline(\n    model = erg_lm_no_outliers,\n    color = ~ \"Regressionsgerade (No Outliers)\"\n  ) %&gt;%\n  gf_point(\n    tip ~ total_bill,\n    color = ~ \"Entfernte Punkte\",\n    data = tips_removed\n  )"
  },
  {
    "objectID": "posts/2020-06-29-cook-abstand/index.html#modelle-in-gleichungsform",
    "href": "posts/2020-06-29-cook-abstand/index.html#modelle-in-gleichungsform",
    "title": "Cook Abstand",
    "section": "Modelle in Gleichungsform",
    "text": "Modelle in Gleichungsform\nDas urspr√ºngliche Modell:\n\\[\n  \\widehat{tips}_{lm} = 0.9202696 + 0.1050245 \\cdot total\\_bill\n\\]\nDas um pot. Ausrei√üer bereinigte Modell:\n\\[\n  \\widehat{tips}_{lm\\_no} 0.7733236 + 0.1117985 \\cdot total\\_bill\n\\]"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html",
    "title": "Regression mit studentisierten Daten",
    "section": "",
    "text": "Bei einer einfachen linearen Regression versuchen wir zu vorgegebenen Datenpunkten \\((x_1, y_1), \\cdots (x_n, y_n)\\) die Parameter einer m√∂glichst passenden Gerade \\(g(x)=\\beta_0 + \\beta_1 \\cdot x\\) zu sch√§tzen.\nDie Sch√§tzung des y-Achsenabschnitts \\(\\hat\\beta_0\\) und der Steigung \\(\\hat\\beta_1\\) erfolgt dabei algebraisch exakt mittels:\n\\[\n\\hat\\beta_0 = \\bar{y} - \\hat\\beta_1 \\cdot \\bar{x} \\quad\\text{und}\\quad \\hat\\beta_1 = \\frac{s_x}{s_y}\\cdot r_{x,y}\n\\]\nDabei sind \\(\\bar{x}\\) bzw. \\(\\bar{y}\\) die Mittelwerte und \\(s_x\\) bzw. \\(s_y\\) die Standardabweichungen der Datenpunkte \\(x_i\\) bzw. \\(y_i\\); dar√ºberhinaus ist \\(r_{x,y}\\) der Korrelationskoeffizient der Datenpunkte.\nBeim studentisieren werden die Datenpunkte bzgl. des Mittelwertes zentriert und bzgl der Standardabweichung normiert:\n\\[\nx_i^{\\text{stud}} = \\frac{x_i-\\bar{x}}{s_x} \\quad\\text{bzw.}\\quad y_i^{\\text{stud}} = \\frac{y_i-\\bar{y}}{s_y}\n\\]\nWas passiert nun durch eine solche Studentisierung (oft auch z-Transformation genannt) mit den gesch√§tzen Parametern?\nDie Mittelwerte \\(\\bar{x}^{stud}\\) und \\(\\bar{y}^{stud}\\) werden zu Null. Die Standardabweichungen \\(s_{x^{stud}}\\) und \\(s_{y^stud}\\) werden zur Eins:\n\\[\n\\bar{x}^{stud}=0=\\bar{y}^{stud} \\qquad s_{x^{stud}}= 1 = s_{y^{stud}}\n\\]\nDer y-Achsenabschnitt wird nun durch\n\\[\n\\hat\\beta_0^{stud}\n= \\bar{y}^{stud} - \\hat\\beta_1^{stud} \\cdot \\bar{x}^{stud}\n= 0 - \\hat\\beta_1^{stud} \\cdot 0 = 0\n\\]\nund die Steigung durch\n\\[\n\\hat\\beta_1^{stud}\n= \\frac{s_{x^{stud}}}{s_{y^{stud}}}\\cdot r_{x^{stud},y^{stud}}\n= \\frac{1}{1}\\cdot r_{x^{stud},y^{stud}} = r_{x^{stud},y^{stud}}\n\\]\ngesch√§tzt.\nF√ºr den Korrelationskoeffienten gilt nun\n\\[\nr_{x^{stud},y^{stud}}\n= \\frac{s_{x^{stud},y^{stud}}}{s_{x^{stud}}\\cdot_{y^{stud}}}\n= \\frac{s_{x^{stud},y^{stud}}}{1 \\cdot 1}\n= s_{x^{stud},y^{stud}}.\n\\]\nDamit Sch√§tzen wir unsere Steigung \\(\\hat\\beta_1^{stud}\\) direkt aus der Kovarianz \\(s_{x^{stud},y^{stud}}\\).\nDamit gilt:\n\\[\n\\hat\\beta_1^{stud} = r_{x^{stud},y^{stud}} = s_{x^{stud},y^{stud}} \\in [-1, 1]\n\\]\nIn Worten zusammengefasst: Im studentisierten Fall ist"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#reproduzierbarkeitsinformationen",
    "title": "Regression mit studentisierten Daten",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#footnotes",
    "href": "posts/2021-06-23-regression-mit-studentisierten-daten/index.html#footnotes",
    "title": "Regression mit studentisierten Daten",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\nDas ‚ÄúCookbook‚Äù zur Datentabelle k√∂nnen Sie mit Hilfe von help(\"mtcars\") aufrufen!‚Ü©Ô∏é\nSie k√∂nnen hier auch die Funktion scale() verwenden!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-06-11-wege-zur-normalverteilung/index.html",
    "href": "posts/2021-06-11-wege-zur-normalverteilung/index.html",
    "title": "Wege zur Normalverteilung",
    "section": "",
    "text": "Der fairen Wurf einer fairen M√ºnze, also eine M√ºnze bei der Kopf und Zahl gleich wahrscheinlich geworfen wird, sei der Ausgang des ersten Weges.\nWir k√∂nnen den M√ºnzwurf mit R simulieren:\nlibrary(mosaic)\nset.seed(2009)\n\nrflip(1)\n\n\nFlipping 1 coin [ Prob(Heads) = 0.5 ] ...\n\nT\n\nNumber of Heads: 0 [Proportion Heads: 0]\nGenauso wie den Wurf zweier M√ºnzen:\nrflip(2)\n\n\nFlipping 2 coins [ Prob(Heads) = 0.5 ] ...\n\nH H\n\nNumber of Heads: 2 [Proportion Heads: 1]\nOder auch von 20 M√ºnzw√ºrfen:\nrflip(20)\n\n\nFlipping 20 coins [ Prob(Heads) = 0.5 ] ...\n\nT T T T H H T T T H H H H T H T T H H H\n\nNumber of Heads: 10 [Proportion Heads: 0.5]\nWir wollen uns daf√ºr interessieren, wie der Zufall auf jeweils \\(n\\) M√ºnzw√ºrfe einwirkt\nUnd wieder holen daf√ºr die drei Experiment jeweils \\(N=10^{4}\\) mal und schauen uns danach anwie die Anzahl der Kopf W√ºrfe variiert:\nN &lt;- 10000\nn &lt;- 1\nvrtlg_1 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_1) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5,n + 0.5)))\n\\(n=2\\)\nn &lt;- 2\nvrtlg_2 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_2) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\n\\(n=20\\)\nn &lt;- 20\nvrtlg_20 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_20) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\n\\(n=50\\)\nn &lt;- 50\nvrtlg_50 &lt;- do(N) * rflip(n)\ngf_bar(~ heads, data = vrtlg_50) %&gt;%\n  gf_refine(scale_x_continuous(breaks = 0:n, limits = c(-0.5, n + 0.5)))\nDer faire Wurf eine fairen M√ºnze \\(X\\) ist aber vorallem ein Gedanken-Expermiment, bei dem wir davon ausgehen, dass die Wahrscheinlichkeit f√ºr Kopf gleich der Wahrscheinlichkeit f√ºr Zahl ist:\n\\[\nP(X = \\text{\"Kopf\"}) = P(X = \\text{\"Zahl\"}) = 50\\,\\% = 0{,}5\n\\]\nWir wollen die beiden Ergebnisse kodieren: \\(\\text{\"Kopf\"}\\) mit \\(1\\) und \\(\\text{\"Zahl\"}\\) mit \\(0\\). Somit k√∂nnen wir schreiben:\n\\[\nP(X = 0)  = 0{,}5 = P(X = 1)\n\\]\nF√ºr denn Fall, dass die M√ºnze nicht mehr fair ist wollen wir vereinbaren, dass wir mit \\(q = P(X = 0)\\) und \\(p = P(X = 1)\\) die jeweiligen Wahrscheinlichkeiten bezeichnen wollen. Es gilt aber immer, dass \\(q+p = 1\\) ist!\nEine Variable \\(X\\) die dem Zufall ein Wert \\(x\\) zuweist, wollen wir Zufallsvariable nennen.1\nAusgehen von der Annahme k√∂nnen wir uns diese theoretischen Verteilungen auch einmal ansehen:\np = 0.5\nn &lt;- 1\ngf_dist(\"binom\", size = n, prob = p)\nn &lt;- 2\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_2)\ndist_2 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_2, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 20\ngf_dist(\"binom\", size = n, prob = p)\nn &lt;- 20\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_20)\ndist_20 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_20, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 50\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_50)\ndist_50 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = 0.5) %&gt;%\n  gf_point(density ~ x, data = dist_50, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 100\nN &lt;- 5 * n**2\nvrtlg_100 &lt;- do(N) * rflip(n)\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_100)\ndist_100 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_100, color = \"lightgreen\", alpha = 0.7)\nn &lt;- 300\nN &lt;- 5 * n**2\nvrtlg_300 &lt;- do(N) * rflip(n)\ntab &lt;- tally( ~ heads, format = \"proportion\", data = vrtlg_300)\ndist_300 &lt;- data.frame(\n  x = as.numeric(names(tab)),\n  density = as.numeric(tab)\n) \n\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_300, color = \"lightgreen\", alpha = 0.7)\nZeichnen wir nun die Gau√ü‚Äôsche Glockenkurve in rot dazu:\ngf_dist(\"binom\", size = n, prob = p) %&gt;%\n  gf_point(density ~ x, data = dist_300, color = \"lightgreen\", alpha = 0.7) %&gt;%\n  gf_dist(\"norm\", mean &lt;- n*p, sd = sqrt(n*p*(1 - p)), color = \"red\")\nDie Gau√ü‚Äôsche Glockenkurve ist die Dichtefunktion der Normalverteilung und ist definiert durch:\n\\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\nMit den beiden Parameter \\(\\mu\\) und \\(\\sigma^2\\) kann man den Mittelwert der Verteilung (auch Erwartungswert genannt) und die Varianz der Verteilung einstellen.\nWir haben diese Werte oben mit den theoretischen Werten der Binomialverteilung \\(\\mu = E[X] = p \\cdot n\\) und \\(\\sigma = \\sqrt{\\sigma^2}= \\sqrt{Var[X]} = \\sqrt{n \\cdot p \\cdot (1-p)}\\) belegt.\nWir sehen, die (simulierten) relativen H√§ufigkeiten der M√ºnzw√ºrfe streben mit steigendem \\(N\\) mehr und mehr in Richtung der (theoretischen) Wahrscheinlichkeiten der Binomialverteilung und diese (mit steigendem \\(n\\)) gegen die Gau√ü‚Äôsche Glockenkurve der Normalverteilung.\nEine Verteilungsfunktion \\(F(x)\\) gibt an, wie wahrscheinlich es ist, einen Wert \\(\\leq x\\) zu beobachten:\n\\[\nF(x) = P(X \\leq x)\n\\]\nNat√ºrlich ist damit immer \\(0 \\leq F(x) \\leq 1\\).\nEine empirische Verteilungsfunktion \\(F_n(x)\\)gibt an, wie gro√ü die relative H√§ufigkeit des eintretens von Werten \\(\\leq x\\) bei einem Stichprobenumfang von \\(n\\) waren: \\[F_n(x) = \\frac{\\text{Anzahl der Werte} \\leq x}{n}\\]\nBetrachen wir nun empirische Verteilungsfunktion unserer Experimente:\nn = 2\ngf_ecdf( ~ heads, data=vrtlg_2)\nn = 20\ngf_ecdf( ~ heads, data=vrtlg_20)\nn = 50\ngf_ecdf( ~ heads, data=vrtlg_50)\nn = 300\ngf_ecdf( ~ heads, data=vrtlg_300)\nTragen wir zur empirische Verteilungsfunktion auch die Verteilungsfunktion der Normalverteilung von oben ein:\nn &lt;- 300\ngf_ecdf( ~ heads, data = vrtlg_300) %&gt;%\n  gf_dist(\"norm\", mean &lt;- n*p, sd = sqrt(n*p*(1 - p)), kind=\"cdf\", color = \"red\")\nDie empirische Verteilungsfunktion strebt also gegen die (theoretisch) Verteilungsfunktion der Normalverteilung. Sie lautet:\n\\[\nF(x) = \\int_{-\\infty}^x f(u)\\, \\text{d} u = \\int_{-\\infty}^x\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "posts/2021-06-11-wege-zur-normalverteilung/index.html#footnotes",
    "href": "posts/2021-06-11-wege-zur-normalverteilung/index.html#footnotes",
    "title": "Wege zur Normalverteilung",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\nEigentlich handelt es sich damit strenggenommen um eine Funktion. Und es m√ºssen noch weitere Eigenschaften erf√ºllt sein. Aber darauf gehen wir hier nicht weiter ein.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabh√§ngige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nF√ºr die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt f√ºr die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie f√ºr wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ f√ºr }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#der-zentrale-grenzwertsatz-der-statistik-bei-identischer-verteilung.",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "",
    "text": "Seien \\(X_1, X_2, ..., X_n\\) unabh√§ngige und identisch verteilte Zufallsvariablen mit bekanntem Erwartungswert \\(E(X_i) = \\mu\\) und bekannter Varianz \\(Var(X_i)=\\sigma^2\\).\nF√ºr die Summe \\(S_n = \\sum_{i=1}^n X_i\\) ist dann der Erwartungswert \\(E(S_n)= n \\cdot \\mu\\) und die Varianz \\(Var(S_n)= n \\cdot \\sigma^2\\).\nDann gilt f√ºr die standardisierte Zufallsvariable\n\\[\n\\begin{align*}\nZ_n &= \\frac{\\left(\\sum\\limits_{i=1}^n X_i\\right) - n \\cdot \\mu}{\\sqrt{n\\cdot \\sigma^2}}\n    = \\frac{S_n - n \\cdot \\mu}{\\sigma \\cdot \\sqrt{n}}\n    = \\frac{n \\cdot \\bar{X}_n-n \\cdot \\mu}{\\sigma \\cdot n / \\sqrt{n}} \\\\\n    &= \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\n    = \\frac{\\bar{X}_n - \\mu}{\\sigma} \\cdot \\sqrt{n},\n\\end{align*}\n\\]\ndass sie f√ºr wachsendes \\(n\\) immer besser durch die Standardnormalverteilung \\(N(0, 1)\\) approximiert werden kann.\nMit anderen Worten:\n\\[\nP(Z_n \\leq x) \\longrightarrow \\Phi(x), \\quad \\text{ f√ºr }\\; n \\rightarrow \\infty\n\\]"
  },
  {
    "objectID": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "href": "posts/2017-04-05-der-zentrale-grenzwertsatz/index.html#ein-beispiel",
    "title": "Der Zentrale Grenzwertsatz",
    "section": "Ein Beispiel:",
    "text": "Ein Beispiel:\nNehmen wir drei Verteilungen mit Zufallsvariable \\(U\\), \\(X\\), \\(Y\\) und jeweils \\(n\\) Realisationen \\(u_1,\\dots, u_n\\), \\(x_1,\\dots, x_n\\), \\(y_1,\\dots, y_n\\).\nW√§hlen wir zun√§chst \\(n=5\\):\n\nu\n\n[1] 19.726 69.683 60.790  0.955 42.901\n\nx\n\n[1]  7.942 15.905 12.917  6.818  4.434\n\ny\n\n[1] 59.961 56.552 51.094 75.288 47.985\n\n\nStandardisieren wir die Werte:\n\nlibrary(mosaic)\nzscore(u)\n\n[1] -0.6695256  1.0830283  0.7710507 -1.3280357  0.1434823\n\nzscore(x)\n\n[1] -0.3543069  1.3440714  0.7067796 -0.5940379 -1.1025063\n\nzscore(y)\n\n[1]  0.1677971 -0.1526624 -0.6657361  1.6085958 -0.9579944\n\n\nDie Behauptung des Zentralengrenzwertsatzes ist nun, dass mit steigender Anzahl an Werten \\(n\\) die standardisierten Werte in der empirischen Verteilungsfunktion sich immer mehr der Verteilungsfunktion der Standardnormalverteilung ann√§hern:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeiterf√ºhrende Literatur und Quellen dieses Eintrags:"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "",
    "text": "Bei einer einfachen Regression versuchen wir zu gegebenen Datenpunkten \\((x_1, y_1), ..., (x_n, y_n)\\) eine m√∂glichst passende Funktion \\(g(x)\\) zu finden, so dass \\[y_i = g(x_i) + e_i\\] gilt. Dabei tolerieren wir eine (kleine) Abweichung \\(e_i\\).\nBei einer einfachen linearen Regression gehen wir davon aus, dass die Datenpunkte (im wesentlichen) auf einer Geraden liegen. Mit \\(g(x)=\\beta_0 + \\beta1 \\cdot x\\) ergibt sich dann f√ºr die Datenpunkte die Gleichung:\n\\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i\\]\nUnsere Aufgabe besteht nun darin die Parameter \\(\\beta_0\\) (y-Achsenabschnitt) und \\(\\beta_1\\) (Steigung) an Hand der \\(n\\) Datenpunkte zu sch√§tzen. Alle unsere Sch√§tzungen kennzeichnen wir mit einem Dach (\\(\\hat{.}\\)), um sie von den (in der Regel unbekannten) Parametern besser zu unterscheiden.\nWir suchen somit nach \\(\\hat\\beta= \\left(\\hat\\beta_0,\\, \\hat\\beta_1\\right)\\), so dass die Gerade \\(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x\\) zu gegebenem \\(x_i\\) eine m√∂glichst gute Sch√§tzung von \\(y_i\\) (genannt \\(\\hat{y}_i\\)) hat:\n\\[\n\\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i\n\\]\nDie Abweichung \\(\\hat{e_i}\\) unserer Sch√§tzung \\(\\hat{y}_i\\) von dem gegebenen Wert \\(y_i\\) l√§sst sich schreiben als:\n\\[\n\\hat{e_i} =  \\hat{y_i} - y_i =  \\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i\n\\]\nWenn wir diese Abweichung √ºber alle \\(i\\) minimieren, finden wir unser \\(\\hat\\beta\\).\nDoch das wirft eine Frage auf: Wie genau messen wir die m√∂glichst kleinste Abweichung der \\(\\hat{e_i}\\) konkret?\nWir betrachten zun√§chst drei einfache Ideen:\nGew√∂hnlich nutzen wir die quadratischen Abweichungen, weshalb wir die drei Ideen ebenso in umgekehrter Reihenfolge betrachten wollen:"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-quadratischen-abweichungen",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "3. Idee: Summe der quadratischen Abweichungen",
    "text": "3. Idee: Summe der quadratischen Abweichungen\nWir bezeichnen mit\n\\[\\begin{aligned}\nQS &= QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n  &= \\sum\\limits_{i=1}^n \\hat{e_i}^2 = \\sum\\limits_{i=1}^n \\left(\\hat{y_i} - y_i \\right)^2 \\\\\n  &= \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\n\\end{aligned}\\]\ndie Quadrat-Summe der Abweichungen.\nGesucht wird \\(\\hat\\beta=\\left(\\hat\\beta_0,\\,\\hat\\beta_1\\right)\\), so das \\(QS\\) minimiert wird.\nDies ist ein Minimierungsproblem, bei dem wir zu mindestens eine (exakte) mathematisch-algebraisch L√∂sung in Form eines station√§ren Punktes finden k√∂nnen. Dazu berechnen wir die Nullstelle der ersten partiellen Ableitung von \\(QS\\) nach \\(\\hat\\beta_0\\) bzw. \\(\\hat\\beta_1\\).\n\nVorbemerkungen\nWegen \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i\\) ist \\(n \\cdot \\bar{x} =\\sum\\limits_{i=1}^n x_i\\) und analog \\(n \\cdot \\bar{y} =\\sum\\limits_{i=1}^n y_i\\)\n\n\nSch√§tzen des y-Achenabschnitts \\(\\hat\\beta_0\\)\nEs ist:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot 1 \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 + \\sum\\limits_{i=1}^n\\hat\\beta_1 \\cdot x_i - \\sum\\limits_{i=1}^n y_i\\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot\\sum\\limits_{i=1}^n x_i - \\sum\\limits_{i=1}^n y_i \\right) \\\\\n  &= 2 \\cdot \\left( n \\cdot \\hat\\beta_0 + \\hat\\beta_1\\cdot n \\cdot \\bar{x} - n \\cdot\\bar{y} \\right) \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right)\n\\end{aligned}\\]\nUm station√§re Punkte zu ermitteln, m√ºssen wir den Ausdruck nun gleich Null setzen und erhalten:\n\\[\\begin{aligned}\n  0 &= \\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS \\\\\n  &= 2 \\cdot n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y} \\right) \\qquad | : (2 \\cdot n) \\\\\n  &= \\hat\\beta_0 + \\hat\\beta_1\\cdot \\bar{x} -\\bar{y}\n\\end{aligned}\\]\nStellen wir nach \\(\\hat\\beta_0\\) um, erhalten wir:\n\\[\\begin{aligned}\n  \\hat\\beta_0 &= - \\hat\\beta_1\\cdot\\bar{x} + \\bar{y} \\\\\n  \\hat\\beta_0 &= \\bar{y} - \\hat\\beta_1\\cdot\\bar{x}\n\\end{aligned}\\]\nUm \\(\\hat\\beta_0\\) zu bestimmen, ben√∂tigen wir \\(\\hat\\beta_1\\).\n\n\nSch√§tzen der Steigung \\(\\hat\\beta_1\\)\nEs ist:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right) \\cdot x_i \\\\\n  &= 2 \\cdot \\left(\\sum\\limits_{i=1}^n \\hat\\beta_0 \\cdot x_i + \\sum\\limits_{i=1}^n \\hat\\beta_1 \\cdot x_i\\cdot x_i- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot \\sum\\limits_{i=1}^n  x_i + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right)\n\\end{aligned}\\]\nWir ersetzen nun \\(\\hat\\beta_0\\) durch \\(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\) und erhalten:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  &=\n  2 \\cdot \\left(\\hat\\beta_0 \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(\\left(\\bar{y} - \\hat\\beta_1\\cdot \\bar{x}\\right) \\cdot n \\cdot \\bar{x} + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - n \\cdot \\hat\\beta_1 \\cdot  \\bar{x}^2  + \\hat\\beta_1 \\cdot\\sum\\limits_{i=1}^n  x_i^2- \\sum\\limits_{i=1}^n y_i \\cdot x_i\\right) \\\\\n  &= 2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n\\end{aligned}\\]\nMit Hilfe des Verschiebesatzes von Steiner (zweimal angewendet) erhalten wir:\n\\[\\begin{aligned}\n  \\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS  \n    &=2 \\cdot \\left(n \\cdot\\bar{y} \\cdot \\bar{x} - \\sum\\limits_{i=1}^n y_i \\cdot x_i  + \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)+ \\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)\\right) \\\\\n    &=2 \\cdot \\left(\\hat\\beta_1 \\cdot \\left(\\sum\\limits_{i=1}^n  x_i^2- n \\cdot  \\bar{x}^2\\right)- \\left(\\sum\\limits_{i=1}^n y_i \\cdot x_i - n \\cdot \\bar{y} \\cdot \\bar{x}   \\right)\\right) \\\\\n    &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)\n\\end{aligned}\\]\nWir setzen nun wieder den Ausdruck gleich Null:\n\\[\\begin{aligned}\n0 &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right)  \\qquad | : 2\\\\\n   &= \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{aligned}\\]\nUnd stellen dann nach \\(\\hat\\beta_1\\) um:\n\\[\\begin{aligned}\n  \\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\n    &= \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\\\\n  \\hat\\beta_1\n    &= \\frac{\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2}\n\\end{aligned}\\]\nWir k√∂nnen nun Z√§hler und Nenner der rechten Seite mit \\(\\frac{1}{n}\\) erweitern und erhalten so:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\\\\n\\end{aligned}\\]\nOder aber wir erweitern mit \\(\\frac{1}{n-1}\\) und erhalten:\n\\[\\begin{aligned}\n\\hat\\beta_1\n      &= \\frac{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})}{\\frac{1}{n-1} \\cdot\\sum\\limits_{i=1}^n  (x_i-\\bar{x})^2} \\\\\n      &= \\frac{s_{x,y}}{s^2_{x}}\n\\end{aligned}\\]\nDamit k√∂nnen wir zur Berechnung sowohl die Kovarianz der Grundgesamtheit \\(\\sigma_{x,y}\\) und die Varianz \\(\\sigma^2_x\\) von \\(x\\), als auch deren Sch√§tzer \\(s_{x,y}\\) und \\(s^2_x\\) verwendet werden!\nDiese Methode nennt sich Methode der kleinsten Quadrate (engl. ordenary least square method) und wir sprechen dann auch von den Kleinste-Quadrate-Sch√§tzern (oder kurz KQ-Sch√§tzer bzw. OLS-Sch√§tzer) \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\).\nErweitern wir den Ausdruck mit Standardabweichung \\(\\sigma_y\\) bzw. \\(s_y\\), so erhalten wir:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{\\sigma_{x,y}}{\\sigma^2_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_x} \\cdot \\frac{\\sigma_y}{\\sigma_y} = \\frac{\\sigma_{x,y}}{\\sigma_x \\cdot \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n&= \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x} \\\\\n\\end{aligned}\\]\nund analog f√ºr die Sch√§tzer:\n\\[\\begin{aligned}\n\\hat\\beta_1 &= \\frac{s_{x,y}}{s^2_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_x} \\cdot \\frac{s_y}{s_y}\n  = \\frac{s_{x,y}}{s_x \\cdot s_y} \\cdot \\frac{s_y}{s_x} \\\\\n&= r_{x,y} \\cdot \\frac{s_y}{s_x} \\\\\n\\end{aligned}\\]\nDie Steigung \\(\\hat\\beta_1\\) hat somit eine direkte Beziehung mit dem Korrelationskoeffizenten \\(\\rho\\) (der Grundgesamtheit) bzw. \\(r\\) (der Stichprobe).\nF√ºr eine Berechnung in R hei√üt dies: wir k√∂nnen die Regressionskoeffizienten \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\) direkt algebraisch ausrechnen, wenn wir\n\ndie Standardabweichungen von \\(x\\) und \\(y\\) und den Korrelationskoeffizienten oder\ndie Varianz von \\(x\\) und Kovarianz von \\(x\\) und \\(y\\)\n\nhaben.\n\n\nEin Beispiel in R:\nAuf Grundlage der Datentabelle mtcars wollen wir Pr√ºfen wie ein linearer Zusammenhang zwischen dem Verbrauch (in Meilen pro Gallone mpg) und der Leistung (Pferdest√§rke hp) modelliert werden kann.1\n\nlibrary(mosaic)\n\n# Wir nehmen die Datentabelle 'mtcars':\nmtcars %&gt;%\n  select(hp, mpg) -&gt; dt\n\n# Ein kurzer Blick auf die Daten:\nfavstats(~ hp, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  146.6875 68.56287\nfavstats(~ mpg, data = dt)[c(\"mean\",\"sd\")]\n#&gt;      mean       sd\n#&gt;  20.09062 6.026948\n\n# Wir vergleichen den Verbrauch (mpg, miles per gallon) \n# mit den Pferdest√§rken (hp) mit Hilfe eines Streudiagramms:\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir zun√§chst die Mittelwerte von \\(x\\) (also ‚Äòhp‚Äô) und \\(y\\) (also ‚Äòmpg‚Äô)\n\n(mean_hp &lt;- mean(~ hp, data = dt))\n#&gt; [1] 146.6875\n(mean_mpg &lt;- mean(~ mpg, data = dt))\n#&gt; [1] 20.09062\n\nund zeichnen die Punkt \\((\\bar{x}, \\bar{y}) = (146.69, 20.09)\\) in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nBerechnen wir nun die Sch√§tzwerte f√ºr die Regressionsgerade\n\n(beta_1 &lt;- cov(mpg ~ hp, data = dt) / var(~ hp, data = dt))\n#&gt; [1] -0.06822828\n(beta_0 &lt;- mean_mpg - beta_1 * mean_hp)\n#&gt; [1] 30.09886\n\nund zeichnen diese in unser Streudiagramm ein:\n\ngf_point(mpg ~ hp, data = dt) %&gt;%\n  gf_hline(yintercept = ~ mean_mpg, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_vline(xintercept = ~ mean_hp, color = \"grey60\", linetype = \"dashed\") %&gt;%\n  gf_point(mean_mpg ~ mean_hp, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_abline(slope = ~ beta_1, intercept = ~beta_0, color = \"dodgerblue\") %&gt;%\n  gf_lims(y = c(5,35))\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift f√ºr die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988605 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nStudentisieren ‚Äì einmal hin und einmal zur√ºck\nWas passiert eigentlich, wenn wir unsere \\(x\\) und \\(y\\) Werte studentisieren (aka standardisieren oder z-transformieren)?\nZur Erinnerung, studentisieren geht so: \\[\nx^{stud} = \\frac{x - \\bar{x}}{s_x}\n\\]\nIn R k√∂nnen wir das mit der Funktion ‚Äòzscore‚Äô wie folgt machen:\n\ndt %&gt;%\n  mutate(\n    hp_stud = zscore(hp),\n    mpg_stud = zscore(mpg)\n  ) -&gt; dt\n\nNat√ºrlich sind die Mittelwerte nun Null und die Standardabweichungen Eins:\n\nfavstats(~ hp_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;           mean sd\n#&gt;  -4.857226e-17  1\nfavstats(~ mpg_stud, data = dt)[c(\"mean\",\"sd\")]\n#&gt;          mean sd\n#&gt;  4.336809e-17  1\n\nDer Grund f√ºr die kleinen Abweichungen von der Null bei den Mittelwerten sind unumg√§ngliche Rundungsfehler, die der Computer macht!\nSchauen wir uns nun das Streudiagramm an, zusammen mit dem Mittelpunkt \\((0,0)\\)\n\ngf_point(mpg_stud ~ hp_stud, data = dt) %&gt;%\n  gf_point(0 ~ 0, color = \"red\", size = 5, alpha = 0.2) %&gt;%\n  gf_lims(y = c(-2, 2))\n\n\n\n\n\n\n\n\nAuch wenn die Skalierungen sich ge√§ndert haben, die Diagramme sind sehr √§hnlich.\nBestimmen wir die Koeffizienten der Regressionsgerade\n\n(beta_stud_1 &lt;- cov(mpg_stud ~ hp_stud, data = dt))\n#&gt; [1] -0.7761684\n(beta_stud_0 &lt;- 0 - beta_stud_1 * 0)\n#&gt; [1] 0\n\nund setzen sie in das Streudiagramm ein:\n\n\n\n\n\n\n\n\n\nWir k√∂nnen das studentisierte Problem auch wieder auf unser urspr√ºngliches zur√ºck rechnen.\nDie Regressionsgerade im studentisierten Problem lautet:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta^{stud}_0 + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761684 \\cdot x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot x^{stud}\n\\end{aligned}\n\\]\nRechnen wir nun mittels der Formel \\[\\hat\\beta_1 = \\hat\\beta_1^{stud} \\cdot \\frac{s_y}{s_x}\\]\ndie Steigung um, so erhalten wir:\n\n(b1 &lt;- beta_stud_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822828\n\nUnd setzen wir das in unsere Gleichung zur Bestimmung von \\(\\hat\\beta_0\\) ein:\n\n(b0 &lt;- mean(dt$mpg) - b1 * mean(dt$hp))\n#&gt; [1] 30.09886\n\nso erhalten wir die Sch√§tzwerte des urspr√ºnglichen Problem.\n\n\nEin anderer Weg um die Regressionskoeffizenten zu bestimmen‚Ä¶\nGehen wir das Problem noch einmal neu an. Wir suchen \\(\\hat\\beta=(\\hat\\beta_0, \\hat\\beta_1)\\) welches \\(QS(\\hat\\beta) = QS(\\hat\\beta_0, \\hat\\beta_1) = \\sum\\limits_{i=1}^n \\left(\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i \\right)^2\\) minimiert.\nStatt es direkt, wie oben durch Null setzen der partiellen Ableitungen, zu bestimmen, w√§hlen wir nun einen mathematisch-numerischen Ansatz und wollen \\(\\hat\\beta \\in \\mathbf{R}^2\\) als Optimierungsproblem mit Hilfe des Gradientenverfahrens l√∂sen.\nBeim Gradientenverfahren wird versucht, ausgehend von einem Startwert \\(\\hat\\beta^0 \\in \\mathbf{R}^2\\), gem√§√ü der Iterationsvorschrift\n\\[\n\\hat\\beta^{k+1} = \\hat\\beta^{k} + \\alpha^k \\cdot d^k\n\\]\nf√ºr alle \\(k=0,1, ...\\) eine N√§herungsl√∂sung f√ºr \\(\\hat\\beta\\) zu finden. Dabei ist \\(\\alpha^k &gt; 0\\) eine positive Schrittweite und \\(d^k\\in\\mathbf{R}^n\\) eine Abstiegsrichtung, welche wir in jedem Iterationsschritt \\(k\\) so bestimmen, dass die Folge \\(\\hat\\beta^k\\) zu einem station√§ren Punkt, unserer N√§herungsl√∂sung, konvergiert.\nIm einfachsten Fall, dem Verfahren des steilsten Abstieges, wird der Abstiegsvektor \\(d^k\\) aus dem Gradienten \\(\\nabla QS\\) wie folgt bestimmt:\n\\[\nd^k = -\\nabla QS\\left(\\hat\\beta^k\\right)\n\\]\nWegen \\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot n \\cdot \\left(  \\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y} \\right)\n\\]\nund\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS = 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y}) \\right)\n\\]\ngilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot(\\hat\\beta_0 + \\hat\\beta_1\\cdot\\bar{x} - \\bar{y})  \\\\\n\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\n\\end{pmatrix}\n\\end{aligned}\n\\]\nWir wollen hier von Anfang an mit den studentisierten Werten arbeiten, weil diese numerisch viele Vorteile haben. Darum vereinfachen sich die beiden partiellen Ableitungen noch einmal zu:\n\\[\n\\frac{\\partial}{\\partial \\hat\\beta_0} \\, QS = 2 \\cdot v\n\\]\nund\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\hat\\beta_1} \\, QS &= 2 \\cdot \\left(\\hat\\beta_1 \\cdot \\sum\\limits_{i=1}^n(x_i-\\bar{x})^2 - \\sum\\limits_{i=1}^n (x_i-\\bar{x}) \\cdot (y_i-\\bar{y})\\right) \\\\\n&= 2 \\cdot (n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{aligned}\n\\]\nSomit gilt:\n\\[\n\\begin{aligned}\n\\nabla QS(\\hat\\beta) &= \\nabla QS(\\hat\\beta_0, \\hat\\beta_1) \\\\\n&= 2 \\cdot \\begin{pmatrix}\nn \\cdot \\hat\\beta_0 \\\\\n(n-1) \\left(\\hat\\beta_1 \\cdot s^2_{x} - s_{x,y}\\right)\n\\end{pmatrix}\n\\end{aligned}\n\\]\nUm die Varianz und die Kovarianz nicht jedesmal neu zu berechnen, speichern wir die Ergebnisse vorab. Ebenso, damit der Quellcode k√ºrzer wird, speichern wir in \\(x\\) und \\(y\\) die studentisierten Werte von \\(hp\\) und \\(mpg\\):\n\n# Vorbereitungen \nvar_x &lt;- var(~ hp_stud, data = dt)\ncov_xy &lt;- cov(mpg_stud ~ hp_stud, data = dt)\n\nn &lt;- length(dt$hp_stud)\n\nx &lt;- dt$hp_stud\ny &lt;- dt$mpg_stud\n\nNun erstellen wir die \\(QS\\) und \\(\\nabla QS\\) Funktionen: Wir definieren diese Funktion wie folgt in R:\n\nqs &lt;- function(b_0, b_1) {\n  sum((b_1 * x - y)**2)\n}\n\nnabla_qs &lt;- function(b_0, b_1) {\n  c(2 * n * b_0,\n    2 * (n - 1) * (b_1 * var_x - cov_xy)\n  )\n}\n\nDie Schrittweite \\(alpha\\) bestimmen wir mit Hilfe der Armijo-Bedingung und der Backtracking Liniensuche: Diese formalisiert das Konzept ‚Äúgen√ºgend‚Äù in der geforderten Verringerung des Funktionswertes. Die Bedingung \\(f(x^k + \\alpha d^k) &lt; f(x^k)\\) wird modifiziert zu \\[f(x^k + \\alpha d^k) \\leq f(x^k) + \\sigma \\alpha \\left(\\nabla f(x^k)\\right)^T d^k,\\] mit \\(\\sigma\\in (0,1)\\). Die Armijo-Bedingung umgeht Konvergenzprobleme der einfachen Bedingung, indem sie fordert, dass die Verringerung zumindest proportional zur Schrittweite und zur Richtungsableitung \\(\\left(\\nabla f(x^k)\\right)^T d^k\\) ist, mit Hilfe der Proportionalit√§tskonstante \\(\\sigma\\). In der Praxis werden oft sehr kleine Werte verwendet, z.B. \\(\\sigma=0.0001\\).\nDie Backtracking-Liniensuche verringert die Schrittweite wiederholt um den Faktor \\(\\rho\\) (rho) , bis die Armijo-Bedingung erf√ºllt ist. Sie terminiert garantiert nach einer endlichen Anzahl von Schritten. Weshalb wir sie hier einsetzen:\n\nalpha_k &lt;- function(b_0, b_1, d_k, alpha = 1, sigma = 0.0001, rho = 0.5) {\n  d_0 &lt;- d_k[1]\n  d_1 &lt;- d_k[2]\n  nabla &lt;- nabla_qs(b_0, b_1)\n  n_0 &lt;- nabla[1]\n  n_1 &lt;- nabla[2]\n\n  lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n  rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n\n  while (lhs &gt; rhs) {\n    alpha &lt;- rho * alpha\n    lhs &lt;- qs(b_0 + alpha*d_0, b_1 + alpha*d_1)\n    rhs &lt;- qs(b_0, b_1) + sigma*alpha*(n_0*d_0 + n_1*d_1)\n  }\n  return(alpha)\n}\n\nEin paar Einstellungen vorab:\n\n# maximale Anzahl an Iterationen\nmax_iter &lt;- 1000\niter &lt;- 0\n\n# Genauigkeit\neps &lt;- 10**-6\n\n# Startwerte\nb_0 &lt;- 0 \nb_1 &lt;- -1 \n\nF√ºr eine vorgegebene Genauigkeit \\(eps=10^{-6}\\), den Startwerten \\(\\hat\\beta_0^0 = 0\\) und \\(\\hat\\beta_1^0 = -1\\) k√∂nnen wir somit das Verfahren starten:\n\nwhile (TRUE) {\n  iter &lt;- iter + 1\n\n  d_k &lt;- -nabla_qs(b_0, b_1)\n\n  ad_ &lt;- alpha_k(b_0, b_1, d_k) * d_k\n\n  x0 &lt;- b_0 + ad_[1]\n  x1 &lt;- b_1 + ad_[2]\n\n  if ((abs(b_0 - x0) &lt; eps) & (abs(b_1 - x1) &lt; eps) | (iter &gt; max_iter)) {\n    break\n  }\n  b_0 &lt;- x0\n  b_1 &lt;- x1\n}\n\nWir haben in \\(203\\) Iterationsschritten das folgende Ergebnis f√ºr die Regressionskoeffizienten:\n\\[\n\\hat\\beta_0^{stud} = 0 \\qquad \\hat\\beta_1^{stud} = -0.7761689\n\\]\nBetrachten wir die daraus erstellte Regressionsgerade:\n\n\n\n\n\n\n\n\n\nUm die Regressionskoeffizienten f√ºr unser urspr√ºngliches Problem zu erhalten m√ºssen wir wie folgt zur√ºck rechnen:\n\n(b1 &lt;- b_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06822832\n(b0 &lt;- mean(dt$mpg) -  b1 * mean(dt$hp))\n#&gt; [1] 30.09887\n\nDie Geradengleichung f√ºr das urspr√ºngliches Problem lautet somit:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988668 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]\n\n\nDie R Funktion optim\nIn R gibt es bessere Optimierungsmethoden, als die hier verwendete. Zum Beispiel k√∂nnen wir die Funktion optim verwenden. Die Funktion optim ben√∂tigt die zu optimierende \\(f(x)\\) und ggf. die Gradientenfunktion \\(gf(x)\\) sowie einen Startpunkt \\(x^0\\):\n\nf &lt;- function(beta) {\n  qs(beta[1], beta[2])\n}\n\ngrf &lt;- function(beta) {\n  nabla_qs(beta[1], beta[2])\n}\n\n# Der eigentliche Aufruf von optim:\nergb &lt;- optim(c(0,-0.5),f ,grf, method = \"CG\")\n\n# Auslesen der Sch√§tzer aus dem Ergebnis:\n(optim_beta_0 &lt;- ergb$par[1])\n#&gt; [1] 0\n(optim_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.7761683\n\nWir erhalten somit f√ºr das studentisierte Problem die Gerade:\n\\[\n\\begin{aligned}\n  \\hat{y}^{stud} &= \\hat\\beta_0^{stud} + \\hat\\beta_1^{stud} \\cdot x^{stud} \\\\\n          &\\approx 0 -0.7761683 \\cdot  x^{stud} \\\\\n          &\\approx 0 -0.776 \\cdot  x^{stud}\n\\end{aligned}\n\\]\nF√ºr das urspr√ºngliche Problem rechnen wir mittels\n\noptim_b1 &lt;- optim_beta_1 * sd(dt$mpg) / sd(dt$hp)\noptim_b0 &lt;- mean(dt$mpg) -  optim_b1 * mean(dt$hp)\n\num und erhalten:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 30.0988601 -0.0682283 \\cdot x \\\\\n          &\\approx 30.099 -0.068 \\cdot x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-summe-der-absoluten-abweichungen",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "2. Idee: Summe der absoluten Abweichungen",
    "text": "2. Idee: Summe der absoluten Abweichungen\nWir √§ndern nun die Abweichungsmessfunktion von der Quadrat-Summe hin zu den Absolut-Summen:\n\\[\nAS = AS(\\hat\\beta) = AS(\\hat\\beta_0, \\hat\\beta_1) = \\sum_{i=1}^n |\\hat{y}_i - y_i|\n\\]\nAuch hier wollen wir mit den studentisierten Daten arbeiten und stellen die Funktion der Absolut-Summen auf:\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  return(sum(abs(b_0 + b_1 * x - y)))\n}\n\nDanach konstruieren wir die zu optimierende Funktion \\(f\\):\n\n# Zu optimierende Funktion\nf &lt;- function(beta) {\n  as(beta[1], beta[2])\n}\n\nDiesmal nutzen wir optim ohne eine Gradientenfunktion:\n\nergb &lt;- optim(c(0,-1), f)\n\n# Sch√§tzer auslesen\n(opti_as_beta_0 &lt;- ergb$par[1])\n#&gt; [1] -0.1304518\n(opti_as_beta_1 &lt;- ergb$par[2])\n#&gt; [1] -0.6844911\n\nSchauen wir uns nun die so erhaltene Gerade im Vergleich mit der ‚Äònormalen‚Äô Regressionsgerade an:\n\n\n\n\n\n\n\n\n\nIn gr√ºn und gestrichelt sehen wir die Gerade aus der Idee der quadratischen Abweichungssummen, in blau die aus der Idee der absoluten Abweichungssummen.\nF√ºr unser urspr√ºngliches Problem rechnen wir um:\n\n# Umrechnen in die urspr√ºngliche Fragestellung\n(as_b1 &lt;- opti_as_beta_1 * sd(dt$mpg) / sd(dt$hp))\n#&gt; [1] -0.06016948\n(as_b0 &lt;- (mean(dt$mpg) - as_b1 * mean(dt$hp)) + opti_as_beta_0 * sd(dt$mpg))\n#&gt; [1] 28.13051\n\nUnd die dazu geh√∂rige Darstellung:\n\n\n\n\n\n\n\n\n\nDie Funktionsvorschrift f√ºr die (blaue) Regressionsgerade lautet:\n\\[\n\\begin{aligned}\n  \\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1 \\cdot x \\\\\n          &\\approx 28.1305094 -0.0601695 \\cdot x \\\\\n          &\\approx 28.131 -0.06 \\cdot x\n\\end{aligned}\n\\]\nDiese Methode nennt sich Median-Regression und ein ein Spezialfall der Quantilsregression, die sich u.a. mit dem R-Paket quantreg unmittelbar umsetzen l√§sst:\n\nlibrary(quantreg)\nergmedianreg &lt;- rq(mpg ~ hp, data = dt)\ncoef(ergmedianreg)\n#&gt; (Intercept)          hp \n#&gt; 28.13050847 -0.06016949"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#idee-betrag-der-summe-der-abweichungen",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "1. Idee: Betrag der Summe der Abweichungen",
    "text": "1. Idee: Betrag der Summe der Abweichungen\nWenn wir die Summe der Abweichungen \\(\\sum\\limits_{i=1}^n \\hat{e}_i\\) minimieren wollen, dann ist es sinnvoll den Betrag davon zu minimieren. Wir suchen also die Sch√§tzer \\(\\hat\\beta_0\\) und \\(\\hat\\beta_1\\), so dass der Ausdruck\n\\[\n\\left| \\sum_{i=1}^n \\hat{e}_i \\right| = \\left| \\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i) \\right|\n\\]\nminimal ist.\nWegen:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (\\hat\\beta_0 + \\hat\\beta_1 \\cdot x_i - y_i)\n&= \\sum_{i=1}^n \\hat\\beta_0 + \\sum_{i=1}^n \\hat\\beta_1 \\cdot x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i \\\\\n&= n \\cdot \\hat\\beta_0 + \\hat\\beta_1 \\cdot n \\cdot \\bar{x} - n \\cdot \\bar{y} \\\\\n&= n \\cdot \\left( \\hat\\beta_0 + \\hat\\beta_1 \\cdot \\bar{x} - \\bar{y} \\right) \\\\\n&= n \\cdot \\left( \\hat\\beta_0 - \\bar{y} + \\hat\\beta_1 \\cdot \\bar{x}  \\right)\n\\end{aligned}\n\\]\nk√∂nnen wir das absolute Minimum bei \\(\\hat\\beta_0 - \\bar{y} =0\\) und \\(\\hat\\beta_1 \\cdot \\bar{x}=0\\) erreichen, was zur L√∂sung \\(\\hat\\beta_0 =\\bar{y}\\) und \\(\\hat\\beta_1 = 0\\) f√ºhrt. Dies ist unser Nullmodel in dem die \\(x_i\\) keinen Einfluss auf die \\(y_i\\) haben und wir daher pauschal die \\(y_i\\) mit \\(\\hat{y}_i=\\bar{y}\\), also dem Mittelwert der \\(y_i\\) absch√§tzen."
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#zusammenfassung",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nAls Vergleich k√∂nnen wir uns die Quadratsumme \\(QS\\) und Absolutsumme \\(AS\\) der drei Modelle einmal ansehen:\n\n# Quadratische Abweichungssummen\nqs &lt;- function(b_0, b_1) {\n  sum(((b_0 + b_1 * dt$hp) - dt$mpg )**2)\n}\n\n# Absolute Abweichungssummen\nas &lt;- function(b_0, b_1) {\n  sum(abs((b_0 + b_1 * dt$hp) - dt$mpg))\n}\n\n\n# Quadratsummen:\nquad_sum &lt;- c(qs(b0, b1), qs(as_b0, as_b1), qs(mean_mpg, 0))\n\n# Absolutsummen:\nabs_sum &lt;- c(as(b0, b1), as(as_b0, as_b1), as(mean_mpg, 0))\n\ntab &lt;- tibble(\n  sums = c(quad_sum, abs_sum),\n  sum_type = rep(c(\"quad\", \"abs\"), each = 3),\n  methode = rep(c(\"Idee 3\", \"Idee 2\", \"Idee 1\"), 2)\n)\n\npivot_wider(tab, names_from=sum_type, values_from=sums, names_sort=T)\n#&gt; # A tibble: 3 √ó 3\n#&gt;   methode   abs  quad\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Idee 3   93.0  448.\n#&gt; 2 Idee 2   87.3  477.\n#&gt; 3 Idee 1  151.  1126."
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#reproduzierbarkeitsinformationen",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "Reproduzierbarkeitsinformationen",
    "text": "Reproduzierbarkeitsinformationen\n\n#&gt; R version 4.5.1 (2025-06-13)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.5\n#&gt; \n#&gt; Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8\n#&gt; \n#&gt; Package version:\n#&gt;   mosaic_1.9.1 quantreg_6.1 tidyr_1.3.1  xfun_0.52"
  },
  {
    "objectID": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "href": "posts/2021-06-09-ueber-die-koeffizienten-einer-linearen-regression/index.html#footnotes",
    "title": "√úber die Koeffizienten einer linearen Regression",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\nDas ‚ÄúCookbook‚Äù zur Datentabelle k√∂nnen Sie mit Hilfe von help(\"mtcars\") aufrufen!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/Willkommen/index.html",
    "href": "posts/Willkommen/index.html",
    "title": "Willkommen in meinem Blog",
    "section": "",
    "text": "Das ist mein erster Blog-Eintrag! Ich werde nun hoffentlich (langsam) meinen Blog von Hugo aus Quarto umstellen. Ich arbeite viel mit R und eine extra Sprache nur f√ºr den Blog ist mir einfach auf Dauer zu umst√§ndlich. Aber mal sehen, wann ich Zeit finde dieses Projekt umzustezen."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "F√ºr die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-f√ºr-r",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbereitungen-f√ºr-r",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "",
    "text": "F√ºr die graphischen Ausgaben nutzen wir R und das Paket mosaic:\n\nlibrary(mosaic)"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#vorbemerkungen-und-notationen",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Vorbemerkungen und Notationen",
    "text": "Vorbemerkungen und Notationen\nDa alle t-Verteilungen symmetrisch sind, betrachten wir im wesendlichen nur den positiven Teil.\nZwei reelle Funktionen \\(f\\), \\(g\\) sind genau dann, im Sinne von de Bruijn1 (¬ß1.4), asymptotisch √§quivalent \\(f \\sim g\\), wenn\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{f(x)}{g(x)} = 1\n\\]\ngilt.\nIst \\(f \\sim g\\), so k√∂nnen wir auch\n\\[\nf(x) = g(x)\\cdot(1+o(1))\n\\]\ndaf√ºr schreiben. Dabei ist \\(h(x) = o(\\phi(x))\\) f√ºr \\(x \\to \\infty\\), falls \\(\\lim\\limits_{x \\to \\infty} \\frac{h(x)}{\\phi(x)} = 0\\) gilt. Aus der asymptotischen √Ñquivalenz von \\(f\\) und \\(g\\) folgt nun direkt:\n\\[\n\\lim\\limits_{x \\to \\infty}\\frac{f(x)}{g(x)}-1 =\\frac{f(x)-g(x)}{g(x)} = 0\n\\]\nMit \\(h(x) = \\frac{f(x)-g(x)}{g(x)}\\) ist \\(h(x) = o(1)\\) und daher \\(f(x)-g(x) = g(x)o(1)\\) und schliesslich \\(f(x) = g(x)+g(x)o(1)\\).\nEin wichtiges Korrolar sagt:\nIst \\(f \\sim g\\), so ist auch \\(\\log(f) \\sim \\log(g)\\)."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-im-allgemeinen",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung im Allgemeinen",
    "text": "Die t-Verteilung im Allgemeinen\nDie Dichtefunktion der t-Verteilung lauten im Allgemeinen:\n\\[\nf_n(x) = \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}}\\quad \\mathrm{f√ºr}\\quad -\\infty &lt; x &lt; +\\infty\n\\]\nwobei wir mit \\(\\Gamma(x)\\) die Gammafunktion\n\\[\n\\Gamma(x)=\\int\\limits_{0}^{+\\infty}t^{x-1}e^{-t}\\operatorname{d}t\n\\]\nbezeichnen. F√ºr einige \\(x\\) nimmt die Gammafunktion leicht zu berechnende Werte an:\nSo ist f√ºr alle \\(n\\in\\mathbf{N_0}\\):\n\\(\\Gamma(n+1) = n!\\) und \\(\\Gamma\\left(n + \\frac{1}{2}\\right) = \\frac{(2n)!}{n!4^n}\\sqrt{\\pi}\\)\nmit der gew√∂hnlichen Fakult√§t \\(n! = \\prod_{i=0}^n i\\), wobei per Definition \\(0!=1\\) ist."
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#die-t-verteilung-mit-einem-freiheitsgrad",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Die t-Verteilung mit einem Freiheitsgrad",
    "text": "Die t-Verteilung mit einem Freiheitsgrad\nF√ºr \\(f_1(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n&= \\frac{\\Gamma\\left(\\frac{2}{2}\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-\\frac{2}{2}} \\\\\n&= \\frac{\\Gamma\\left(1\\right)} {\\sqrt{\\pi}~\\Gamma\\left(\\frac{1}{2}\\right)}\\left(1+x^{2}\\right)^{-1} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_1(x) &= \\frac{1} {\\sqrt{\\pi} \\cdot \\sqrt{\\pi}} \\cdot \\left(1+x^{2}\\right)^{-1} \\\\\n       &= \\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\n\\end{align*}\n\\]\nDas ist die Dichtefunktion der standardisierten Cauchy-Verteilung\n\\[\nf_{(\\mu,\\lambda)}(x) = \\frac{1}{\\pi} \\cdot \\frac{\\lambda}{\\lambda^2+(x-\\mu)^2}\n\\]\nmit (\\(\\mu = 0\\) und \\(\\lambda=1\\)), welche ‚Äì bekannterma√üen ‚Äì keinen Erwartungwert hat.\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_1(k \\cdot x)}{f_1(x)}\n      &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\pi} \\cdot \\frac{1}{1+(kx)^{2}}}{\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}} = \\lim_{x \\to +\\infty} \\frac{1+x^2}{1+k^2x^2} \\\\\n      &=\\lim_{x \\to +\\infty} \\frac{\\frac{1}{x^2}+\\frac{x^2}{x^2}}{\\frac{1}{x^2}+k^2\\frac{x^2}{x^2}} =\\frac{1}{k^2}=k^{-2}\n\\end{align*}\n\\]\nf√ºr alle reellen \\(k&gt;0\\) ist \\(f_1(x)\\) eine regul√§r variierende Funktion mit Variationsindex \\(\\rho = -2\\).\nDie √úberlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\overline{F}_1(x) = \\int_x^\\infty f_1(t) \\operatorname{d}t = \\frac{1}{\\pi} \\cdot \\int_x^\\infty  \\frac{1}{1+x^{2}} \\operatorname{d}t = \\frac{\\arctan(x)}{\\pi}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen d√ºrfen.\nEs gilt nun:\n\\[\n\\arctan`(x)= \\frac{1}{1+x^2} \\to \\frac{1}{x^2} \\text{ f√ºr } x\\to \\infty\n\\]\nGenauer gilt wegen\n\\[\n\\lim\\limits_{x \\to \\infty} \\frac{\\frac{1}{1+x^2}}{\\frac{1}{x^2}}\n= \\lim\\limits_{x \\to \\infty} \\frac{x^2}{1+x^2} =1,\n\\]\ndass \\(\\frac{1}{1+x^2} \\sim \\frac{1}{x^2}\\), also asymptotisch √§quivalent sind und somit auch \\(\\log\\left(\\frac{1}{1+x^2}\\right) \\sim \\log\\left(\\frac{1}{x^2}\\right)\\).\nZusammen gefasst gilt somit: \\[\n\\log\\left(\\frac{1}{\\pi} \\cdot \\frac{1}{1+x^{2}}\\right) \\to -2\\log(x) - \\log(\\pi) \\text{ f√ºr } x \\to \\infty\n\\]\nSei \\(f_1^*(x) = C \\cdot x^{-\\alpha}\\) mit \\(\\alpha = 2\\) und \\(C=\\frac{1}{\\pi} \\approx0.3183\\).\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 1\n\nf_star &lt;- function(x) {\n  alpha &lt;- 2\n  C &lt;- 1/pi\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie gro√ü ist nun der (absolute) Fehler zwischen \\(f_1^*\\) und \\(f_1\\)?\nEine Grafik von \\(f_1^*-f_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(x**-2 - 1/(1+x**2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nGenauer gilt:\n\\[\nf_1^*(x) - f_1(x) = \\frac{1}{x^2+x^4}\n\\]\nWir k√∂nnen also f√ºr ein hinreichend gro√ües \\(x &gt;&gt; 1\\) statt \\(f_1\\) auch \\(f_1^*\\) verwenden und erhalten somit:\n\\[\n\\begin{align*}\n\\overline{F}_1(x) &\\approx \\int_x^\\infty f_1^*(t) \\operatorname{d}t\n  = \\int_x^\\infty  C \\cdot t^{-\\alpha} \\operatorname{d}t \\\\\n  &= \\frac{1}{\\pi} \\cdot \\int_x^\\infty  t^{-2} \\operatorname{d}t\n  = \\frac{1}{\\pi}\\left[\\lim\\limits_{\\epsilon \\to \\infty} \\left(-\\epsilon^{-1}\\right) -\\left(-x^{-1}\\right)\\right]\\\\\n  &= \\frac{1}{\\pi}\\cdot\\left[0 + \\frac{1}{x}\\right] = \\frac{1}{\\pi \\cdot x}\n\\end{align*}\n\\]\nWie hinreichend ist hier hinreichend gro√ü?\nTaleb schreibt an dieser Stelle gerne, dass man jenseits des Karamata-Punktes die Karamata-Konstante anwenden kann. Beides Begriffe, zu denen ich zun√§chst keine echte Definition gefunden habe.\nJovan Karamata ist der Begr√ºnder der langsam variierend Funktionen. 1930 zeite er, dass eine positive stetige Funktion \\(L\\) auf den positiven reellen Zahlen genau dann langsam variierend ist, also f√ºr alle \\(t &gt; 0\\) die Bedingung\n\\[\nL(t\\,x)/L(x) \\to 1 \\qquad\\text{f√ºr}\\qquad x \\to \\infty\n\\]\nerf√ºllt, wenn sie f√ºr ein \\(a &gt; 0\\) f√ºr \\(x &gt; a\\) in der Form\n\\[\nL(x) = c(x) \\cdot \\exp \\left(\\int_a^x\\!\\varepsilon(t)/t \\ dt \\right)\n\\] mit \\(c(x) \\to c &gt; 0\\) und \\(\\varepsilon(x) \\to 0\\) f√ºr \\(x \\to \\infty\\) geschrieben werden kann.\nIch vermute also, dass wir \\(L(x)\\) ab dem Punkt \\(a\\) n√§herungsweise durch \\(c(x)\\) besser sogar durch die Konstante \\(c\\) ersetzen k√∂nnten.\nDie Karamata-Konstante ist \\(\\rho = -\\alpha\\), also \\(\\rho = c\\)?\nDer Karamata-Punkt bleibt etwas nebul√∂ser. Es k√∂nnte sich hier um den Punkt \\(a\\) handeln und vermutlich k√∂nnte man hier so argumentieren:\nWenn die Fehler zwischen \\(f\\) und \\(f^*\\) hinreichend klein ist.\nHierf√ºr k√∂nnte man einen absoluten Fehler oder einen relativen Fehler als Ma√üstab ansehen.\nF√ºr einen relativen Fehler vielleicht \\(\\frac{f^*-f}{x} &lt; k\\)?\nOder man betrachtet hier gleich \\(\\frac{f^*-f}{\\log(x)} &lt; k^*\\)?"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#t-verteilung-mit-zwei-freiheitsgeraden",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "t-Verteilung mit zwei Freiheitsgeraden",
    "text": "t-Verteilung mit zwei Freiheitsgeraden\nF√ºr \\(f_2(x)\\) ergibt sich somit:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)} {\\sqrt{n\\pi}~\\Gamma\\left(\\frac{n}{2}\\right)}\\left(1+\\frac{x^{2}}{n}\\right)^{-\\frac{n+1}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(\\frac{2}{2}\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n       &= \\frac{\\Gamma\\left(\\frac{3}{2}\\right)} {\\sqrt{2\\pi}~\\Gamma\\left(1\\right)}\\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n\\end{align*}\n\\]\nWegen \\(\\Gamma(1) = 0! = 1\\) und \\(\\Gamma\\left(\\frac{3}{2}\\right)=\\frac{\\sqrt{\\pi}}{2}\\) ergibt sich nun:\n\\[\n\\begin{align*}\nf_2(x) &= \\frac{1}{2\\sqrt{2}} \\cdot \\left(1+\\frac{x^{2}}{2}\\right)^{-\\frac{3}{2}} \\\\\n  &= \\frac{1}{\\sqrt[2]{2^3} \\cdot \\sqrt[2]{\\left(1+\\frac{x^{2}}{2}\\right)^3}}  \\\\\n  &= \\frac{1}{(x^2+2)^{\\frac{3}{2}}} \\\\\n  &= \\frac{1}{\\sqrt{(x^2+2)^3}}\n\\end{align*}\n\\]\nWegen\n\\[\n\\begin{align*}\n\\lim_{x \\to +\\infty} \\frac{f_2(k \\cdot x)}{f_2(x)}\n    &= \\lim_{x \\to +\\infty} \\frac{\\frac{1}{\\sqrt{((k\\cdot x)^2+2)^3}}}{\\frac{1}{\\sqrt{(x^2+2)^3}}} = \\lim_{x \\to +\\infty} \\frac{\\sqrt{(x^2+2)^3}}{\\sqrt{((k\\cdot x)^2+2)^3}} \\\\\n    &= \\lim_{x \\to +\\infty} \\left(\\frac{x^2+2}{k^2x^2+2}\\right)^\\frac{3}{2}=\\lim_{x \\to +\\infty} \\left(\\frac{\\frac{x^2}{x^2}+\\frac{2}{x^2}}{k^2\\frac{x^2}{x^2}+\\frac{2}{x^2}}\\right)^\\frac{3}{2} \\\\\n    &=\\left(\\frac{1}{k^2}\\right)^\\frac{3}{2}=\\frac{1}{k^3}=k^{-3}\n\\end{align*}\n\\]\nf√ºr alle reellen \\(k&gt;0\\) ist \\(f_2(x)\\) eine regul√§r variierende Funktion mit Variationsindex \\(\\rho = -3\\).\nDie √úberlebensfunktion zur t-Verteilung mit einem Freiheitsgrad lautet nun:\n\\[\n\\begin{align*}\n\\overline{F}_2(x) &= \\int_x^\\infty f_2(t) \\operatorname{d}t = \\int_x^\\infty \\frac{1}{\\sqrt{(t^2+2)^3}} \\operatorname{d}t \\\\\n  &= \\frac{x}{2 \\cdot \\sqrt{x^2+2}}\n\\end{align*}\n\\]\nda wir das optionale \\(+C\\) mit \\(C=0\\) annehmen d√ºrfen.\nEs gilt f√ºr jedes feste \\(k&gt;0\\):\n\\[\n\\begin{align*}\n\\lim\\limits_{x \\to \\infty} \\frac{\\overline{F}_2(k x)}{\\overline{F}_2(x)} &= \\lim\\limits_{x \\to \\infty}k \\cdot \\sqrt{\\frac{x^2+2}{k^2x^2+2}} \\\\\n  &=  k \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{\\frac{1}{k^2} \\cdot \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} \\\\\n  &= \\frac{k}{k} \\cdot \\lim\\limits_{x \\to \\infty}  \\sqrt{ \\frac{x^2+2}{x^2+\\frac{2}{k^2}}} = 1\\end{align*}\n\\]\nWegen\n\\[\n\\lim_{x \\to \\infty} \\frac{\\frac{1}{\\sqrt{(x^2+2)^3}}}{\\frac{1}{x^3}} =\\lim\\limits_{x \\to \\infty} \\frac{x^3}{(\\sqrt{x^2+2})^3} = \\lim\\limits_{x \\to \\infty} \\left(\\frac{x}{\\sqrt{x^2+2}}\\right)^3= 1\n\\]\nist \\(f_2 \\sim f^*_2\\) und somit auch \\(\\log(f_2) \\sim \\log(f^*_2)\\).\nAus \\(\\log\\left(\\frac{1}{x^3}\\right) = \\log(1)- 3\\cdot\\log(x)\\) k√∂nnen wir daher auf \\(\\alpha = 3\\) und \\(C=1\\) schliesse und schreiben:\n\\[\nf_2^*(x) = C \\cdot x^{-\\alpha} = x^{-3}\n\\]\nSchauen wir uns das einmal als Grafik an:\n\nlower_bound &lt;- 2\nupper_bound &lt;- 100\ndfree &lt;- 2\n\nf_star &lt;- function(x) {\n  alpha &lt;- 3\n  C &lt;- 1\n  C * x**(-alpha)\n}\n\nx &lt;- seq(lower_bound, upper_bound, 0.1)\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line(f_star(x) ~ x, \n          color = \"darkgreen\")\n\n\n\n\n\n\n\n\nHier eine doppelt-logarithmische Darstellung:\n\ngf_dist(\"t\", df = dfree, \n        xlim = c(lower_bound, upper_bound), \n        color = \"darkred\") %&gt;%\n  gf_line( f_star(x) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )\n\n\n\n\n\n\n\n\nWie gro√ü ist nun der absolute Fehler zwischen \\(f_2^*\\) und \\(f_2\\) genau?\nEine Grafik zeigt von \\(f_2^*-2_1\\) zeigt:\n\nx &lt;- seq(1,1000,1)\n  gf_line(f_star(x) - dt(x,df=2) ~ x, \n           color = \"darkgreen\") %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n  )"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#fussnoten",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fussnoten",
    "text": "Fussnoten"
  },
  {
    "objectID": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "href": "posts/2021-02-14-ueber-die-t-verteilung-mit-einem-bzw-zwei-freiheitsgraden/index.html#footnotes",
    "title": "√úber die t-Verteilung mit einem bzw. zwei Freiheitsgraden",
    "section": "Fu√ünoten",
    "text": "Fu√ünoten\n\n\nde Bruijn, N. G. (1981), Asymptotic Methods in Analysis, Dover Publications, ISBN 9780486642215‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html",
    "href": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html",
    "title": "Wie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert",
    "section": "",
    "text": "Am besten mit dem Raspberry Pi Installer von hier: https://www.raspberrypi.com/software/\nF√ºr einen Raspberry Zero: Raspberry Pi OS (32-bit) Lite\nF√ºr einen Raspberry Zero 2 W: Raspberry Pi OS (64-bit) Lite\nWichtig MIT SSH konfigurieren!\n\n\n\nAuf dem RaspberryPI Zero ist ein Waveshare 2-13inch-e-Paper HAT installiert. Alles Wichtige dazu findet sich unter: https://www.waveshare.com/wiki/2.13inch_e-Paper_HAT_Manual#Working_With_Raspberry_Pi\nDamit das Display richtig funktioniert sollte man in der config.txt die folgende Zeile anpassen:\ndtparam=spi=on\n\n\n\nEs gibt viele Weg die zum Ziel f√ºhren (k√∂nnen).\nZum Beispiel mit dem Befehl nmap:\n&gt; nmap -sn 192.168.2.0/24\nOder einfach den Hostnamen ‚Äúraspberrpi.local‚Äù nutzen.\nMan kann sich somit ggf. mittels ssh und dem folgenden Kommando anmelden:\n&gt; ssh norman@raspberrypi.local\nEbenso kann R PI Connect benutzt werden, wenn man es vorher eingerichtet hat.\n\n\n\nIch habe bei GitHub ein kleines Programm geschrieben, welches die aktuelle IP Address (IP4 und IP6 sowie die aktuelle Uhrzeit) auf dem Display ausgibt.\nEs ist unter https://github.com/NMarkgraf/pi-startup-display zu finden und wird einfach in das Hauptverzeichnis des Nutzers ‚Äúnorman‚Äù unter ‚Äú/home/norman‚Äù per\n&gt; git clone https://github.com/NMarkgraf/pi-startup-display \ninstalliert.\nSp√§ter kann dann im Verzeichnis pi-startup-display einfach das aktuelle Release von GutHUb durch den folgenden Befehl geladen werden:\n&gt; git pull\nDamit dieses Skript nach jedem Neustart automatisch (einmal) gestartet wird bin ich nach dem Beispiel von https://webnist.de/automatisches-starten-eines-python-skripts-auf-dem-raspberry-pi-mit-systemd/ vorgegangen.\nAber ich habe die service Datei ‚Äú/etc/systemd/system/startup-display.service‚Äù wie folgt angepasst:\n[Unit]\nDescription=startup-display\nAfter=network.target\n \n[Service]\nExecStart=/usr/bin/python3 -u /home/norman/pi-startup-display/start-display.py \nWorkingDirectory=/home/norman/pi-startup-display/\n#StandardOutput=inherit\nStandardOutput=file:/tmp/start-display.log\n#StandardError=inherit\nStandardError=file:/tmp/start-display-error.log\n#Restart=always\nRestart=on-failture\nRestartSec=60\nUser=norman\n \n[Install]\nWantedBy=multi-user.target\nZum anlegen habe ich den Editor ‚Äúnano‚Äù genutzt und die Datei gleich mit angelegt.\n&gt; sudo nano /etc/systemd/system/startup-display.service\nDie ge√§nderte Datei muss nun initialisiert werden mit:\n&gt; sudo systemctl daemon-reload\n&gt; sudo systemctl enable startup-display.service\nDann k√∂nnen wir alles, zum Testen, von Hand starten:\n&gt; sudo systemctl start startup-display.service\nOb alles zu unserer Zufriedenheit funktioniert k√∂nnen wir mit der Status-Anzeige mit dem folgenden Befehl pr√ºfen:\n&gt; sudo systemctl status startup-display.service\nUnd nat√ºrlich ggf. auch stoppen:\n&gt; sudo systemctl stop startup-display.service"
  },
  {
    "objectID": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#einrichten-des-raspberry-zero",
    "href": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#einrichten-des-raspberry-zero",
    "title": "Wie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert",
    "section": "",
    "text": "Am besten mit dem Raspberry Pi Installer von hier: https://www.raspberrypi.com/software/\nF√ºr einen Raspberry Zero: Raspberry Pi OS (32-bit) Lite\nF√ºr einen Raspberry Zero 2 W: Raspberry Pi OS (64-bit) Lite\nWichtig MIT SSH konfigurieren!\n\n\n\nAuf dem RaspberryPI Zero ist ein Waveshare 2-13inch-e-Paper HAT installiert. Alles Wichtige dazu findet sich unter: https://www.waveshare.com/wiki/2.13inch_e-Paper_HAT_Manual#Working_With_Raspberry_Pi\nDamit das Display richtig funktioniert sollte man in der config.txt die folgende Zeile anpassen:\ndtparam=spi=on\n\n\n\nEs gibt viele Weg die zum Ziel f√ºhren (k√∂nnen).\nZum Beispiel mit dem Befehl nmap:\n&gt; nmap -sn 192.168.2.0/24\nOder einfach den Hostnamen ‚Äúraspberrpi.local‚Äù nutzen.\nMan kann sich somit ggf. mittels ssh und dem folgenden Kommando anmelden:\n&gt; ssh norman@raspberrypi.local\nEbenso kann R PI Connect benutzt werden, wenn man es vorher eingerichtet hat.\n\n\n\nIch habe bei GitHub ein kleines Programm geschrieben, welches die aktuelle IP Address (IP4 und IP6 sowie die aktuelle Uhrzeit) auf dem Display ausgibt.\nEs ist unter https://github.com/NMarkgraf/pi-startup-display zu finden und wird einfach in das Hauptverzeichnis des Nutzers ‚Äúnorman‚Äù unter ‚Äú/home/norman‚Äù per\n&gt; git clone https://github.com/NMarkgraf/pi-startup-display \ninstalliert.\nSp√§ter kann dann im Verzeichnis pi-startup-display einfach das aktuelle Release von GutHUb durch den folgenden Befehl geladen werden:\n&gt; git pull\nDamit dieses Skript nach jedem Neustart automatisch (einmal) gestartet wird bin ich nach dem Beispiel von https://webnist.de/automatisches-starten-eines-python-skripts-auf-dem-raspberry-pi-mit-systemd/ vorgegangen.\nAber ich habe die service Datei ‚Äú/etc/systemd/system/startup-display.service‚Äù wie folgt angepasst:\n[Unit]\nDescription=startup-display\nAfter=network.target\n \n[Service]\nExecStart=/usr/bin/python3 -u /home/norman/pi-startup-display/start-display.py \nWorkingDirectory=/home/norman/pi-startup-display/\n#StandardOutput=inherit\nStandardOutput=file:/tmp/start-display.log\n#StandardError=inherit\nStandardError=file:/tmp/start-display-error.log\n#Restart=always\nRestart=on-failture\nRestartSec=60\nUser=norman\n \n[Install]\nWantedBy=multi-user.target\nZum anlegen habe ich den Editor ‚Äúnano‚Äù genutzt und die Datei gleich mit angelegt.\n&gt; sudo nano /etc/systemd/system/startup-display.service\nDie ge√§nderte Datei muss nun initialisiert werden mit:\n&gt; sudo systemctl daemon-reload\n&gt; sudo systemctl enable startup-display.service\nDann k√∂nnen wir alles, zum Testen, von Hand starten:\n&gt; sudo systemctl start startup-display.service\nOb alles zu unserer Zufriedenheit funktioniert k√∂nnen wir mit der Status-Anzeige mit dem folgenden Befehl pr√ºfen:\n&gt; sudo systemctl status startup-display.service\nUnd nat√ºrlich ggf. auch stoppen:\n&gt; sudo systemctl stop startup-display.service"
  },
  {
    "objectID": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#postgresql-installieren",
    "href": "posts/2025-08-09-Wie-mensch-eine-Datenbank-auf-einen-Raspbarry-Zero-installiert/index.html#postgresql-installieren",
    "title": "Wie mensch eine Datenbank auf einem RASPBERRY Pi Zero installiert",
    "section": "PostgreSQL installieren",
    "text": "PostgreSQL installieren\nIch gehe wie folgt vor:\n\nSchritt 1: Installieren von PostgreSQL\nDa es ein deb Paket gibt, nehme ich den leichten Weg:\n&gt; sudo apt install postgresql\n\n\nSchritt 2: Wechseln auf Nutzer:in ‚Äúpostgres‚Äù\nWir kommunizieren mit PostgreSQL durch den Benutzer ‚Äúpostgres‚Äù, in den wir uns verwandelt:\n&gt; sudo su postgres\n\n\nSchritt 3: Herunterladen der Demo-Daten, entpacken und installieren\nDie erste Demo-Daten erhalten wir durch:\n&gt; curl -L -O https://github.com/lerocha/netflixdb/releases/download/v1.0.16/netflixdb-postgres.zip\n&gt; unzip netflixdb-postgres.zip\nDa Daten k√∂nnen wir dann in die Datenbank ‚Äúnetfix‚Äù einf√ºgren:\n&gt; sudo su postgres\n&gt; psql -d netflix\n&gt; \\i /home/norman/netflixdb-postgres.sql\nJetzt ist die Datenbank ‚Äúnetflix‚Äù in PostgreSQL installiert.\nUnd wir k√∂nnen diese nun mit den folgenden Parametern ansprechen:\n  ip:       ???? \n  port:     5342\n  user:     postgres\n  password: postgresdemo \n  dbname:   netflix\n\n\nSchritt 4: AdminerNEO installieren\nAdminerNEO ist ein Webtool f√ºr die Verwaltung von PostgreSQL.\nWir installieren die aktuelle Version von der Website: https://www.adminneo.org/download"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu m√∂chte mensch zwei (oder mehr) Grafiken neben- oder √ºbereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zun√§chst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausf√ºhren oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "href": "posts/2021-12-27-grafiken-nebeneinander-setzen-mit-ggplot2-oder-ggformula/index.html#wie-menschen-grafiken-von-ggplot2-oder-ggformula-nebeneinander-setzten-kann",
    "title": "Grafiken nebeneinander setzen mit ggplot2 oder ggformula",
    "section": "",
    "text": "Ab und zu m√∂chte mensch zwei (oder mehr) Grafiken neben- oder √ºbereinander setzen.\nHier ein kurzes Beispiel, wie das gelingen kann. Zun√§chst bereiten wir alles vor, in dem wir die Pakete laden und die Daten freigeben.\n\nlibrary(mosaic)\n# ggf. einmal vorher den Befehl:\n# install.packages(\"gridExtra\")\n# ausf√ºhren oder das Paket \"gridExtra\" installieren.\nlibrary(gridExtra)\ndata(iris)  # Datensatz bereitstellen\n\nDann erstellen nun vier Plots und speichern diese in vier Variablen:\n\n# Ersten Plot erstellen\nplot1 &lt;- gf_point(Sepal.Length ~ Sepal.Width, data = iris)\n\n# Zweiten Plot erstellen\nplot2 &lt;- gf_point(Petal.Length ~ Petal.Width, data = iris)\n\n# Dritten Plot erstellen\nplot3 &lt;- gf_point(Sepal.Length ~ Petal.Width, data = iris)\n\n# Vierten Plot erstellen\nplot4 &lt;- gf_point(Petal.Length ~ Sepal.Width, data = iris)\n\nNun arrangieren wir die Plots entsprechend:\n\n# Zwei Plots nebeneinander in einer Zeile: \ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Zwei Plots untereinander in einer Spalte\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\n\n\n\n\n\n# Vier Plots in einer Matrix:\ngrid.arrange(plot1, plot2, plot3, plot4, ncol = 2, nrow = 2)"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgef√ºhrt.\nAls Testfall diente das klassische n-Damen-Problem, dessen L√∂sungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#kleiner-geschwindigkeitstest-mit-python3-pypy-und-graalpython",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "",
    "text": "Heute habe ich mit Hilfe von Pyenv einen kompakten Performance-Vergleich zwischen (C)Python3, PyPy und GraalPython durchgef√ºhrt.\nAls Testfall diente das klassische n-Damen-Problem, dessen L√∂sungsalgorithmus ich als Basis verwendet habe.\nDen Quellcode findet man hier: https://github.com/sol-prog/N-Queens-Puzzle\n\n\n\nCPython (3.9.4): Calculation took 316.97 seconds\nPyPy (3.7-7.3.3): Calculation took 10.14 seconds\nGraalPython (21.0.0): Calculation took 15.75 seconds"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse f√ºr n = 1 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‚Äòimport nqueens; nqueens.main()‚Äô\nerzeugt.\n\n\nDie Ergebnisse des Test vom 20.7.2025\n\nCPython:\n\n3.13.1: 1 loop, best of 50: 673 msec per loop\n3.13.5: 1 loop, best of 50: 661 msec per loop\n\nPyPy:\n\n3.10-7.3.17: 1 loops, average of 50: 75.6 +- 2.9 msec per loop (using standard deviation)\n3.11-7.3.19: 1 loops, average of 50: 74.9 +- 2.89 msec per loop (using standard deviation)\n\nGraalPython/GraalPy\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1):1 loop, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 1 loop, best of 50: 30 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 1 loop, best of 50: 24.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 1 loop, best of 50: 28.1 msec per loop"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup-1",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#neues-setup-1",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Neues Setup",
    "text": "Neues Setup\nDie Ergebnisse f√ºr n = 5 wurden mit der Zeile\npython -m timeit -r 50 -n 1 ‚Äòimport nqueens; nqueens.main()‚Äô\nerzeugt.\n\nDie Ergebnisse des Test vom 4.12.2025 (iMac 24.6.0 x86_64 i386)\n\nCPython:\n\nPython 3.13.5: 5 loops, best of 50: 656 msec per loop\nPython 3.13.10: 5 loops, best of 50: 647 msec per loop\nPython 3.14.0: 5 loops, best of 50: 665 msec per loop\nPython 3.14.1: 5 loops, best of 50: 701 msec per loop\n\nPyPy:\n\nPython 3.11.11 (0253c85bf5f8, Feb 26 2025, 10:43:06) [PyPy 7.3.19 with GCC Apple LLVM 15.0.0 (clang-1500.3.9.4)]: 5 loops, average of 50: 85.7 +- 2.08 msec per loop (using standard deviation)\nPython 3.11.13 (413c9b7f57f5, Jul 03 2025, 18:04:31) [PyPy 7.3.20 with GCC Apple LLVM 16.0.0 (clang-1600.0.26.6)]: 5 loops, average of 50: 87.6 +- 3.06 msec per loop (using standard deviation)\n\nGraalPy(thon):\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1): 5 loops, best of 50: 25.5 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 5 loops, best of 50: 33.6 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 5 loops, best of 50: 24.4 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 5 loops, best of 50: 32.2 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.2): 5 loops, best of 50: 24.4 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.2): 5 loops, best of 50: 32.3 msec per loop\nGraalPy 3.12.8 (Oracle GraalVM Native 25.0.1): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.12.8 (GraalVM CE Native 25.0.1): 5 loops, best of 50: 29.6 msec per loop"
  },
  {
    "objectID": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#die-ergebnisse-des-test-vom-4.12.2025-macbookpro-24.6.0-arm64-arm",
    "href": "posts/2021-04-08-graalpy-pypy--und-python-geschwindigkeitstest/index.html#die-ergebnisse-des-test-vom-4.12.2025-macbookpro-24.6.0-arm64-arm",
    "title": "GraalPy , PyPy und (C)Python im kurzen Geschwindigkeitstest",
    "section": "Die Ergebnisse des Test vom 4.12.2025 (MacBookPro 24.6.0 arm64 arm)",
    "text": "Die Ergebnisse des Test vom 4.12.2025 (MacBookPro 24.6.0 arm64 arm)\n\nCPython:\n\nPython 3.13.5: 5 loops, best of 50: 661 msec per loop\nPython 3.13.10: 5 loops, best of 50: 653 msec per loop\nPython 3.14.0: 5 loops, best of 50: 469 msec per loop\nPython 3.14.1: 5 loops, best of 50: 469 msec per loop\n\nPyPy:\n\nPython 3.11.11 (0253c85bf5f8, Feb 26 2025, 10:42:49) [PyPy 7.3.19 with GCC Apple LLVM 15.0.0 (clang-1500.3.9.4)]: 5 loops, average of 50: 74.2 +- 0.681 msec per loop (using standard deviation)\nPython 3.11.13 (413c9b7f57f5, Jul 03 2025, 18:04:06) [PyPy 7.3.20 with GCC Apple LLVM 16.0.0 (clang-1600.0.26.6)]: 5 loops, average of 50: 73.3 +- 0.688 msec per loop (using standard deviation)\n\nGraalPy(thon):\n\nGraalPy 3.11.7 (Oracle GraalVM Native 24.1.1): 5 loops, best of 50: 24.9 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.1.1): 5 loops, best of 50: 29.1 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.1): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.1): 5 loops, best of 50: 27.3 msec per loop\nGraalPy 3.11.7 (Oracle GraalVM Native 24.2.2): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.11.7 (GraalVM CE Native 24.2.2): 5 loops, best of 50: 27.2 msec per loop\nGraalPy 3.12.8 (Oracle GraalVM Native 25.0.1): 5 loops, best of 50: 23.6 msec per loop\nGraalPy 3.12.8 (GraalVM CE Native 25.0.1): 5 loops, best of 50: 24.9 msec per loop"
  },
  {
    "objectID": "publications/2004-09-XX_LEARN2CONTROL/index.html",
    "href": "publications/2004-09-XX_LEARN2CONTROL/index.html",
    "title": "LEARN2CONTROL",
    "section": "",
    "text": "LEARN2CONTROL ist eine computergest√ºtzte Lernumgebung, die es erm√∂glicht, vorhandenes Grundlagenwissen im Bereich der Regelungstechnik durch selbstst√§ndiges Lernen in einem projektorientierten Umfeld zu vertiefen. Der Lernschwerpunkt liegt dabei besonders auf der Vermittlung der inneren Abh√§ngigkeiten und Wechselwirkungen der unterschiedlichen Verfahren und Methoden der Regelungstechnik"
  },
  {
    "objectID": "publications/2004-09-XX_LEARN2CONTROL/index.html#zusammenfassung",
    "href": "publications/2004-09-XX_LEARN2CONTROL/index.html#zusammenfassung",
    "title": "LEARN2CONTROL",
    "section": "",
    "text": "LEARN2CONTROL ist eine computergest√ºtzte Lernumgebung, die es erm√∂glicht, vorhandenes Grundlagenwissen im Bereich der Regelungstechnik durch selbstst√§ndiges Lernen in einem projektorientierten Umfeld zu vertiefen. Der Lernschwerpunkt liegt dabei besonders auf der Vermittlung der inneren Abh√§ngigkeiten und Wechselwirkungen der unterschiedlichen Verfahren und Methoden der Regelungstechnik"
  },
  {
    "objectID": "projekte/FUMS/index.html",
    "href": "projekte/FUMS/index.html",
    "title": "FUMS - Fresh Up your Maths Skills",
    "section": "",
    "text": "Crash-Kurs durch die Mathematik f√ºr Studierende der Informatik / Wirtschaftsinformatik. Quasi ein kleiner Vorkurs mit den wichtistens Wiederholungen der Schulmathematik.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdr√ºckliches und schriftliches Einverst√§ndnis benutzt werden.\nF√ºr alle anderen gilt:\n¬© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "projekte/FUMS/index.html#fums---fresh-up-your-maths-skills",
    "href": "projekte/FUMS/index.html#fums---fresh-up-your-maths-skills",
    "title": "FUMS - Fresh Up your Maths Skills",
    "section": "",
    "text": "Crash-Kurs durch die Mathematik f√ºr Studierende der Informatik / Wirtschaftsinformatik. Quasi ein kleiner Vorkurs mit den wichtistens Wiederholungen der Schulmathematik.\nAuf Grund der aktuellen Lage besteht ein striktes Nutzungsverbot durch Dozent:innen der FOM. Das Werk und kein Teil dieses Werkes darf von Dozent:innen der FOM ohne mein ausdr√ºckliches und schriftliches Einverst√§ndnis benutzt werden.\nF√ºr alle anderen gilt:\n¬© by N.Markgraf published under CC-BY-SA-NC 3.0 de"
  },
  {
    "objectID": "projekte/WS-Quarto/index.html",
    "href": "projekte/WS-Quarto/index.html",
    "title": "Workshop zum Thema quarto",
    "section": "",
    "text": "Workshop rund um das Thema ‚Äúquarto‚Äù. Mit den Themen:\n\nWas ist quarto und was ist es nicht\nWie installiere ich quarto\nAuswahl der Ausgabe-formate in quarto\nWie erzeuge ich die Ausgaben (aka rendern von Dokumenten)\nDie quarto CLI\nAufbau der YAML-Kopf eines quarto markdown documents (=.qmd)\nGrundlagen des markdowns\nImplizite und expliziete Formatierung mittels quarto\nErstellung von Webseiten, RevealJS-Pr√§sentationen und PDF Dokumente"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Blog",
    "section": "",
    "text": "Statistik21\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2014\n\n\nKarsten L√ºbke, Matthias Gehrke, Norman Markgaf\n\n\n\n\n\n\n\n\n\n\n\n\nLEARN2CONTROL\n\n\nProjektorientiertes Lernen f√ºr ein besseres Gesamtverst√§ndnis in der regelungstechnischen Ausbildung\n\n\n\n\n\n\n\n\nSep 1, 2004\n\n\nAndreas Liefeld, Marten V√∂lker, Manuel Remelhe, Kai Dadhe, Sebastian Engell, Carsten Fritsch, Norman Markgaf\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Datenschutzerklaerung.html",
    "href": "Datenschutzerklaerung.html",
    "title": "Datenschutzerklaerung",
    "section": "",
    "text": "Personenbezogene Daten (nachfolgend zumeist nur ‚ÄûDaten‚Äú genannt) werden von uns nur im Rahmen der Erforderlichkeit sowie zum Zwecke der Bereitstellung eines funktionsf√§higen und nutzerfreundlichen Internetauftritts, inklusive seiner Inhalte und der dort angebotenen Leistungen, verarbeitet. Gem√§√ü Art. 4 Ziffer 1. der Verordnung (EU) 2016/679, also der Datenschutz-Grundverordnung (nachfolgend nur ‚ÄûDSGVO‚Äú genannt), gilt als ‚ÄûVerarbeitung‚Äú jeder mit oder ohne Hilfe automatisierter Verfahren ausgef√ºhrter Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten, wie das Erheben, das Erfassen, die Organisation, das Ordnen, die Speicherung, die Anpassung oder Ver√§nderung, das Auslesen, das Abfragen, die Verwendung, die Offenlegung durch √úbermittlung, Verbreitung oder eine andere Form der Bereitstellung, den Abgleich oder die Verkn√ºpfung, die Einschr√§nkung, das L√∂schen oder die Vernichtung. Mit der nachfolgenden Datenschutzerkl√§rung informieren wir Sie insbesondere √ºber Art, Umfang, Zweck, Dauer und Rechtsgrundlage der Verarbeitung personenbezogener Daten, soweit wir entweder allein oder gemeinsam mit anderen √ºber die Zwecke und Mittel der Verarbeitung entscheiden. Zudem informieren wir Sie nachfolgend √ºber die von uns zu Optimierungszwecken sowie zur Steigerung der Nutzungsqualit√§t eingesetzten Fremdkomponenten, soweit hierdurch Dritte Daten in wiederum eigener Verantwortung verarbeiten.\nUnsere Datenschutzerkl√§rung ist wie folgt gegliedert:\nI. Informationen √ºber uns als Verantwortliche\n‚Ä¶"
  },
  {
    "objectID": "Datenschutzerklaerung.html#i.-informationen-√ºber-uns-als-verantwortliche",
    "href": "Datenschutzerklaerung.html#i.-informationen-√ºber-uns-als-verantwortliche",
    "title": "Datenschutzerklaerung",
    "section": "I. Informationen √ºber uns als Verantwortliche",
    "text": "I. Informationen √ºber uns als Verantwortliche\nVerantwortlicher Anbieter dieses Internetauftritts im datenschutzrechtlichen Sinne ist:\nNorman Markgraf\nGuts-Muths-Weg 19\n45136 Essen\nDeutschland\nTelefon: +49-176-20077335\nE-Mail: admin(at)sefiroth.net\n\nDatenschutzbeauftragte/r beim Anbieter ist:\nNorman Markgraf"
  },
  {
    "objectID": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "href": "Datenschutzerklaerung.html#iii.-informationen-zur-datenverarbeitung",
    "title": "Datenschutzerklaerung",
    "section": "III. Informationen zur Datenverarbeitung",
    "text": "III. Informationen zur Datenverarbeitung\nIhre bei Nutzung unseres Internetauftritts verarbeiteten Daten werden gel√∂scht oder gesperrt, sobald der Zweck der Speicherung entf√§llt, der L√∂schung der Daten keine gesetzlichen Aufbewahrungspflichten entgegenstehen und nachfolgend keine anderslautenden Angaben zu einzelnen Verarbeitungsverfahren gemacht werden.\n\nCookies\n\nSitzungs-Cookies/Session-Cookies\n\nWir verwenden mit unserem Internetauftritt sog. Cookies. Cookies sind kleine Textdateien oder andere Speichertechnologien, die durch den von Ihnen eingesetzten Internet-Browser auf Ihrem Endger√§t ablegt und gespeichert werden. Durch diese Cookies werden im individuellen Umfang bestimmte Informationen von Ihnen, wie beispielsweise Ihre Browser- oder Standortdaten oder Ihre IP-Adresse, verarbeitet. Durch diese Verarbeitung wird unser Internetauftritt benutzerfreundlicher, effektiver und sicherer, da die Verarbeitung bspw. die Wiedergabe unseres Internetauftritts in unterschiedlichen Sprachen oder das Angebot einer Warenkorbfunktion erm√∂glicht. Rechtsgrundlage dieser Verarbeitung ist Art. 6 Abs. 1 lit b.) DSGVO, sofern diese Cookies Daten zur Vertragsanbahnung oder Vertragsabwicklung verarbeitet werden. Falls die Verarbeitung nicht der Vertragsanbahnung oder Vertragsabwicklung dient, liegt unser berechtigtes Interesse in der Verbesserung der Funktionalit√§t unseres Internetauftritts. Rechtsgrundlage ist in dann Art. 6 Abs. 1 lit. f) DSGVO. Mit Schlie√üen Ihres Internet-Browsers werden diese Session-Cookies gel√∂scht.\n\nDrittanbieter-Cookies\n\nGegebenenfalls werden mit unserem Internetauftritt auch Cookies von Partnerunternehmen, mit denen wir zum Zwecke der Werbung, der Analyse oder der Funktionalit√§ten unseres Internetauftritts zusammenarbeiten, verwendet. Die Einzelheiten hierzu, insbesondere zu den Zwecken und den Rechtsgrundlagen der Verarbeitung solcher Drittanbieter-Cookies, entnehmen Sie bitte den nachfolgenden Informationen.\n\nBeseitigungsm√∂glichkeit\n\nSie k√∂nnen die Installation der Cookies durch eine Einstellung Ihres Internet-Browsers verhindern oder einschr√§nken. Ebenfalls k√∂nnen Sie bereits gespeicherte Cookies jederzeit l√∂schen. Die hierf√ºr erforderlichen Schritte und Ma√ünahmen h√§ngen jedoch von Ihrem konkret genutzten Internet-Browser ab. Bei Fragen benutzen Sie daher bitte die Hilfefunktion oder Dokumentation Ihres Internet-Browsers oder wenden sich an dessen Hersteller bzw. Support. Bei sog. Flash-Cookies kann die Verarbeitung allerdings nicht √ºber die Einstellungen des Browsers unterbunden werden. Stattdessen m√ºssen Sie insoweit die Einstellung Ihres Flash-Players √§ndern. Auch die hierf√ºr erforderlichen Schritte und Ma√ünahmen h√§ngen von Ihrem konkret genutzten Flash-Player ab. Bei Fragen benutzen Sie daher bitte ebenso die Hilfefunktion oder Dokumentation Ihres Flash-Players oder wenden sich an den Hersteller bzw. Benutzer-Support. Sollten Sie die Installation der Cookies verhindern oder einschr√§nken, kann dies allerdings dazu f√ºhren, dass nicht s√§mtliche Funktionen unseres Internetauftritts vollumf√§nglich nutzbar sind.\n\n\nGoogle Analytics\nIn unserem Internetauftritt setzen wir Google Analytics ein. Hierbei handelt es sich um einen Webanalysedienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur ‚ÄûGoogle‚Äú genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild (‚ÄûEU-US Privacy Shield‚Äú) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Der Dienst Google Analytics dient zur Analyse des Nutzungsverhaltens unseres Internetauftritts. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Analyse, Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Nutzungs- und nutzerbezogene Informationen, wie bspw. IP-Adresse, Ort, Zeit oder H√§ufigkeit des Besuchs unseres Internetauftritts, werden dabei an einen Server von Google in den USA √ºbertragen und dort gespeichert. Allerdings nutzen wir Google Analytics mit der sog. Anonymisierungsfunktion. Durch diese Funktion k√ºrzt Google die IP-Adresse schon innerhalb der EU bzw. des EWR. Die so erhobenen Daten werden wiederum von Google genutzt, um uns eine Auswertung √ºber den Besuch unseres Internetauftritts sowie √ºber die dortigen Nutzungsaktivit√§ten zur Verf√ºgung zu stellen. Auch k√∂nnen diese Daten genutzt werden, um weitere Dienstleistungen zu erbringen, die mit der Nutzung unseres Internetauftritts und der Nutzung des Internets zusammenh√§ngen. Google gibt an, Ihre IP-Adresse nicht mit anderen Daten zu verbinden. Zudem h√§lt Google unter https://www.google.com/intl/de/policies/privacy/partners weitere datenschutzrechtliche Informationen f√ºr Sie bereit, so bspw. auch zu den M√∂glichkeiten, die Datennutzung zu unterbinden. Zudem bietet Google unter https://tools.google.com/dlpage/gaoptout?hl=de ein sog. Deaktivierungs-Add-on nebst weiteren Informationen hierzu an. Dieses Add-on l√§sst sich mit den g√§ngigen Internet-Browsern installieren und bietet Ihnen weitergehende Kontrollm√∂glichkeit √ºber die Daten, die Google bei Aufruf unseres Internetauftritts erfasst. Dabei teilt das Add-on dem JavaScript (ga.js) von Google Analytics mit, dass Informationen zum Besuch unseres Internetauftritts nicht an Google Analytics √ºbermittelt werden sollen. Dies verhindert aber nicht, dass Informationen an uns oder an andere Webanalysedienste √ºbermittelt werden. Ob und welche weiteren Webanalysedienste von uns eingesetzt werden, erfahren Sie nat√ºrlich ebenfalls in dieser Datenschutzerkl√§rung.\n\n\nGoogle-Maps\nIn unserem Internetauftritt setzen wir Google Maps zur Darstellung unseres Standorts sowie zur Erstellung einer Anfahrtsbeschreibung ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur ‚ÄûGoogle‚Äú genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild (‚ÄûEU-US Privacy Shield‚Äú) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu erm√∂glichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Sofern Sie die in unseren Internetauftritt eingebundene Komponente Google Maps aufrufen, speichert Google √ºber Ihren Internet-Browser ein Cookie auf Ihrem Endger√§t. Um unseren Standort anzuzeigen und eine Anfahrtsbeschreibung zu erstellen, werden Ihre Nutzereinstellungen und -daten verarbeitet. Hierbei k√∂nnen wir nicht ausschlie√üen, dass Google Server in den USA einsetzt. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung der Funktionalit√§t unseres Internetauftritts. Durch die so hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Anfahrtsbeschreibung zu √ºbermitteln ist. Sofern Sie mit dieser Verarbeitung nicht einverstanden sind, haben Sie die M√∂glichkeit, die Installation der Cookies durch die entsprechenden Einstellungen in Ihrem Internet-Browser zu verhindern. Einzelheiten hierzu finden Sie vorstehend unter dem Punkt ‚ÄûCookies‚Äú. Zudem erfolgt die Nutzung von Google Maps sowie der √ºber Google Maps erlangten Informationen nach den Google-Nutzungsbedingungen https://policies.google.com/terms?gl=DE&hl=de und den Gesch√§ftsbedingungen f√ºr Google Maps https://www.google.com/intl/de_de/help/terms_maps.html. √úberdies bietet Google unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitergehende Informationen an.\n\n\nGoogle reCAPTCHA\nIn unserem Internetauftritt setzen wir Google reCAPTCHA zur √úberpr√ºfung und Vermeidung von Interaktionen auf unserer Internetseite durch automatisierte Zugriffe, bspw. durch sog. Bots, ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur ‚ÄûGoogle‚Äú genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild (‚ÄûEU-US Privacy Shield‚Äú) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Durch diesen Dienst kann Google ermitteln, von welcher Webseite eine Anfrage gesendet wird sowie von welcher IP-Adresse aus Sie die sog. reCAPTCHA-Eingabebox verwenden. Neben Ihrer IP-Adresse werden wom√∂glich noch weitere Informationen durch Google erfasst, die f√ºr das Angebot und die Gew√§hrleistung dieses Dienstes notwendig sind. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Sicherheit unseres Internetauftritts sowie in der Abwehr unerw√ºnschter, automatisierter Zugriffe in Form von Spam o.√§.. Google bietet unter https://policies.google.com/privacy weitergehende Informationen zu dem allgemeinen Umgang mit Ihren Nutzerdaten an.\n\n\nGoogle Fonts\nIn unserem Internetauftritt setzen wir Google Fonts zur Darstellung externer Schriftarten ein. Es handelt sich hierbei um einen Dienst der Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043 USA, nachfolgend nur ‚ÄûGoogle‚Äú genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild (‚ÄûEU-US Privacy Shield‚Äú) https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active garantiert Google, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Um die Darstellung bestimmter Schriften in unserem Internetauftritt zu erm√∂glichen, wird bei Aufruf unseres Internetauftritts eine Verbindung zu dem Google-Server in den USA aufgebaut. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Optimierung und dem wirtschaftlichen Betrieb unseres Internetauftritts. Durch die bei Aufruf unseres Internetauftritts hergestellte Verbindung zu Google kann Google ermitteln, von welcher Website Ihre Anfrage gesendet worden ist und an welche IP-Adresse die Darstellung der Schrift zu √ºbermitteln ist. Google bietet unter https://adssettings.google.com/authenticated https://policies.google.com/privacy weitere Informationen an und zwar insbesondere zu den M√∂glichkeiten der Unterbindung der Datennutzung.\n\n\n‚ÄûFacebook‚Äú-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Facebook ein. Bei Facebook handelt es sich um einen Internetservice der facebook Inc., 1601 S. California Ave, Palo Alto, CA 94304, USA. In der EU wird dieser Service wiederum von der Facebook Ireland Limited, 4 Grand Canal Square, Dublin 2, Irland, betrieben, nachfolgend beide nur ‚ÄûFacebook‚Äú genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild (‚ÄûEU-US Privacy Shield‚Äú) https://www.privacyshield.gov/participant?id=a2zt0000000GnywAAC&status=Active garantiert Facebook, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualit√§tsverbesserung unseres Internetauftritts. Weitergehende Informationen √ºber die m√∂glichen Plug-ins sowie √ºber deren jeweilige Funktionen h√§lt Facebook unter https://developers.facebook.com/docs/plugins/ f√ºr Sie bereit.\nSofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, l√§dt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Facebook in den USA herunter. Aus technischen Gr√ºnden ist es dabei notwendig, dass Facebook Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Facebook eingeloggt sein, w√§hrend Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Facebook erkannt. Die so gesammelten Informationen weist Facebook wom√∂glich Ihrem dortigen pers√∂nlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. ‚ÄûGef√§llt mir‚Äú-Button von Facebook benutzen, werden diese Informationen in Ihrem Facebook-Nutzerkonto gespeichert und ggf. √ºber die Plattform von Facebook ver√∂ffentlicht. Wenn Sie das verhindern m√∂chten, m√ºssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Facebook ausloggen oder durch den Einsatz eines Add-ons f√ºr Ihren Internetbrowser verhindern, dass das Laden des Facebook-Plug-in blockiert wird. Weitergehende Informationen √ºber die Erhebung und Nutzung von Daten sowie Ihre diesbez√ºglichen Rechte und Schutzm√∂glichkeiten h√§lt Facebook in den unter https://www.facebook.com/policy.php abrufbaren Datenschutzhinweisen bereit.\n\n\nX-(ehemals ‚ÄûTwitter‚Äú)-Social-Plug-in\nIn unserem Internetauftritt setzen wir das Plug-in des Social-Networks Twitter ein. Bei X handelt es sich um einen Internetservice der X.AI Corp., 795 Folsom St., Suite 600, San Francisco, CA 94107, USA, nachfolgend nur ‚ÄûxAI‚Äú genannt. Durch die Zertifizierung nach dem EU-US-Datenschutzschild (‚ÄûEU-US Privacy Shield‚Äú) https://www.privacyshield.gov/participant?id=a2zt0000000TORzAAO&status=Active garantiert Twitter, dass die Datenschutzvorgaben der EU auch bei der Verarbeitung von Daten in den USA eingehalten werden. Rechtsgrundlage ist Art. 6 Abs. 1 lit. f) DSGVO. Unser berechtigtes Interesse liegt in der Qualit√§tsverbesserung unseres Internetauftritts. Sofern das Plug-in auf einer der von Ihnen besuchten Seiten unseres Internetauftritts hinterlegt ist, l√§dt Ihr Internet-Browser eine Darstellung des Plug-ins von den Servern von Twitter in den USA herunter. Aus technischen Gr√ºnden ist es dabei notwendig, dass Twitter Ihre IP-Adresse verarbeitet. Daneben werden aber auch Datum und Uhrzeit des Besuchs unserer Internetseiten erfasst. Sollten Sie bei Twitter eingeloggt sein, w√§hrend Sie eine unserer mit dem Plug-in versehenen Internetseite besuchen, werden die durch das Plug-in gesammelten Informationen Ihres konkreten Besuchs von Twitter erkannt. Die so gesammelten Informationen weist Twitter wom√∂glich Ihrem dortigen pers√∂nlichen Nutzerkonto zu. Sofern Sie also bspw. den sog. ‚ÄûTeilen‚Äú-Button von Twitter benutzen, werden diese Informationen in Ihrem Twitter-Nutzerkonto gespeichert und ggf. √ºber die Plattform von Twitter ver√∂ffentlicht. Wenn Sie das verhindern m√∂chten, m√ºssen Sie sich entweder vor dem Besuch unseres Internetauftritts bei Twitter ausloggen oder die entsprechenden Einstellungen in Ihrem Twitter-Benutzerkonto vornehmen. Weitergehende Informationen √ºber die Erhebung und Nutzung von Daten sowie Ihre diesbez√ºglichen Rechte und Schutzm√∂glichkeiten h√§lt Twitter in den unter https://twitter.com/privacy abrufbaren Datenschutzhinweisen bereit.\nMuster-Datenschutzerkl√§rung der Anwaltskanzlei Wei√ü & Partner"
  }
]